{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "\"Machine learning tools\"\n",
    "import pickle\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "\n",
    "\n",
    "from classification.datasets import Dataset\n",
    "from classification.utils.audio_student import AudioUtil, Feature_vector_DS\n",
    "\n",
    "from classification.utils.plots import (\n",
    "    plot_decision_boundaries,\n",
    "    plot_specgram,\n",
    "    show_confusion_matrix,\n",
    ")\n",
    "from classification.utils.utils import accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chainsaw\n",
      "fire\n",
      "fireworks\n",
      "gunshot\n"
     ]
    }
   ],
   "source": [
    "### TO RUN\n",
    "dataset = Dataset()\n",
    "classnames = dataset.list_classes()\n",
    "\n",
    "print(\"\\n\".join(classnames))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TO RUN\n",
    "fm_dir = \"data/feature_matrices/\"  # where to save the features matrices\n",
    "model_dir = \"data/models/xgb_new_features\"  # where to save the models\n",
    "os.makedirs(fm_dir, exist_ok=True)\n",
    "os.makedirs(model_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TO RUN\n",
    "\n",
    "\"Creation of the dataset\"\n",
    "myds = Feature_vector_DS(dataset, Nft=512, nmel=20, duration=950, shift_pct=0.0)\n",
    "\n",
    "\"Some attributes...\"\n",
    "myds.nmel\n",
    "myds.duration\n",
    "myds.shift_pct\n",
    "myds.sr\n",
    "myds.data_aug\n",
    "myds.ncol\n",
    "\n",
    "idx = 0\n",
    "\n",
    "# XGBOOST PARAMETERS\n",
    "n_estimators = 123\n",
    "max_depth = 14\n",
    "learning_rate = 0.1091\n",
    "subsample = 0.5191\n",
    "colsample_bytree = 0.9210"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the training matrix : (182, 400)\n",
      "Shape of the test matrix : (79, 400)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "from sklearn.calibration import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "train_pct = 0.7\n",
    "data_aug_factor = 1\n",
    "featveclen = len(myds[\"fire\", 0, \"\", \"\"])  # Same for all classes\n",
    "classnames = [\"chainsaw\", \"fire\", \"fireworks\", \"gunshot\"]  # Or wherever you store class names\n",
    "nclass = len(classnames)\n",
    "\n",
    "# Determine number of samples per class\n",
    "naudio_per_class = {\"chainsaw\" : 70, \"fire\" : 76, \"fireworks\" : 75, \"gunshot\" : 40}\n",
    "\n",
    "\n",
    "# Allocate feature matrix\n",
    "total_samples_basic = sum(naudio_per_class[c] for c in classnames)\n",
    "X_basic = np.zeros((total_samples_basic, featveclen))\n",
    "y_basic = np.zeros((total_samples_basic), dtype=object)\n",
    "total_samples_basic\n",
    "# Fill feature matrix\n",
    "idx = 0\n",
    "for class_idx, classname in enumerate(classnames):\n",
    "    for i in range(naudio_per_class[classname]):\n",
    "        featvec = myds[classname, i, \"\", \"\"]\n",
    "        X_basic[idx, :] = featvec\n",
    "        y_basic[idx] = classname\n",
    "        idx += 1\n",
    "\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_basic, y_basic, test_size=0.3, random_state=42)\n",
    "scaler = StandardScaler()\n",
    "X_train_norm = scaler.fit_transform(X_train)\n",
    "X_test_norm = scaler.transform(X_test)\n",
    "\n",
    "le           = LabelEncoder()\n",
    "y_train     = le.fit_transform(y_train)\n",
    "y_test      = le.transform(y_test)\n",
    "\n",
    "# Save the feature matrix and labels\n",
    "np.save(os.path.join(fm_dir, \"X_train.npy\"), X_train)\n",
    "np.save(os.path.join(fm_dir, \"X_test.npy\"), X_test)\n",
    "np.save(os.path.join(fm_dir, \"y_train.npy\"), y_train)\n",
    "np.save(os.path.join(fm_dir, \"y_test.npy\"), y_test)\n",
    "np.save(os.path.join(fm_dir, \"X_train_norm.npy\"), X_train_norm)\n",
    "np.save(os.path.join(fm_dir, \"X_test_norm.npy\"), X_test_norm)\n",
    "\n",
    "print(f\"Shape of the training matrix : {X_train.shape}\")\n",
    "print(f\"Shape of the test matrix : {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now create a new augmented dataset and observe if the classification results improve. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of transformations :  3\n",
      "Shape of the training matrix : (546, 400)\n",
      "Shape of the test matrix : (237, 400)\n",
      "------------------------------------------------------------\n",
      "Transformations: ['original', 'noise', 'shifting']. Labels aligned dynamically with class sizes.\n"
     ]
    }
   ],
   "source": [
    "### AUGMENTED DATASET\n",
    "list_augmentation = [\"original\", \"noise\", \"shifting\"]\n",
    "myds.mod_data_aug(list_augmentation)\n",
    "print(\"Number of transformations : \", myds.data_aug_factor)\n",
    "\n",
    "\n",
    "# Préparer les splits\n",
    "X_train_list, y_train_list = [], []\n",
    "X_test_list,  y_test_list  = [], []\n",
    "\n",
    "for classname in classnames:\n",
    "    n = naudio_per_class[classname]\n",
    "\n",
    "    # Création des indices de base pour les sons originaux\n",
    "    original_indices = list(range(n))\n",
    "    train_idx, test_idx = train_test_split(original_indices, test_size=0.3, random_state=42)\n",
    "\n",
    "    for i in train_idx:\n",
    "        for aug in list_augmentation:\n",
    "            featvec = myds[classname, i, aug, \"\"]\n",
    "            X_train_list.append(featvec)\n",
    "            y_train_list.append(classname)\n",
    "\n",
    "    for i in test_idx:\n",
    "        for aug in list_augmentation:\n",
    "            featvec = myds[classname, i, aug, \"\"]\n",
    "            X_test_list.append(featvec)\n",
    "            y_test_list.append(classname)\n",
    "\n",
    "# Conversion en tableaux numpy\n",
    "X_train_aug = np.array(X_train_list)\n",
    "y_train_aug = np.array(y_train_list, dtype=object)\n",
    "\n",
    "X_test_aug = np.array(X_test_list)\n",
    "y_test_aug = np.array(y_test_list, dtype=object)\n",
    "\n",
    "# Save features and labels\n",
    "scaler = StandardScaler()\n",
    "X_train_aug_norm = scaler.fit_transform(X_train_aug)\n",
    "X_test_aug_norm = scaler.fit_transform(X_test_aug)\n",
    "\n",
    "y_train_aug     = le.fit_transform(y_train_aug)\n",
    "y_test_aug      = le.transform(y_test_aug)\n",
    "\n",
    "np.save(os.path.join(fm_dir, \"X_train_aug.npy\"), X_train_aug)\n",
    "np.save(os.path.join(fm_dir, \"X_test_aug.npy\"), X_test_aug)\n",
    "np.save(os.path.join(fm_dir, \"y_train_aug.npy\"), y_train_aug)\n",
    "np.save(os.path.join(fm_dir, \"y_test_aug.npy\"), y_test_aug)\n",
    "np.save(os.path.join(fm_dir, \"X_train_aug_norm.npy\"), X_train_aug_norm)\n",
    "np.save(os.path.join(fm_dir, \"X_test_aug_norm.npy\"), X_test_aug_norm)\n",
    "\n",
    "print(f\"Shape of the training matrix : {X_train_aug.shape}\")\n",
    "print(f\"Shape of the test matrix : {X_test_aug.shape}\")\n",
    "print(f\"------------------------------------------------------------\")\n",
    "print(f\"Transformations: {list_augmentation}. Labels aligned dynamically with class sizes.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Variance (both) ajoutée !  Nouvelles dimensions : 440 features\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------------------------------------------\n",
    "# PARAMÈTRES\n",
    "# ------------------------------------------------------------------\n",
    "NEW_FEATURES   = True         # active / désactive le bloc\n",
    "VARIANCE_MODE  = \"both\"       # \"band\" | \"column\" | \"both\"\n",
    "N_MELS         = 20           # doit correspondre à vos melspectrogrammes\n",
    "# ------------------------------------------------------------------\n",
    "\n",
    "if NEW_FEATURES:\n",
    "\n",
    "    def add_variance_features(X, n_mels=N_MELS, mode=\"both\"):\n",
    "        \"\"\"\n",
    "        Ajoute les variances au vecteur de base.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : ndarray shape (n_samples, n_mels * n_cols)\n",
    "            Matrice de features d'origine (melspectrogrammes aplatis).\n",
    "        n_mels : int\n",
    "            Nombre de bandes Mel (lignes du spectrogramme avant flatten).\n",
    "        mode : {\"band\", \"column\", \"both\"}\n",
    "            - \"band\"   : variance temporelle par bande (n_mels valeurs)\n",
    "            - \"column\" : variance spectrale par trame  (n_cols valeurs)\n",
    "            - \"both\"   : concatène les deux\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        X_new : ndarray shape (n_samples, n_features + ...)\n",
    "                 Matrice augmentée.\n",
    "        \"\"\"\n",
    "        if mode not in {\"band\", \"column\", \"both\"}:\n",
    "            raise ValueError(\"mode must be 'band', 'column' or 'both'\")\n",
    "\n",
    "        n_cols = X.shape[1] // n_mels\n",
    "        X_out = []\n",
    "\n",
    "        for vec in X:\n",
    "            mel = vec.reshape(n_mels, n_cols)  # remet en (mels, cols)\n",
    "            feats = [vec]                      # vecteur original\n",
    "\n",
    "            if mode in {\"band\", \"both\"}:\n",
    "                var_band = np.var(mel, axis=1, ddof=0)      # (n_mels,)\n",
    "                feats.append(var_band)\n",
    "\n",
    "            if mode in {\"column\", \"both\"}:\n",
    "                var_col = np.var(mel, axis=0, ddof=0)       # (n_cols,)\n",
    "                feats.append(var_col)\n",
    "\n",
    "            X_out.append(np.concatenate(feats))\n",
    "\n",
    "        return np.vstack(X_out).astype(np.float32)\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    # APPLICATION AUX QUATRE JEU(X) DE FEATURES\n",
    "    # ------------------------------------------------------------------\n",
    "    X_train          = add_variance_features(X_train,          N_MELS, VARIANCE_MODE)\n",
    "    X_test           = add_variance_features(X_test,           N_MELS, VARIANCE_MODE)\n",
    "    X_train_aug      = add_variance_features(X_train_aug,      N_MELS, VARIANCE_MODE)\n",
    "    X_test_aug       = add_variance_features(X_test_aug,       N_MELS, VARIANCE_MODE)\n",
    "    X_train_norm     = add_variance_features(X_train_norm,    N_MELS, VARIANCE_MODE)\n",
    "    X_test_norm      = add_variance_features(X_test_norm,     N_MELS, VARIANCE_MODE)\n",
    "    X_train_aug_norm = add_variance_features(X_train_aug_norm, N_MELS, VARIANCE_MODE)\n",
    "    X_test_aug_norm  = add_variance_features(X_test_aug_norm,  N_MELS, VARIANCE_MODE)\n",
    "    # ------------------------------------------------------------------\n",
    "    \n",
    "    np.save(os.path.join(fm_dir, \"X_train.npy\"), X_train)\n",
    "    np.save(os.path.join(fm_dir, \"X_test.npy\"), X_test)\n",
    "    np.save(os.path.join(fm_dir, \"X_train_norm.npy\"), X_train_norm)\n",
    "    np.save(os.path.join(fm_dir, \"X_test_norm.npy\"), X_test_norm)\n",
    "    np.save(os.path.join(fm_dir, \"X_train_aug.npy\"), X_train_aug)\n",
    "    np.save(os.path.join(fm_dir, \"X_test_aug.npy\"), X_test_aug)\n",
    "    np.save(os.path.join(fm_dir, \"X_train_aug_norm.npy\"), X_train_aug_norm)\n",
    "    np.save(os.path.join(fm_dir, \"X_test_aug_norm.npy\"), X_test_aug_norm)\n",
    "        \n",
    "    print(f\"✅ Variance ({VARIANCE_MODE}) ajoutée !  \"\n",
    "          f\"Nouvelles dimensions : {X_train.shape[1]} features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from classification.utils.plots import plot_specgram_textlabel\n",
    "# ------------------------------------------------------------------\n",
    "BASIC = False\n",
    "if BASIC:\n",
    "    # Charger les données\n",
    "    X = np.load(os.path.join(fm_dir, \"X_test.npy\"), allow_pickle=True)\n",
    "    y = np.load(os.path.join(fm_dir, \"y_test.npy\"), allow_pickle=True)\n",
    "\n",
    "    # Dossier où sauvegarder les images\n",
    "    save_dir = os.path.join(\"src/classification/soundfiles_melspec\")\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "    # Initialiser les compteurs par classe\n",
    "    class_counters = {}\n",
    "\n",
    "    for i in range(len(X)):\n",
    "        melspec = X[i]\n",
    "        class_of_spec = y[i]\n",
    "\n",
    "        if class_of_spec not in class_counters:\n",
    "            class_counters[class_of_spec] = 0\n",
    "        class_idx = class_counters[class_of_spec]\n",
    "\n",
    "        fig, ax = plt.subplots()\n",
    "        plot_specgram_textlabel(  # ✅ fonction corrigée\n",
    "            melspec.reshape((22, 20)),\n",
    "            ax=ax,\n",
    "            is_mel=True,\n",
    "            title=f\"MEL Spectrogram - {class_of_spec} #{class_idx}\",\n",
    "            xlabel=\"Mel vector\",\n",
    "            textlabel=f\"{class_of_spec}\",\n",
    "        )\n",
    "        plt.tight_layout()\n",
    "        save_path = os.path.join(save_dir, f\"melspec_{class_of_spec}_{class_idx}.png\")\n",
    "        fig.savefig(save_path)\n",
    "        plt.close(fig)\n",
    "\n",
    "        class_counters[class_of_spec] += 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FINAL MODEL SAVE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Scenario A : PCA NOAUG NONORM ===\n",
      "Overall Test Accuracy: 0.6456\n",
      "Mean CV Accuracy  (5‑fold): 0.5950\n",
      "Per‑class metrics:\n",
      "  Class 0: Precision=0.4444, Recall=0.4211, Accuracy=0.4211\n",
      "  Class 1: Precision=0.7727, Recall=0.6071, Accuracy=0.6071\n",
      "  Class 2: Precision=0.5556, Recall=0.7895, Accuracy=0.7895\n",
      "  Class 3: Precision=0.9167, Recall=0.8462, Accuracy=0.8462\n",
      "\n",
      "=== Scenario B : NOPCA NOAUG NONORM ===\n",
      "Overall Test Accuracy: 0.8987\n",
      "Mean CV Accuracy  (5‑fold): 0.7733\n",
      "Per‑class metrics:\n",
      "  Class 0: Precision=0.9412, Recall=0.8421, Accuracy=0.8421\n",
      "  Class 1: Precision=0.8667, Recall=0.9286, Accuracy=0.9286\n",
      "  Class 2: Precision=0.8421, Recall=0.8421, Accuracy=0.8421\n",
      "  Class 3: Precision=1.0000, Recall=1.0000, Accuracy=1.0000\n",
      "\n",
      "=== Scenario C : PCA AUG NONORM ===\n",
      "Overall Test Accuracy: 0.7173\n",
      "Mean CV Accuracy  (5‑fold): 0.6071\n",
      "Per‑class metrics:\n",
      "  Class 0: Precision=0.6154, Recall=0.5079, Accuracy=0.5079\n",
      "  Class 1: Precision=0.7500, Recall=0.7826, Accuracy=0.7826\n",
      "  Class 2: Precision=0.7222, Recall=0.7536, Accuracy=0.7536\n",
      "  Class 3: Precision=0.7805, Recall=0.8889, Accuracy=0.8889\n",
      "\n",
      "=== Scenario D : NOPCA AUG NONORM ===\n",
      "Overall Test Accuracy: 0.8734\n",
      "Mean CV Accuracy  (5‑fold): 0.8227\n",
      "Per‑class metrics:\n",
      "  Class 0: Precision=0.8209, Recall=0.8730, Accuracy=0.8730\n",
      "  Class 1: Precision=0.8955, Recall=0.8696, Accuracy=0.8696\n",
      "  Class 2: Precision=0.8429, Recall=0.8551, Accuracy=0.8551\n",
      "  Class 3: Precision=1.0000, Recall=0.9167, Accuracy=0.9167\n",
      "\n",
      "=== Scenario E : NOPCA NOAUG NORM ===\n",
      "Overall Test Accuracy: 0.8987\n",
      "Mean CV Accuracy  (5‑fold): 0.8108\n",
      "Per‑class metrics:\n",
      "  Class 0: Precision=0.9444, Recall=0.8947, Accuracy=0.8947\n",
      "  Class 1: Precision=0.8667, Recall=0.9286, Accuracy=0.9286\n",
      "  Class 2: Precision=0.8421, Recall=0.8421, Accuracy=0.8421\n",
      "  Class 3: Precision=1.0000, Recall=0.9231, Accuracy=0.9231\n",
      "\n",
      "=== Scenario F : PCA NOAUG NORM ===\n",
      "Overall Test Accuracy: 0.7975\n",
      "Mean CV Accuracy  (5‑fold): 0.7217\n",
      "Per‑class metrics:\n",
      "  Class 0: Precision=0.8462, Recall=0.5789, Accuracy=0.5789\n",
      "  Class 1: Precision=0.8182, Recall=0.9643, Accuracy=0.9643\n",
      "  Class 2: Precision=0.7143, Recall=0.7895, Accuracy=0.7895\n",
      "  Class 3: Precision=0.8333, Recall=0.7692, Accuracy=0.7692\n",
      "\n",
      "=== Scenario G : PCA AUG NORM ===\n",
      "Overall Test Accuracy: 0.8312\n",
      "Mean CV Accuracy  (5‑fold): 0.6925\n",
      "Per‑class metrics:\n",
      "  Class 0: Precision=0.8333, Recall=0.7937, Accuracy=0.7937\n",
      "  Class 1: Precision=0.8657, Recall=0.8406, Accuracy=0.8406\n",
      "  Class 2: Precision=0.8261, Recall=0.8261, Accuracy=0.8261\n",
      "  Class 3: Precision=0.7805, Recall=0.8889, Accuracy=0.8889\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 199\u001b[0m\n\u001b[1;32m    197\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m F: evaluate_model(xgb_F, X_test_F,  y_test, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mScenario F : PCA NOAUG NORM\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    198\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m G: evaluate_model(xgb_G, X_test_G,  y_test_aug, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mScenario G : PCA AUG NORM\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 199\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m H: evaluate_model(xgb_H, X_test_aug_norm, y_test_aug, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mScenario H : NOPCA AUG NORM\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[21], line 179\u001b[0m, in \u001b[0;36mevaluate_model\u001b[0;34m(model, X_test, y_test, description)\u001b[0m\n\u001b[1;32m    176\u001b[0m cm        \u001b[38;5;241m=\u001b[39m confusion_matrix(y_test, preds, labels\u001b[38;5;241m=\u001b[39mclasses)\n\u001b[1;32m    178\u001b[0m test_acc_per_class \u001b[38;5;241m=\u001b[39m [cm[i, i] \u001b[38;5;241m/\u001b[39m cm[i, :]\u001b[38;5;241m.\u001b[39msum() \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(classes))]\n\u001b[0;32m--> 179\u001b[0m cv_acc \u001b[38;5;241m=\u001b[39m \u001b[43mcross_val_score\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcv\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscoring\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43maccuracy\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mmean()\n\u001b[1;32m    181\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m=== \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdescription\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m ===\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOverall Test Accuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnp\u001b[38;5;241m.\u001b[39mmean(preds\u001b[38;5;250m \u001b[39m\u001b[38;5;241m==\u001b[39m\u001b[38;5;250m \u001b[39my_test)\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/master_projet_elec/LELEC210X/.venv/lib/python3.9/site-packages/sklearn/utils/_param_validation.py:216\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    210\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    211\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m    212\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m    213\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m    214\u001b[0m         )\n\u001b[1;32m    215\u001b[0m     ):\n\u001b[0;32m--> 216\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    217\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[1;32m    219\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[1;32m    220\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[1;32m    221\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[1;32m    222\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[1;32m    223\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    224\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    225\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[1;32m    226\u001b[0m     )\n",
      "File \u001b[0;32m~/master_projet_elec/LELEC210X/.venv/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:684\u001b[0m, in \u001b[0;36mcross_val_score\u001b[0;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, params, pre_dispatch, error_score)\u001b[0m\n\u001b[1;32m    681\u001b[0m \u001b[38;5;66;03m# To ensure multimetric format is not supported\u001b[39;00m\n\u001b[1;32m    682\u001b[0m scorer \u001b[38;5;241m=\u001b[39m check_scoring(estimator, scoring\u001b[38;5;241m=\u001b[39mscoring)\n\u001b[0;32m--> 684\u001b[0m cv_results \u001b[38;5;241m=\u001b[39m \u001b[43mcross_validate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    685\u001b[0m \u001b[43m    \u001b[49m\u001b[43mestimator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    686\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    687\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    688\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgroups\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    689\u001b[0m \u001b[43m    \u001b[49m\u001b[43mscoring\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mscore\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mscorer\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    690\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcv\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    691\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    692\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    693\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    694\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpre_dispatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpre_dispatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    695\u001b[0m \u001b[43m    \u001b[49m\u001b[43merror_score\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merror_score\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    696\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    697\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m cv_results[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_score\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m~/master_projet_elec/LELEC210X/.venv/lib/python3.9/site-packages/sklearn/utils/_param_validation.py:216\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    210\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    211\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m    212\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m    213\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m    214\u001b[0m         )\n\u001b[1;32m    215\u001b[0m     ):\n\u001b[0;32m--> 216\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    217\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[1;32m    219\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[1;32m    220\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[1;32m    221\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[1;32m    222\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[1;32m    223\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    224\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    225\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[1;32m    226\u001b[0m     )\n",
      "File \u001b[0;32m~/master_projet_elec/LELEC210X/.venv/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:411\u001b[0m, in \u001b[0;36mcross_validate\u001b[0;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, params, pre_dispatch, return_train_score, return_estimator, return_indices, error_score)\u001b[0m\n\u001b[1;32m    408\u001b[0m \u001b[38;5;66;03m# We clone the estimator to make sure that all the folds are\u001b[39;00m\n\u001b[1;32m    409\u001b[0m \u001b[38;5;66;03m# independent, and that it is pickle-able.\u001b[39;00m\n\u001b[1;32m    410\u001b[0m parallel \u001b[38;5;241m=\u001b[39m Parallel(n_jobs\u001b[38;5;241m=\u001b[39mn_jobs, verbose\u001b[38;5;241m=\u001b[39mverbose, pre_dispatch\u001b[38;5;241m=\u001b[39mpre_dispatch)\n\u001b[0;32m--> 411\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mparallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    412\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_fit_and_score\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    413\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    414\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    415\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    416\u001b[0m \u001b[43m        \u001b[49m\u001b[43mscorer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscorers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    417\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    418\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtest\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    419\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    420\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparameters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    421\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfit_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrouted_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mestimator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    422\u001b[0m \u001b[43m        \u001b[49m\u001b[43mscore_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrouted_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscorer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscore\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    423\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_train_score\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_train_score\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    424\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_times\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    425\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_estimator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_estimator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    426\u001b[0m \u001b[43m        \u001b[49m\u001b[43merror_score\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merror_score\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    427\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    428\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mindices\u001b[49m\n\u001b[1;32m    429\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    431\u001b[0m _warn_or_raise_about_fit_failures(results, error_score)\n\u001b[1;32m    433\u001b[0m \u001b[38;5;66;03m# For callable scoring, the return type is only know after calling. If the\u001b[39;00m\n\u001b[1;32m    434\u001b[0m \u001b[38;5;66;03m# return type is a dictionary, the error scores can now be inserted with\u001b[39;00m\n\u001b[1;32m    435\u001b[0m \u001b[38;5;66;03m# the correct key.\u001b[39;00m\n",
      "File \u001b[0;32m~/master_projet_elec/LELEC210X/.venv/lib/python3.9/site-packages/sklearn/utils/parallel.py:77\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     72\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[1;32m     73\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     74\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[1;32m     75\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[1;32m     76\u001b[0m )\n\u001b[0;32m---> 77\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/master_projet_elec/LELEC210X/.venv/lib/python3.9/site-packages/joblib/parallel.py:1918\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1916\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_sequential_output(iterable)\n\u001b[1;32m   1917\u001b[0m     \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[0;32m-> 1918\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1920\u001b[0m \u001b[38;5;66;03m# Let's create an ID that uniquely identifies the current call. If the\u001b[39;00m\n\u001b[1;32m   1921\u001b[0m \u001b[38;5;66;03m# call is interrupted early and that the same instance is immediately\u001b[39;00m\n\u001b[1;32m   1922\u001b[0m \u001b[38;5;66;03m# re-used, this id will be used to prevent workers that were\u001b[39;00m\n\u001b[1;32m   1923\u001b[0m \u001b[38;5;66;03m# concurrently finalizing a task from the previous call to run the\u001b[39;00m\n\u001b[1;32m   1924\u001b[0m \u001b[38;5;66;03m# callback.\u001b[39;00m\n\u001b[1;32m   1925\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n",
      "File \u001b[0;32m~/master_projet_elec/LELEC210X/.venv/lib/python3.9/site-packages/joblib/parallel.py:1847\u001b[0m, in \u001b[0;36mParallel._get_sequential_output\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1845\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_batches \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1846\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m-> 1847\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1848\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_completed_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1849\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprint_progress()\n",
      "File \u001b[0;32m~/master_projet_elec/LELEC210X/.venv/lib/python3.9/site-packages/sklearn/utils/parallel.py:139\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    137\u001b[0m     config \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m    138\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig):\n\u001b[0;32m--> 139\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/master_projet_elec/LELEC210X/.venv/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:866\u001b[0m, in \u001b[0;36m_fit_and_score\u001b[0;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, score_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, split_progress, candidate_progress, error_score)\u001b[0m\n\u001b[1;32m    864\u001b[0m         estimator\u001b[38;5;241m.\u001b[39mfit(X_train, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\n\u001b[1;32m    865\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 866\u001b[0m         \u001b[43mestimator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    868\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m    869\u001b[0m     \u001b[38;5;66;03m# Note fit time as time until error\u001b[39;00m\n\u001b[1;32m    870\u001b[0m     fit_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start_time\n",
      "File \u001b[0;32m~/master_projet_elec/LELEC210X/.venv/lib/python3.9/site-packages/xgboost/core.py:726\u001b[0m, in \u001b[0;36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    724\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sig\u001b[38;5;241m.\u001b[39mparameters, args):\n\u001b[1;32m    725\u001b[0m     kwargs[k] \u001b[38;5;241m=\u001b[39m arg\n\u001b[0;32m--> 726\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/master_projet_elec/LELEC210X/.venv/lib/python3.9/site-packages/xgboost/sklearn.py:1599\u001b[0m, in \u001b[0;36mXGBClassifier.fit\u001b[0;34m(self, X, y, sample_weight, base_margin, eval_set, verbose, xgb_model, sample_weight_eval_set, base_margin_eval_set, feature_weights)\u001b[0m\n\u001b[1;32m   1579\u001b[0m model, metric, params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_configure_fit(xgb_model, params)\n\u001b[1;32m   1580\u001b[0m train_dmatrix, evals \u001b[38;5;241m=\u001b[39m _wrap_evaluation_matrices(\n\u001b[1;32m   1581\u001b[0m     missing\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmissing,\n\u001b[1;32m   1582\u001b[0m     X\u001b[38;5;241m=\u001b[39mX,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1596\u001b[0m     feature_types\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeature_types,\n\u001b[1;32m   1597\u001b[0m )\n\u001b[0;32m-> 1599\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_Booster \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1600\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1601\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_dmatrix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1602\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_num_boosting_rounds\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1603\u001b[0m \u001b[43m    \u001b[49m\u001b[43mevals\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mevals\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1604\u001b[0m \u001b[43m    \u001b[49m\u001b[43mearly_stopping_rounds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mearly_stopping_rounds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1605\u001b[0m \u001b[43m    \u001b[49m\u001b[43mevals_result\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mevals_result\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1606\u001b[0m \u001b[43m    \u001b[49m\u001b[43mobj\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1607\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcustom_metric\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetric\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1608\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1609\u001b[0m \u001b[43m    \u001b[49m\u001b[43mxgb_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1610\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1611\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1613\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobjective):\n\u001b[1;32m   1614\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobjective \u001b[38;5;241m=\u001b[39m params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mobjective\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m~/master_projet_elec/LELEC210X/.venv/lib/python3.9/site-packages/xgboost/core.py:726\u001b[0m, in \u001b[0;36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    724\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sig\u001b[38;5;241m.\u001b[39mparameters, args):\n\u001b[1;32m    725\u001b[0m     kwargs[k] \u001b[38;5;241m=\u001b[39m arg\n\u001b[0;32m--> 726\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/master_projet_elec/LELEC210X/.venv/lib/python3.9/site-packages/xgboost/training.py:181\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(params, dtrain, num_boost_round, evals, obj, feval, maximize, early_stopping_rounds, evals_result, verbose_eval, xgb_model, callbacks, custom_metric)\u001b[0m\n\u001b[1;32m    179\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cb_container\u001b[38;5;241m.\u001b[39mbefore_iteration(bst, i, dtrain, evals):\n\u001b[1;32m    180\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m--> 181\u001b[0m \u001b[43mbst\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43miteration\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfobj\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cb_container\u001b[38;5;241m.\u001b[39mafter_iteration(bst, i, dtrain, evals):\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/master_projet_elec/LELEC210X/.venv/lib/python3.9/site-packages/xgboost/core.py:2101\u001b[0m, in \u001b[0;36mBooster.update\u001b[0;34m(self, dtrain, iteration, fobj)\u001b[0m\n\u001b[1;32m   2097\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_assign_dmatrix_features(dtrain)\n\u001b[1;32m   2099\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m fobj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   2100\u001b[0m     _check_call(\n\u001b[0;32m-> 2101\u001b[0m         \u001b[43m_LIB\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mXGBoosterUpdateOneIter\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2102\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mc_int\u001b[49m\u001b[43m(\u001b[49m\u001b[43miteration\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtrain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle\u001b[49m\n\u001b[1;32m   2103\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2104\u001b[0m     )\n\u001b[1;32m   2105\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2106\u001b[0m     pred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredict(dtrain, output_margin\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Config\n",
    "TEST_SET = True\n",
    "\n",
    "A = True  # PCA NOAUG NONORM\n",
    "B = True  # NOPCA NOAUG NONORM\n",
    "C = True  # PCA AUG NONORM\n",
    "D = True  # NOPCA AUG NONORM\n",
    "E = True  # NOPCA NOAUG NONORM\n",
    "F = True  # PCA NOAUG Z-SCORE\n",
    "G = True  # PCA AUG Z-SCORE\n",
    "H = True  # NOPCA AUG Z-SCORE\n",
    "\n",
    "\n",
    "# === Données NON augmentées ===\n",
    "X_train        = np.load(os.path.join(fm_dir, \"X_train.npy\"))\n",
    "X_test         = np.load(os.path.join(fm_dir, \"X_test.npy\"))\n",
    "y_train        = np.load(os.path.join(fm_dir, \"y_train.npy\"), allow_pickle=True)\n",
    "y_test         = np.load(os.path.join(fm_dir, \"y_test.npy\"), allow_pickle=True)\n",
    "\n",
    "X_train_norm   = np.load(os.path.join(fm_dir, \"X_train_norm.npy\"))\n",
    "X_test_norm    = np.load(os.path.join(fm_dir, \"X_test_norm.npy\"))\n",
    "\n",
    "# === Données AUGMENTÉES ===\n",
    "X_train_aug        = np.load(os.path.join(fm_dir, \"X_train_aug.npy\"))\n",
    "X_test_aug         = np.load(os.path.join(fm_dir, \"X_test_aug.npy\"))\n",
    "y_train_aug        = np.load(os.path.join(fm_dir, \"y_train_aug.npy\"), allow_pickle=True)\n",
    "y_test_aug         = np.load(os.path.join(fm_dir, \"y_test_aug.npy\"), allow_pickle=True)\n",
    "\n",
    "X_train_aug_norm   = np.load(os.path.join(fm_dir, \"X_train_aug_norm.npy\"))\n",
    "X_test_aug_norm    = np.load(os.path.join(fm_dir, \"X_test_aug_norm.npy\"))\n",
    "\n",
    "\n",
    "# =========================\n",
    "# SCÉNARIO A –– PCA, NO AUG, NON‑NORM\n",
    "# =========================\n",
    "if A:\n",
    "    pca_A = PCA(n_components=0.98)\n",
    "    X_train_A = pca_A.fit_transform(X_train)\n",
    "    X_test_A  = pca_A.transform(X_test)\n",
    "\n",
    "    with open(os.path.join(model_dir, \"pca_noaug_nonorm.pickle\"), \"wb\") as f:\n",
    "        pickle.dump(pca_A, f)\n",
    "\n",
    "    xgb_A = XGBClassifier(n_estimators=n_estimators, max_depth=max_depth, learning_rate=learning_rate,\n",
    "                          subsample=subsample, colsample_bytree=colsample_bytree,\n",
    "                          eval_metric='mlogloss', random_state=42)\n",
    "    xgb_A.fit(X_train_A, y_train)\n",
    "\n",
    "    with open(os.path.join(model_dir, \"xgb_new_features_pca_noaug_nonorm.pickle\"), \"wb\") as f:\n",
    "        pickle.dump(xgb_A, f)\n",
    "\n",
    "# =========================\n",
    "# SCÉNARIO B –– NOPCA, NO AUG, NON‑NORM\n",
    "# =========================\n",
    "if B:\n",
    "    xgb_B = XGBClassifier(n_estimators=n_estimators, max_depth=max_depth, learning_rate=learning_rate,\n",
    "                          subsample=subsample, colsample_bytree=colsample_bytree,\n",
    "                          eval_metric='mlogloss', random_state=42)\n",
    "    xgb_B.fit(X_train, y_train)\n",
    "\n",
    "    with open(os.path.join(model_dir, \"xgb_new_features_nopca_noaug_nonorm.pickle\"), \"wb\") as f:\n",
    "        pickle.dump(xgb_B, f)\n",
    "\n",
    "# =========================\n",
    "# SCÉNARIO C –– PCA, AUG, NON‑NORM\n",
    "# =========================\n",
    "if C:\n",
    "    pca_C = PCA(n_components=0.98)\n",
    "    X_train_C = pca_C.fit_transform(X_train_aug)\n",
    "    X_test_C  = pca_C.transform(X_test_aug)\n",
    "\n",
    "    with open(os.path.join(model_dir, \"pca_aug_nonorm.pickle\"), \"wb\") as f:\n",
    "        pickle.dump(pca_C, f)\n",
    "\n",
    "    xgb_C = XGBClassifier(n_estimators=n_estimators, max_depth=max_depth, learning_rate=learning_rate,\n",
    "                          subsample=subsample, colsample_bytree=colsample_bytree,\n",
    "                          eval_metric='mlogloss', random_state=42)\n",
    "    xgb_C.fit(X_train_C, y_train_aug)\n",
    "\n",
    "    with open(os.path.join(model_dir, \"xgb_new_features_pca_aug_nonorm.pickle\"), \"wb\") as f:\n",
    "        pickle.dump(xgb_C, f)\n",
    "\n",
    "# =========================\n",
    "# SCÉNARIO D –– NOPCA, AUG, NON‑NORM\n",
    "# =========================\n",
    "if D:\n",
    "    xgb_D = XGBClassifier(n_estimators=n_estimators, max_depth=max_depth, learning_rate=learning_rate,\n",
    "                          subsample=subsample, colsample_bytree=colsample_bytree,\n",
    "                          eval_metric='mlogloss', random_state=42)\n",
    "    xgb_D.fit(X_train_aug, y_train_aug)\n",
    "\n",
    "    with open(os.path.join(model_dir, \"xgb_new_features_nopca_aug_nonorm.pickle\"), \"wb\") as f:\n",
    "        pickle.dump(xgb_D, f)\n",
    "\n",
    "# =========================\n",
    "# SCÉNARIO E –– NOPCA, NO AUG, NORM\n",
    "# =========================\n",
    "if E:\n",
    "    xgb_E = XGBClassifier(n_estimators=n_estimators, max_depth=max_depth, learning_rate=learning_rate,\n",
    "                          subsample=subsample, colsample_bytree=colsample_bytree,\n",
    "                          eval_metric='mlogloss', random_state=42)\n",
    "    xgb_E.fit(X_train_norm, y_train)\n",
    "\n",
    "    with open(os.path.join(model_dir, \"xgb_new_features_nopca_noaug_nonorm_dup.pickle\"), \"wb\") as f:\n",
    "        pickle.dump(xgb_E, f)\n",
    "\n",
    "# =========================\n",
    "# SCÉNARIO F –– Z‑SCORE, PCA, NO AUG\n",
    "# =========================\n",
    "if F:\n",
    "    pca_F = PCA(n_components=0.98)\n",
    "    X_train_F = pca_F.fit_transform(X_train_norm)\n",
    "    X_test_F  = pca_F.transform(X_test_norm)\n",
    "\n",
    "    with open(os.path.join(model_dir, \"pca_noaug_norm.pickle\"), \"wb\") as f:\n",
    "        pickle.dump(pca_F, f)\n",
    "\n",
    "    xgb_F = XGBClassifier(n_estimators=n_estimators, max_depth=max_depth, learning_rate=learning_rate,\n",
    "                          subsample=subsample, colsample_bytree=colsample_bytree,\n",
    "                          eval_metric='mlogloss', random_state=42)\n",
    "    xgb_F.fit(X_train_F, y_train)\n",
    "\n",
    "    with open(os.path.join(model_dir, \"xgb_new_features_pca_noaug_norm.pickle\"), \"wb\") as f:\n",
    "        pickle.dump(xgb_F, f)\n",
    "\n",
    "# =========================\n",
    "# SCÉNARIO G –– Z‑SCORE, PCA, AUG\n",
    "# =========================\n",
    "if G:\n",
    "    pca_G = PCA(n_components=0.98)\n",
    "    X_train_G = pca_G.fit_transform(X_train_aug_norm)\n",
    "    X_test_G  = pca_G.transform(X_test_aug_norm)\n",
    "\n",
    "    with open(os.path.join(model_dir, \"pca_aug_norm.pickle\"), \"wb\") as f:\n",
    "        pickle.dump(pca_G, f)\n",
    "\n",
    "    xgb_G = XGBClassifier(n_estimators=n_estimators, max_depth=max_depth, learning_rate=learning_rate,\n",
    "                          subsample=subsample, colsample_bytree=colsample_bytree,\n",
    "                          eval_metric='mlogloss', random_state=42)\n",
    "    xgb_G.fit(X_train_G, y_train_aug)\n",
    "\n",
    "    with open(os.path.join(model_dir, \"xgb_new_features_pca_aug_norm.pickle\"), \"wb\") as f:\n",
    "        pickle.dump(xgb_G, f)\n",
    "\n",
    "# =========================\n",
    "# SCÉNARIO H –– Z‑SCORE, AUG, NOPCA\n",
    "# =========================\n",
    "if H:\n",
    "    xgb_H = XGBClassifier(n_estimators=n_estimators, max_depth=max_depth, learning_rate=learning_rate,\n",
    "                          subsample=subsample, colsample_bytree=colsample_bytree,\n",
    "                          eval_metric='mlogloss', random_state=42)\n",
    "    xgb_H.fit(X_train_aug_norm, y_train_aug)\n",
    "\n",
    "    with open(os.path.join(model_dir, \"xgb_new_features_nopca_aug_norm.pickle\"), \"wb\") as f:\n",
    "        pickle.dump(xgb_H, f)\n",
    "\n",
    "# ==========================================================\n",
    "# FONCTION D'ÉVALUATION COMMUNE\n",
    "# ==========================================================\n",
    "def evaluate_model(model, X_test, y_test, description):\n",
    "    preds = model.predict(X_test)\n",
    "    classes = np.unique(y_test)\n",
    "\n",
    "    precision = precision_score(y_test, preds, average=None, labels=classes)\n",
    "    recall    = recall_score(y_test,    preds, average=None, labels=classes)\n",
    "    cm        = confusion_matrix(y_test, preds, labels=classes)\n",
    "\n",
    "    test_acc_per_class = [cm[i, i] / cm[i, :].sum() for i in range(len(classes))]\n",
    "    cv_acc = cross_val_score(model, X_test, y_test, cv=5, scoring='accuracy').mean()\n",
    "\n",
    "    print(f\"\\n=== {description} ===\")\n",
    "    print(f\"Overall Test Accuracy: {np.mean(preds == y_test):.4f}\")\n",
    "    print(f\"Mean CV Accuracy  (5‑fold): {cv_acc:.4f}\")\n",
    "    print(\"Per‑class metrics:\")\n",
    "    for i, cls in enumerate(classes):\n",
    "        print(f\"  Class {cls}: Precision={precision[i]:.4f}, Recall={recall[i]:.4f}, Accuracy={test_acc_per_class[i]:.4f}\")\n",
    "\n",
    "# ==========================================================\n",
    "# ÉVALUATION DE TOUTES LES VARIANTES\n",
    "# ==========================================================\n",
    "if TEST_SET:\n",
    "    if A: evaluate_model(xgb_A, X_test_A, y_test, \"Scenario A : PCA NOAUG NONORM\")\n",
    "    if B: evaluate_model(xgb_B, X_test,    y_test, \"Scenario B : NOPCA NOAUG NONORM\")\n",
    "    if C: evaluate_model(xgb_C, X_test_C,  y_test_aug, \"Scenario C : PCA AUG NONORM\")\n",
    "    if D: evaluate_model(xgb_D, X_test_aug, y_test_aug, \"Scenario D : NOPCA AUG NONORM\")\n",
    "    if E: evaluate_model(xgb_E, X_test_norm,    y_test, \"Scenario E : NOPCA NOAUG NORM\")\n",
    "    if F: evaluate_model(xgb_F, X_test_F,  y_test, \"Scenario F : PCA NOAUG NORM\")\n",
    "    if G: evaluate_model(xgb_G, X_test_G,  y_test_aug, \"Scenario G : PCA AUG NORM\")\n",
    "    if H: evaluate_model(xgb_H, X_test_aug_norm, y_test_aug, \"Scenario H : NOPCA AUG NORM\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HYPERPARAMETER TUNING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Bayesian Optimization...\n",
      "|   iter    |  target   | colsam... | learni... | max_depth | n_esti... | subsample |\n",
      "-------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 73\u001b[0m\n\u001b[1;32m     70\u001b[0m best_score_so_far \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1.0\u001b[39m\n\u001b[1;32m     71\u001b[0m early_stop_threshold \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.90\u001b[39m\n\u001b[0;32m---> 73\u001b[0m \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmaximize\u001b[49m\u001b[43m(\u001b[49m\u001b[43minit_points\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minit_points\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_iter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_iter\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, res \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(optimizer\u001b[38;5;241m.\u001b[39mres):\n\u001b[1;32m     76\u001b[0m     score \u001b[38;5;241m=\u001b[39m res[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtarget\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[0;32m~/master_projet_elec/LELEC210X/.venv/lib/python3.9/site-packages/bayes_opt/bayesian_optimization.py:338\u001b[0m, in \u001b[0;36mBayesianOptimization.maximize\u001b[0;34m(self, init_points, n_iter)\u001b[0m\n\u001b[1;32m    336\u001b[0m     x_probe \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msuggest()\n\u001b[1;32m    337\u001b[0m     iteration \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m--> 338\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprobe\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_probe\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlazy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    340\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bounds_transformer \u001b[38;5;129;01mand\u001b[39;00m iteration \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    341\u001b[0m     \u001b[38;5;66;03m# The bounds transformer should only modify the bounds after\u001b[39;00m\n\u001b[1;32m    342\u001b[0m     \u001b[38;5;66;03m# the init_points points (only for the true iterations)\u001b[39;00m\n\u001b[1;32m    343\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mset_bounds(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bounds_transformer\u001b[38;5;241m.\u001b[39mtransform(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_space))\n",
      "File \u001b[0;32m~/master_projet_elec/LELEC210X/.venv/lib/python3.9/site-packages/bayes_opt/bayesian_optimization.py:270\u001b[0m, in \u001b[0;36mBayesianOptimization.probe\u001b[0;34m(self, params, lazy)\u001b[0m\n\u001b[1;32m    268\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_queue\u001b[38;5;241m.\u001b[39mappend(params)\n\u001b[1;32m    269\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 270\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_space\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprobe\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    271\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdispatch(Events\u001b[38;5;241m.\u001b[39mOPTIMIZATION_STEP)\n",
      "File \u001b[0;32m~/master_projet_elec/LELEC210X/.venv/lib/python3.9/site-packages/bayes_opt/target_space.py:418\u001b[0m, in \u001b[0;36mTargetSpace.probe\u001b[0;34m(self, params)\u001b[0m\n\u001b[1;32m    416\u001b[0m     error_msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo target function has been provided.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    417\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(error_msg)\n\u001b[0;32m--> 418\u001b[0m target \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mdict_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    420\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_constraint \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    421\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mregister(x, target)\n",
      "Cell \u001b[0;32mIn[11], line 47\u001b[0m, in \u001b[0;36mxgb_cv\u001b[0;34m(n_estimators, max_depth, learning_rate, subsample, colsample_bytree)\u001b[0m\n\u001b[1;32m     35\u001b[0m max_depth \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(max_depth)\n\u001b[1;32m     37\u001b[0m model \u001b[38;5;241m=\u001b[39m XGBClassifier(\n\u001b[1;32m     38\u001b[0m     n_estimators\u001b[38;5;241m=\u001b[39mn_estimators,\n\u001b[1;32m     39\u001b[0m     max_depth\u001b[38;5;241m=\u001b[39mmax_depth,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     44\u001b[0m     random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m\n\u001b[1;32m     45\u001b[0m )\n\u001b[0;32m---> 47\u001b[0m cv_scores \u001b[38;5;241m=\u001b[39m \u001b[43mcross_val_score\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcv\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscoring\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43maccuracy\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m cv_scores\u001b[38;5;241m.\u001b[39mmean()\n",
      "File \u001b[0;32m~/master_projet_elec/LELEC210X/.venv/lib/python3.9/site-packages/sklearn/utils/_param_validation.py:216\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    210\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    211\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m    212\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m    213\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m    214\u001b[0m         )\n\u001b[1;32m    215\u001b[0m     ):\n\u001b[0;32m--> 216\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    217\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[1;32m    219\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[1;32m    220\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[1;32m    221\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[1;32m    222\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[1;32m    223\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    224\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    225\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[1;32m    226\u001b[0m     )\n",
      "File \u001b[0;32m~/master_projet_elec/LELEC210X/.venv/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:684\u001b[0m, in \u001b[0;36mcross_val_score\u001b[0;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, params, pre_dispatch, error_score)\u001b[0m\n\u001b[1;32m    681\u001b[0m \u001b[38;5;66;03m# To ensure multimetric format is not supported\u001b[39;00m\n\u001b[1;32m    682\u001b[0m scorer \u001b[38;5;241m=\u001b[39m check_scoring(estimator, scoring\u001b[38;5;241m=\u001b[39mscoring)\n\u001b[0;32m--> 684\u001b[0m cv_results \u001b[38;5;241m=\u001b[39m \u001b[43mcross_validate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    685\u001b[0m \u001b[43m    \u001b[49m\u001b[43mestimator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    686\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    687\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    688\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgroups\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    689\u001b[0m \u001b[43m    \u001b[49m\u001b[43mscoring\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mscore\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mscorer\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    690\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcv\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    691\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    692\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    693\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    694\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpre_dispatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpre_dispatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    695\u001b[0m \u001b[43m    \u001b[49m\u001b[43merror_score\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merror_score\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    696\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    697\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m cv_results[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_score\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m~/master_projet_elec/LELEC210X/.venv/lib/python3.9/site-packages/sklearn/utils/_param_validation.py:216\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    210\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    211\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m    212\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m    213\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m    214\u001b[0m         )\n\u001b[1;32m    215\u001b[0m     ):\n\u001b[0;32m--> 216\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    217\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[1;32m    219\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[1;32m    220\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[1;32m    221\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[1;32m    222\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[1;32m    223\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    224\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    225\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[1;32m    226\u001b[0m     )\n",
      "File \u001b[0;32m~/master_projet_elec/LELEC210X/.venv/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:411\u001b[0m, in \u001b[0;36mcross_validate\u001b[0;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, params, pre_dispatch, return_train_score, return_estimator, return_indices, error_score)\u001b[0m\n\u001b[1;32m    408\u001b[0m \u001b[38;5;66;03m# We clone the estimator to make sure that all the folds are\u001b[39;00m\n\u001b[1;32m    409\u001b[0m \u001b[38;5;66;03m# independent, and that it is pickle-able.\u001b[39;00m\n\u001b[1;32m    410\u001b[0m parallel \u001b[38;5;241m=\u001b[39m Parallel(n_jobs\u001b[38;5;241m=\u001b[39mn_jobs, verbose\u001b[38;5;241m=\u001b[39mverbose, pre_dispatch\u001b[38;5;241m=\u001b[39mpre_dispatch)\n\u001b[0;32m--> 411\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mparallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    412\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_fit_and_score\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    413\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    414\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    415\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    416\u001b[0m \u001b[43m        \u001b[49m\u001b[43mscorer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscorers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    417\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    418\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtest\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    419\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    420\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparameters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    421\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfit_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrouted_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mestimator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    422\u001b[0m \u001b[43m        \u001b[49m\u001b[43mscore_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrouted_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscorer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscore\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    423\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_train_score\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_train_score\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    424\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_times\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    425\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_estimator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_estimator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    426\u001b[0m \u001b[43m        \u001b[49m\u001b[43merror_score\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merror_score\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    427\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    428\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mindices\u001b[49m\n\u001b[1;32m    429\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    431\u001b[0m _warn_or_raise_about_fit_failures(results, error_score)\n\u001b[1;32m    433\u001b[0m \u001b[38;5;66;03m# For callable scoring, the return type is only know after calling. If the\u001b[39;00m\n\u001b[1;32m    434\u001b[0m \u001b[38;5;66;03m# return type is a dictionary, the error scores can now be inserted with\u001b[39;00m\n\u001b[1;32m    435\u001b[0m \u001b[38;5;66;03m# the correct key.\u001b[39;00m\n",
      "File \u001b[0;32m~/master_projet_elec/LELEC210X/.venv/lib/python3.9/site-packages/sklearn/utils/parallel.py:77\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     72\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[1;32m     73\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     74\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[1;32m     75\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[1;32m     76\u001b[0m )\n\u001b[0;32m---> 77\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/master_projet_elec/LELEC210X/.venv/lib/python3.9/site-packages/joblib/parallel.py:1918\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1916\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_sequential_output(iterable)\n\u001b[1;32m   1917\u001b[0m     \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[0;32m-> 1918\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1920\u001b[0m \u001b[38;5;66;03m# Let's create an ID that uniquely identifies the current call. If the\u001b[39;00m\n\u001b[1;32m   1921\u001b[0m \u001b[38;5;66;03m# call is interrupted early and that the same instance is immediately\u001b[39;00m\n\u001b[1;32m   1922\u001b[0m \u001b[38;5;66;03m# re-used, this id will be used to prevent workers that were\u001b[39;00m\n\u001b[1;32m   1923\u001b[0m \u001b[38;5;66;03m# concurrently finalizing a task from the previous call to run the\u001b[39;00m\n\u001b[1;32m   1924\u001b[0m \u001b[38;5;66;03m# callback.\u001b[39;00m\n\u001b[1;32m   1925\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n",
      "File \u001b[0;32m~/master_projet_elec/LELEC210X/.venv/lib/python3.9/site-packages/joblib/parallel.py:1847\u001b[0m, in \u001b[0;36mParallel._get_sequential_output\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1845\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_batches \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1846\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m-> 1847\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1848\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_completed_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1849\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprint_progress()\n",
      "File \u001b[0;32m~/master_projet_elec/LELEC210X/.venv/lib/python3.9/site-packages/sklearn/utils/parallel.py:139\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    137\u001b[0m     config \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m    138\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig):\n\u001b[0;32m--> 139\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/master_projet_elec/LELEC210X/.venv/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:866\u001b[0m, in \u001b[0;36m_fit_and_score\u001b[0;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, score_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, split_progress, candidate_progress, error_score)\u001b[0m\n\u001b[1;32m    864\u001b[0m         estimator\u001b[38;5;241m.\u001b[39mfit(X_train, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\n\u001b[1;32m    865\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 866\u001b[0m         \u001b[43mestimator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    868\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m    869\u001b[0m     \u001b[38;5;66;03m# Note fit time as time until error\u001b[39;00m\n\u001b[1;32m    870\u001b[0m     fit_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start_time\n",
      "File \u001b[0;32m~/master_projet_elec/LELEC210X/.venv/lib/python3.9/site-packages/xgboost/core.py:726\u001b[0m, in \u001b[0;36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    724\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sig\u001b[38;5;241m.\u001b[39mparameters, args):\n\u001b[1;32m    725\u001b[0m     kwargs[k] \u001b[38;5;241m=\u001b[39m arg\n\u001b[0;32m--> 726\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/master_projet_elec/LELEC210X/.venv/lib/python3.9/site-packages/xgboost/sklearn.py:1599\u001b[0m, in \u001b[0;36mXGBClassifier.fit\u001b[0;34m(self, X, y, sample_weight, base_margin, eval_set, verbose, xgb_model, sample_weight_eval_set, base_margin_eval_set, feature_weights)\u001b[0m\n\u001b[1;32m   1579\u001b[0m model, metric, params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_configure_fit(xgb_model, params)\n\u001b[1;32m   1580\u001b[0m train_dmatrix, evals \u001b[38;5;241m=\u001b[39m _wrap_evaluation_matrices(\n\u001b[1;32m   1581\u001b[0m     missing\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmissing,\n\u001b[1;32m   1582\u001b[0m     X\u001b[38;5;241m=\u001b[39mX,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1596\u001b[0m     feature_types\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeature_types,\n\u001b[1;32m   1597\u001b[0m )\n\u001b[0;32m-> 1599\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_Booster \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1600\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1601\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_dmatrix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1602\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_num_boosting_rounds\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1603\u001b[0m \u001b[43m    \u001b[49m\u001b[43mevals\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mevals\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1604\u001b[0m \u001b[43m    \u001b[49m\u001b[43mearly_stopping_rounds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mearly_stopping_rounds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1605\u001b[0m \u001b[43m    \u001b[49m\u001b[43mevals_result\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mevals_result\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1606\u001b[0m \u001b[43m    \u001b[49m\u001b[43mobj\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1607\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcustom_metric\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetric\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1608\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1609\u001b[0m \u001b[43m    \u001b[49m\u001b[43mxgb_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1610\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1611\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1613\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobjective):\n\u001b[1;32m   1614\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobjective \u001b[38;5;241m=\u001b[39m params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mobjective\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m~/master_projet_elec/LELEC210X/.venv/lib/python3.9/site-packages/xgboost/core.py:726\u001b[0m, in \u001b[0;36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    724\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sig\u001b[38;5;241m.\u001b[39mparameters, args):\n\u001b[1;32m    725\u001b[0m     kwargs[k] \u001b[38;5;241m=\u001b[39m arg\n\u001b[0;32m--> 726\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/master_projet_elec/LELEC210X/.venv/lib/python3.9/site-packages/xgboost/training.py:181\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(params, dtrain, num_boost_round, evals, obj, feval, maximize, early_stopping_rounds, evals_result, verbose_eval, xgb_model, callbacks, custom_metric)\u001b[0m\n\u001b[1;32m    179\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cb_container\u001b[38;5;241m.\u001b[39mbefore_iteration(bst, i, dtrain, evals):\n\u001b[1;32m    180\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m--> 181\u001b[0m \u001b[43mbst\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43miteration\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfobj\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cb_container\u001b[38;5;241m.\u001b[39mafter_iteration(bst, i, dtrain, evals):\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/master_projet_elec/LELEC210X/.venv/lib/python3.9/site-packages/xgboost/core.py:2101\u001b[0m, in \u001b[0;36mBooster.update\u001b[0;34m(self, dtrain, iteration, fobj)\u001b[0m\n\u001b[1;32m   2097\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_assign_dmatrix_features(dtrain)\n\u001b[1;32m   2099\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m fobj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   2100\u001b[0m     _check_call(\n\u001b[0;32m-> 2101\u001b[0m         \u001b[43m_LIB\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mXGBoosterUpdateOneIter\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2102\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mc_int\u001b[49m\u001b[43m(\u001b[49m\u001b[43miteration\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtrain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle\u001b[49m\n\u001b[1;32m   2103\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2104\u001b[0m     )\n\u001b[1;32m   2105\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2106\u001b[0m     pred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredict(dtrain, output_margin\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import cross_val_score, train_test_split\n",
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score\n",
    "from bayes_opt import BayesianOptimization\n",
    "\n",
    "# Your custom accuracy function\n",
    "from classification.utils.utils import accuracy\n",
    "\n",
    "# --- CONFIG FLAGS ---\n",
    "NORMALIZATION = True   # utilise X_train_aug_norm\n",
    "TRANSFORMATION = True  # utilise jeu de données augmenté\n",
    "\n",
    "# === Chargement des données ===\n",
    "if TRANSFORMATION and NORMALIZATION:\n",
    "    X = np.load(os.path.join(fm_dir, \"X_train_aug_norm.npy\" if NORMALIZATION else \"X_train_aug.npy\"))\n",
    "    y = np.load(os.path.join(fm_dir, \"y_train_aug.npy\"), allow_pickle=True)\n",
    "elif TRANSFORMATION:\n",
    "    X = np.load(os.path.join(fm_dir, \"X_train_aug.npy\" if NORMALIZATION else \"X_train.npy\"))\n",
    "    y = np.load(os.path.join(fm_dir, \"y_train_aug.npy\"), allow_pickle=True)\n",
    "elif NORMALIZATION:\n",
    "    X = np.load(os.path.join(fm_dir, \"X_train_norm.npy\" if NORMALIZATION else \"X_train.npy\"))\n",
    "    y = np.load(os.path.join(fm_dir, \"y_train.npy\"), allow_pickle=True)\n",
    "else:\n",
    "    X = np.load(os.path.join(fm_dir, \"X_train.npy\"))\n",
    "    y = np.load(os.path.join(fm_dir, \"y_train.npy\"), allow_pickle=True)\n",
    "\n",
    "# --- STEP 2: Define the Objective Function for Bayesian Optimization ---\n",
    "def xgb_cv(n_estimators, max_depth, learning_rate, subsample, colsample_bytree):\n",
    "    n_estimators = int(n_estimators)\n",
    "    max_depth = int(max_depth)\n",
    "\n",
    "    model = XGBClassifier(\n",
    "        n_estimators=n_estimators,\n",
    "        max_depth=max_depth,\n",
    "        learning_rate=learning_rate,\n",
    "        subsample=subsample,\n",
    "        colsample_bytree=colsample_bytree,\n",
    "        eval_metric='mlogloss',\n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "    cv_scores = cross_val_score(model, X, y, cv=5, scoring='accuracy')\n",
    "    return cv_scores.mean()\n",
    "\n",
    "# --- STEP 3: Define hyperparameter search space ---\n",
    "pbounds = {\n",
    "    'n_estimators': (50, 400),\n",
    "    'max_depth': (2, 15),\n",
    "    'learning_rate': (0.01, 0.3),\n",
    "    'subsample': (0.5, 1),\n",
    "    'colsample_bytree': (0.5, 1)\n",
    "}\n",
    "\n",
    "optimizer = BayesianOptimization(\n",
    "    f=xgb_cv,\n",
    "    pbounds=pbounds,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# --- STEP 4: Run Bayesian Optimization ---\n",
    "init_points = 3\n",
    "n_iter = 20\n",
    "print(\"Starting Bayesian Optimization...\")\n",
    "\n",
    "best_score_so_far = -1.0\n",
    "early_stop_threshold = 0.90\n",
    "\n",
    "optimizer.maximize(init_points=init_points, n_iter=n_iter)\n",
    "\n",
    "for i, res in enumerate(optimizer.res):\n",
    "    score = res['target']\n",
    "    print(f\"Iteration {i+1}, CV Accuracy: {score:.4f}, Parameters: {res['params']}\")\n",
    "    if score > best_score_so_far:\n",
    "        best_score_so_far = score\n",
    "    if best_score_so_far > early_stop_threshold:\n",
    "        print(f\"\\nEarly stopping: Found cross-validation accuracy above {early_stop_threshold}\\n\")\n",
    "        break\n",
    "\n",
    "# --- STEP 5: Retrieve best hyperparameters ---\n",
    "best_params = optimizer.max['params']\n",
    "best_n_estimators = int(best_params['n_estimators'])\n",
    "best_max_depth = int(best_params['max_depth'])\n",
    "best_learning_rate = best_params['learning_rate']\n",
    "best_subsample = best_params['subsample']\n",
    "best_colsample_bytree = best_params['colsample_bytree']\n",
    "\n",
    "print(\"\\n=== BEST HYPERPARAMETERS FOUND ===\")\n",
    "print(f\"n_estimators = {best_n_estimators}\")\n",
    "print(f\"max_depth = {best_max_depth}\")\n",
    "print(f\"learning_rate = {best_learning_rate:.4f}\")\n",
    "print(f\"subsample = {best_subsample:.4f}\")\n",
    "print(f\"colsample_bytree = {best_colsample_bytree:.4f}\")\n",
    "print(f\"CV Accuracy = {optimizer.max['target']:.4f}\")\n",
    "\n",
    "# --- STEP 6: Final evaluation on hold-out test set ---\n",
    "X_test = np.load(os.path.join(fm_dir, \"X_test_aug_norm.npy\" if (TRANSFORMATION and NORMALIZATION) else \n",
    "                              \"X_test_aug.npy\" if TRANSFORMATION else\n",
    "                              \"X_test_norm.npy\" if NORMALIZATION else\n",
    "                              \"X_test.npy\"))\n",
    "y_test = np.load(os.path.join(fm_dir, \"y_test_aug.npy\" if TRANSFORMATION else \"y_test.npy\"), allow_pickle=True)\n",
    "\n",
    "final_model = XGBClassifier(\n",
    "    n_estimators=best_n_estimators,\n",
    "    max_depth=best_max_depth,\n",
    "    learning_rate=best_learning_rate,\n",
    "    subsample=best_subsample,\n",
    "    colsample_bytree=best_colsample_bytree,\n",
    "    eval_metric='mlogloss',\n",
    "    random_state=999\n",
    ")\n",
    "\n",
    "final_model.fit(X, y)\n",
    "y_pred = final_model.predict(X_test)\n",
    "test_acc = accuracy(y_pred, y_test)\n",
    "\n",
    "print(\"\\n=== FINAL EVALUATION ON HOLDOUT TEST SET ===\")\n",
    "print(f\"Test Accuracy: {test_acc:.4f}\")\n",
    "\n",
    "# --- Save best model and optionally normalization stats ---\n",
    "os.makedirs(\"models\", exist_ok=True)\n",
    "with open(\"models/xgb_bayesopt_best_model.pkl\", \"wb\") as f:\n",
    "    pickle.dump(final_model, f)\n",
    "\n",
    "if NORMALIZATION:\n",
    "    with open(\"models/zscore_normalization_stats.pickle\", \"wb\") as f:\n",
    "        pickle.dump((np.mean(X, axis=0), np.std(X, axis=0)), f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
