{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "\"Machine learning tools\"\n",
    "import pickle\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "\n",
    "\n",
    "from classification.datasets import Dataset\n",
    "from classification.utils.audio_student import AudioUtil, Feature_vector_DS\n",
    "\n",
    "from classification.utils.plots import (\n",
    "    plot_decision_boundaries,\n",
    "    plot_specgram,\n",
    "    show_confusion_matrix,\n",
    ")\n",
    "from classification.utils.utils import accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chainsaw\n",
      "fire\n",
      "fireworks\n",
      "gunshot\n"
     ]
    }
   ],
   "source": [
    "### TO RUN\n",
    "dataset = Dataset()\n",
    "classnames = dataset.list_classes()\n",
    "\n",
    "print(\"\\n\".join(classnames))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TO RUN\n",
    "fm_dir = \"data/feature_matrices/\"  # where to save the features matrices\n",
    "model_dir = \"data/models/xgb_new_features\"  # where to save the models\n",
    "os.makedirs(fm_dir, exist_ok=True)\n",
    "os.makedirs(model_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TO RUN\n",
    "\n",
    "\"Creation of the dataset\"\n",
    "myds = Feature_vector_DS(dataset, Nft=512, nmel=20, duration=950, shift_pct=0.0)\n",
    "\n",
    "\"Some attributes...\"\n",
    "myds.nmel\n",
    "myds.duration\n",
    "myds.shift_pct\n",
    "myds.sr\n",
    "myds.data_aug\n",
    "myds.ncol\n",
    "\n",
    "idx = 0\n",
    "\n",
    "# XGBOOST PARAMETERS\n",
    "n_estimators = 130\n",
    "max_depth = 2\n",
    "learning_rate = 0.2286\n",
    "subsample = 0.5984\n",
    "colsample_bytree = 0.6445"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the training matrix : (182, 400)\n",
      "Shape of the test matrix : (79, 400)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "from sklearn.calibration import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "train_pct = 0.7\n",
    "data_aug_factor = 1\n",
    "featveclen = len(myds[\"fire\", 0, \"\", \"\"])  # Same for all classes\n",
    "classnames = [\"chainsaw\", \"fire\", \"fireworks\", \"gunshot\"]  # Or wherever you store class names\n",
    "nclass = len(classnames)\n",
    "\n",
    "# Determine number of samples per class\n",
    "naudio_per_class = {\"chainsaw\" : 70, \"fire\" : 76, \"fireworks\" : 75, \"gunshot\" : 40}\n",
    "\n",
    "\n",
    "# Allocate feature matrix\n",
    "total_samples_basic = sum(naudio_per_class[c] for c in classnames)\n",
    "X_basic = np.zeros((total_samples_basic, featveclen))\n",
    "y_basic = np.zeros((total_samples_basic), dtype=object)\n",
    "total_samples_basic\n",
    "# Fill feature matrix\n",
    "idx = 0\n",
    "for class_idx, classname in enumerate(classnames):\n",
    "    for i in range(naudio_per_class[classname]):\n",
    "        featvec = myds[classname, i, \"\", \"\"]\n",
    "        X_basic[idx, :] = featvec\n",
    "        y_basic[idx] = classname\n",
    "        idx += 1\n",
    "\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_basic, y_basic, test_size=0.3, random_state=42)\n",
    "scaler = StandardScaler()\n",
    "X_train_norm = scaler.fit_transform(X_train)\n",
    "X_test_norm = scaler.transform(X_test)\n",
    "\n",
    "le           = LabelEncoder()\n",
    "y_train     = le.fit_transform(y_train)\n",
    "y_test      = le.transform(y_test)\n",
    "\n",
    "# Save the feature matrix and labels\n",
    "np.save(os.path.join(fm_dir, \"X_train.npy\"), X_train)\n",
    "np.save(os.path.join(fm_dir, \"X_test.npy\"), X_test)\n",
    "np.save(os.path.join(fm_dir, \"y_train.npy\"), y_train)\n",
    "np.save(os.path.join(fm_dir, \"y_test.npy\"), y_test)\n",
    "np.save(os.path.join(fm_dir, \"X_train_norm.npy\"), X_train_norm)\n",
    "np.save(os.path.join(fm_dir, \"X_test_norm.npy\"), X_test_norm)\n",
    "\n",
    "print(f\"Shape of the training matrix : {X_train.shape}\")\n",
    "print(f\"Shape of the test matrix : {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now create a new augmented dataset and observe if the classification results improve. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of transformations :  3\n",
      "Shape of the training matrix : (546, 400)\n",
      "Shape of the test matrix : (237, 400)\n",
      "------------------------------------------------------------\n",
      "Transformations: ['original', 'noise', 'shifting']. Labels aligned dynamically with class sizes.\n"
     ]
    }
   ],
   "source": [
    "### AUGMENTED DATASET\n",
    "list_augmentation = [\"original\", \"noise\", \"shifting\"]\n",
    "myds.mod_data_aug(list_augmentation)\n",
    "print(\"Number of transformations : \", myds.data_aug_factor)\n",
    "\n",
    "\n",
    "# Préparer les splits\n",
    "X_train_list, y_train_list = [], []\n",
    "X_test_list,  y_test_list  = [], []\n",
    "\n",
    "for classname in classnames:\n",
    "    n = naudio_per_class[classname]\n",
    "\n",
    "    # Création des indices de base pour les sons originaux\n",
    "    original_indices = list(range(n))\n",
    "    train_idx, test_idx = train_test_split(original_indices, test_size=0.3, random_state=42)\n",
    "\n",
    "    for i in train_idx:\n",
    "        for aug in list_augmentation:\n",
    "            featvec = myds[classname, i, aug, \"\"]\n",
    "            X_train_list.append(featvec)\n",
    "            y_train_list.append(classname)\n",
    "\n",
    "    for i in test_idx:\n",
    "        for aug in list_augmentation:\n",
    "            featvec = myds[classname, i, aug, \"\"]\n",
    "            X_test_list.append(featvec)\n",
    "            y_test_list.append(classname)\n",
    "\n",
    "# Conversion en tableaux numpy\n",
    "X_train_aug = np.array(X_train_list)\n",
    "y_train_aug = np.array(y_train_list, dtype=object)\n",
    "\n",
    "X_test_aug = np.array(X_test_list)\n",
    "y_test_aug = np.array(y_test_list, dtype=object)\n",
    "\n",
    "# Save features and labels\n",
    "scaler = StandardScaler()\n",
    "X_train_aug_norm = scaler.fit_transform(X_train_aug)\n",
    "X_test_aug_norm = scaler.fit_transform(X_test_aug)\n",
    "\n",
    "y_train_aug     = le.fit_transform(y_train_aug)\n",
    "y_test_aug      = le.transform(y_test_aug)\n",
    "\n",
    "np.save(os.path.join(fm_dir, \"X_train_aug.npy\"), X_train_aug)\n",
    "np.save(os.path.join(fm_dir, \"X_test_aug.npy\"), X_test_aug)\n",
    "np.save(os.path.join(fm_dir, \"y_train_aug.npy\"), y_train_aug)\n",
    "np.save(os.path.join(fm_dir, \"y_test_aug.npy\"), y_test_aug)\n",
    "np.save(os.path.join(fm_dir, \"X_train_aug_norm.npy\"), X_train_aug_norm)\n",
    "np.save(os.path.join(fm_dir, \"X_test_aug_norm.npy\"), X_test_aug_norm)\n",
    "\n",
    "print(f\"Shape of the training matrix : {X_train_aug.shape}\")\n",
    "print(f\"Shape of the test matrix : {X_test_aug.shape}\")\n",
    "print(f\"------------------------------------------------------------\")\n",
    "print(f\"Transformations: {list_augmentation}. Labels aligned dynamically with class sizes.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "NEW_FEATURES = False\n",
    "if NEW_FEATURES:\n",
    "    from classification.utils.plots import plot_specgram_textlabel\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "    def add_energy_feature(X):\n",
    "        X_new = []\n",
    "        for i in range(len(X)):\n",
    "            melvec = X[i, :]\n",
    "            mel_spec = melvec.reshape(20, 20)\n",
    "            energy_line = np.sum(mel_spec, axis=0)  # somme des lignes pour chaque colonne\n",
    "            mel_spec_aug = np.vstack([mel_spec, energy_line])  # ajoute comme 21ème ligne\n",
    "            X_new.append(mel_spec_aug.flatten())  # retransforme en vecteur de taille 420\n",
    "        return np.array(X_new)\n",
    "\n",
    "\n",
    "    X_basic = add_energy_feature(X_basic)\n",
    "    X_basic_aug = add_energy_feature(X_basic_aug)\n",
    "    X_basic_norm = add_energy_feature(X_basic_norm)\n",
    "    X_basic_aug_norm = add_energy_feature(X_basic_aug_norm)\n",
    "\n",
    "    # === Save the new feature matrices ===\n",
    "    np.save(fm_dir + \"X_basic_aug_norm.npy\", X_basic_aug_norm)\n",
    "    np.save(fm_dir + \"X_basic_norm.npy\", X_basic_norm)\n",
    "    np.save(fm_dir + \"X_basic_aug.npy\", X_basic_aug)\n",
    "    np.save(fm_dir + \"y_basic_aug.npy\", y_basic_aug)\n",
    "    np.save(fm_dir + \"X_basic.npy\", X_basic)\n",
    "    np.save(fm_dir + \"y_basic.npy\", y_basic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASIC = False\n",
    "if BASIC:\n",
    "    # Charger les données\n",
    "    X = np.load(os.path.join(fm_dir, \"X_basic.npy\"), allow_pickle=True)\n",
    "    y = np.load(os.path.join(fm_dir, \"y_basic.npy\"), allow_pickle=True)\n",
    "\n",
    "    # Dossier où sauvegarder les images\n",
    "    save_dir = os.path.join(\"src/classification/soundfiles_melspec\")\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "    # Initialiser les compteurs par classe\n",
    "    class_counters = {}\n",
    "\n",
    "    for i in range(len(X)):\n",
    "        melspec = X[i]\n",
    "        class_of_spec = y[i]\n",
    "\n",
    "        if class_of_spec not in class_counters:\n",
    "            class_counters[class_of_spec] = 0\n",
    "        class_idx = class_counters[class_of_spec]\n",
    "\n",
    "        fig, ax = plt.subplots()\n",
    "        plot_specgram_textlabel(  # ✅ fonction corrigée\n",
    "            melspec.reshape((21, 20)),\n",
    "            ax=ax,\n",
    "            is_mel=True,\n",
    "            title=f\"MEL Spectrogram - {class_of_spec} #{class_idx}\",\n",
    "            xlabel=\"Mel vector\",\n",
    "            textlabel=f\"{class_of_spec}\",\n",
    "        )\n",
    "        plt.tight_layout()\n",
    "        save_path = os.path.join(save_dir, f\"melspec_{class_of_spec}_{class_idx}.png\")\n",
    "        fig.savefig(save_path)\n",
    "        plt.close(fig)\n",
    "\n",
    "        class_counters[class_of_spec] += 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FINAL MODEL SAVE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Scenario A : PCA NOAUG NONORM ===\n",
      "Overall Test Accuracy: 0.7468\n",
      "Mean CV Accuracy  (5‑fold): 0.6208\n",
      "Per‑class metrics:\n",
      "  Class 0: Precision=0.7895, Recall=0.7895, Accuracy=0.7895\n",
      "  Class 1: Precision=0.7778, Recall=0.7500, Accuracy=0.7500\n",
      "  Class 2: Precision=0.5714, Recall=0.6316, Accuracy=0.6316\n",
      "  Class 3: Precision=0.9167, Recall=0.8462, Accuracy=0.8462\n",
      "\n",
      "=== Scenario B : NOPCA NOAUG NONORM ===\n",
      "Overall Test Accuracy: 0.8608\n",
      "Mean CV Accuracy  (5‑fold): 0.8108\n",
      "Per‑class metrics:\n",
      "  Class 0: Precision=0.8421, Recall=0.8421, Accuracy=0.8421\n",
      "  Class 1: Precision=0.8621, Recall=0.8929, Accuracy=0.8929\n",
      "  Class 2: Precision=0.8000, Recall=0.8421, Accuracy=0.8421\n",
      "  Class 3: Precision=1.0000, Recall=0.8462, Accuracy=0.8462\n",
      "\n",
      "=== Scenario C : PCA AUG NONORM ===\n",
      "Overall Test Accuracy: 0.7890\n",
      "Mean CV Accuracy  (5‑fold): 0.7011\n",
      "Per‑class metrics:\n",
      "  Class 0: Precision=0.7857, Recall=0.8730, Accuracy=0.8730\n",
      "  Class 1: Precision=0.8551, Recall=0.8551, Accuracy=0.8551\n",
      "  Class 2: Precision=0.6857, Recall=0.6957, Accuracy=0.6957\n",
      "  Class 3: Precision=0.8929, Recall=0.6944, Accuracy=0.6944\n",
      "\n",
      "=== Scenario D : NOPCA AUG NONORM ===\n",
      "Overall Test Accuracy: 0.8481\n",
      "Mean CV Accuracy  (5‑fold): 0.8101\n",
      "Per‑class metrics:\n",
      "  Class 0: Precision=0.7917, Recall=0.9048, Accuracy=0.9048\n",
      "  Class 1: Precision=0.9219, Recall=0.8551, Accuracy=0.8551\n",
      "  Class 2: Precision=0.7857, Recall=0.7971, Accuracy=0.7971\n",
      "  Class 3: Precision=0.9677, Recall=0.8333, Accuracy=0.8333\n",
      "\n",
      "=== Scenario E : NOPCA NOAUG NONORM (dup) ===\n",
      "Overall Test Accuracy: 0.8608\n",
      "Mean CV Accuracy  (5‑fold): 0.8108\n",
      "Per‑class metrics:\n",
      "  Class 0: Precision=0.8421, Recall=0.8421, Accuracy=0.8421\n",
      "  Class 1: Precision=0.8621, Recall=0.8929, Accuracy=0.8929\n",
      "  Class 2: Precision=0.8000, Recall=0.8421, Accuracy=0.8421\n",
      "  Class 3: Precision=1.0000, Recall=0.8462, Accuracy=0.8462\n",
      "\n",
      "=== Scenario F : PCA NOAUG NORM ===\n",
      "Overall Test Accuracy: 0.8101\n",
      "Mean CV Accuracy  (5‑fold): 0.6583\n",
      "Per‑class metrics:\n",
      "  Class 0: Precision=0.9231, Recall=0.6316, Accuracy=0.6316\n",
      "  Class 1: Precision=0.8125, Recall=0.9286, Accuracy=0.9286\n",
      "  Class 2: Precision=0.7143, Recall=0.7895, Accuracy=0.7895\n",
      "  Class 3: Precision=0.8462, Recall=0.8462, Accuracy=0.8462\n",
      "\n",
      "=== Scenario G : PCA AUG NORM ===\n",
      "Overall Test Accuracy: 0.7764\n",
      "Mean CV Accuracy  (5‑fold): 0.7384\n",
      "Per‑class metrics:\n",
      "  Class 0: Precision=0.7619, Recall=0.7619, Accuracy=0.7619\n",
      "  Class 1: Precision=0.8507, Recall=0.8261, Accuracy=0.8261\n",
      "  Class 2: Precision=0.7143, Recall=0.7246, Accuracy=0.7246\n",
      "  Class 3: Precision=0.7838, Recall=0.8056, Accuracy=0.8056\n",
      "\n",
      "=== Scenario H : NOPCA AUG NORM ===\n",
      "Overall Test Accuracy: 0.8523\n",
      "Mean CV Accuracy  (5‑fold): 0.8101\n",
      "Per‑class metrics:\n",
      "  Class 0: Precision=0.7722, Recall=0.9683, Accuracy=0.9683\n",
      "  Class 1: Precision=0.9492, Recall=0.8116, Accuracy=0.8116\n",
      "  Class 2: Precision=0.8308, Recall=0.7826, Accuracy=0.7826\n",
      "  Class 3: Precision=0.9118, Recall=0.8611, Accuracy=0.8611\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Config\n",
    "TEST_SET = True\n",
    "\n",
    "A = True  # PCA NOAUG NONORM\n",
    "B = True  # NOPCA NOAUG NONORM\n",
    "C = True  # PCA AUG NONORM\n",
    "D = True  # NOPCA AUG NONORM\n",
    "E = True  # NOPCA NOAUG NONORM\n",
    "F = True  # PCA NOAUG Z-SCORE\n",
    "G = True  # PCA AUG Z-SCORE\n",
    "H = True  # NOPCA AUG Z-SCORE\n",
    "\n",
    "\n",
    "# === Données NON augmentées ===\n",
    "X_train        = np.load(os.path.join(fm_dir, \"X_train.npy\"))\n",
    "X_test         = np.load(os.path.join(fm_dir, \"X_test.npy\"))\n",
    "y_train        = np.load(os.path.join(fm_dir, \"y_train.npy\"), allow_pickle=True)\n",
    "y_test         = np.load(os.path.join(fm_dir, \"y_test.npy\"), allow_pickle=True)\n",
    "\n",
    "X_train_norm   = np.load(os.path.join(fm_dir, \"X_train_norm.npy\"))\n",
    "X_test_norm    = np.load(os.path.join(fm_dir, \"X_test_norm.npy\"))\n",
    "\n",
    "# === Données AUGMENTÉES ===\n",
    "X_train_aug        = np.load(os.path.join(fm_dir, \"X_train_aug.npy\"))\n",
    "X_test_aug         = np.load(os.path.join(fm_dir, \"X_test_aug.npy\"))\n",
    "y_train_aug        = np.load(os.path.join(fm_dir, \"y_train_aug.npy\"), allow_pickle=True)\n",
    "y_test_aug         = np.load(os.path.join(fm_dir, \"y_test_aug.npy\"), allow_pickle=True)\n",
    "\n",
    "X_train_aug_norm   = np.load(os.path.join(fm_dir, \"X_train_aug_norm.npy\"))\n",
    "X_test_aug_norm    = np.load(os.path.join(fm_dir, \"X_test_aug_norm.npy\"))\n",
    "\n",
    "\n",
    "# =========================\n",
    "# SCÉNARIO A –– PCA, NO AUG, NON‑NORM\n",
    "# =========================\n",
    "if A:\n",
    "    pca_A = PCA(n_components=0.98)\n",
    "    X_train_A = pca_A.fit_transform(X_train)\n",
    "    X_test_A  = pca_A.transform(X_test)\n",
    "\n",
    "    with open(os.path.join(model_dir, \"pca_noaug_nonorm.pkl\"), \"wb\") as f:\n",
    "        pickle.dump(pca_A, f)\n",
    "\n",
    "    xgb_A = XGBClassifier(n_estimators=n_estimators, max_depth=max_depth, learning_rate=learning_rate,\n",
    "                          subsample=subsample, colsample_bytree=colsample_bytree,\n",
    "                          eval_metric='mlogloss', random_state=42)\n",
    "    xgb_A.fit(X_train_A, y_train)\n",
    "\n",
    "    with open(os.path.join(model_dir, \"xgb_pca_noaug_nonorm.pkl\"), \"wb\") as f:\n",
    "        pickle.dump(xgb_A, f)\n",
    "\n",
    "# =========================\n",
    "# SCÉNARIO B –– NOPCA, NO AUG, NON‑NORM\n",
    "# =========================\n",
    "if B:\n",
    "    xgb_B = XGBClassifier(n_estimators=n_estimators, max_depth=max_depth, learning_rate=learning_rate,\n",
    "                          subsample=subsample, colsample_bytree=colsample_bytree,\n",
    "                          eval_metric='mlogloss', random_state=42)\n",
    "    xgb_B.fit(X_train, y_train)\n",
    "\n",
    "    with open(os.path.join(model_dir, \"xgb_nopca_noaug_nonorm.pkl\"), \"wb\") as f:\n",
    "        pickle.dump(xgb_B, f)\n",
    "\n",
    "# =========================\n",
    "# SCÉNARIO C –– PCA, AUG, NON‑NORM\n",
    "# =========================\n",
    "if C:\n",
    "    pca_C = PCA(n_components=0.98)\n",
    "    X_train_C = pca_C.fit_transform(X_train_aug)\n",
    "    X_test_C  = pca_C.transform(X_test_aug)\n",
    "\n",
    "    with open(os.path.join(model_dir, \"pca_aug_nonorm.pkl\"), \"wb\") as f:\n",
    "        pickle.dump(pca_C, f)\n",
    "\n",
    "    xgb_C = XGBClassifier(n_estimators=n_estimators, max_depth=max_depth, learning_rate=learning_rate,\n",
    "                          subsample=subsample, colsample_bytree=colsample_bytree,\n",
    "                          eval_metric='mlogloss', random_state=42)\n",
    "    xgb_C.fit(X_train_C, y_train_aug)\n",
    "\n",
    "    with open(os.path.join(model_dir, \"xgb_pca_aug_nonorm.pkl\"), \"wb\") as f:\n",
    "        pickle.dump(xgb_C, f)\n",
    "\n",
    "# =========================\n",
    "# SCÉNARIO D –– NOPCA, AUG, NON‑NORM\n",
    "# =========================\n",
    "if D:\n",
    "    xgb_D = XGBClassifier(n_estimators=n_estimators, max_depth=max_depth, learning_rate=learning_rate,\n",
    "                          subsample=subsample, colsample_bytree=colsample_bytree,\n",
    "                          eval_metric='mlogloss', random_state=42)\n",
    "    xgb_D.fit(X_train_aug, y_train_aug)\n",
    "\n",
    "    with open(os.path.join(model_dir, \"xgb_nopca_aug_nonorm.pkl\"), \"wb\") as f:\n",
    "        pickle.dump(xgb_D, f)\n",
    "\n",
    "# =========================\n",
    "# SCÉNARIO E –– NOPCA, NO AUG, NORM\n",
    "# =========================\n",
    "if E:\n",
    "    xgb_E = XGBClassifier(n_estimators=n_estimators, max_depth=max_depth, learning_rate=learning_rate,\n",
    "                          subsample=subsample, colsample_bytree=colsample_bytree,\n",
    "                          eval_metric='mlogloss', random_state=42)\n",
    "    xgb_E.fit(X_train_norm, y_train)\n",
    "\n",
    "    with open(os.path.join(model_dir, \"xgb_nopca_noaug_nonorm_dup.pkl\"), \"wb\") as f:\n",
    "        pickle.dump(xgb_E, f)\n",
    "\n",
    "# =========================\n",
    "# SCÉNARIO F –– Z‑SCORE, PCA, NO AUG\n",
    "# =========================\n",
    "if F:\n",
    "    pca_F = PCA(n_components=0.98)\n",
    "    X_train_F = pca_F.fit_transform(X_train_norm)\n",
    "    X_test_F  = pca_F.transform(X_test_norm)\n",
    "\n",
    "    with open(os.path.join(model_dir, \"pca_noaug_norm.pkl\"), \"wb\") as f:\n",
    "        pickle.dump(pca_F, f)\n",
    "\n",
    "    xgb_F = XGBClassifier(n_estimators=n_estimators, max_depth=max_depth, learning_rate=learning_rate,\n",
    "                          subsample=subsample, colsample_bytree=colsample_bytree,\n",
    "                          eval_metric='mlogloss', random_state=42)\n",
    "    xgb_F.fit(X_train_F, y_train)\n",
    "\n",
    "    with open(os.path.join(model_dir, \"xgb_pca_noaug_norm.pkl\"), \"wb\") as f:\n",
    "        pickle.dump(xgb_F, f)\n",
    "\n",
    "# =========================\n",
    "# SCÉNARIO G –– Z‑SCORE, PCA, AUG\n",
    "# =========================\n",
    "if G:\n",
    "    pca_G = PCA(n_components=0.98)\n",
    "    X_train_G = pca_G.fit_transform(X_train_aug_norm)\n",
    "    X_test_G  = pca_G.transform(X_test_aug_norm)\n",
    "\n",
    "    with open(os.path.join(model_dir, \"pca_aug_norm.pkl\"), \"wb\") as f:\n",
    "        pickle.dump(pca_G, f)\n",
    "\n",
    "    xgb_G = XGBClassifier(n_estimators=n_estimators, max_depth=max_depth, learning_rate=learning_rate,\n",
    "                          subsample=subsample, colsample_bytree=colsample_bytree,\n",
    "                          eval_metric='mlogloss', random_state=42)\n",
    "    xgb_G.fit(X_train_G, y_train_aug)\n",
    "\n",
    "    with open(os.path.join(model_dir, \"xgb_pca_aug_norm.pkl\"), \"wb\") as f:\n",
    "        pickle.dump(xgb_G, f)\n",
    "\n",
    "# =========================\n",
    "# SCÉNARIO H –– Z‑SCORE, AUG, NOPCA\n",
    "# =========================\n",
    "if H:\n",
    "    xgb_H = XGBClassifier(n_estimators=n_estimators, max_depth=max_depth, learning_rate=learning_rate,\n",
    "                          subsample=subsample, colsample_bytree=colsample_bytree,\n",
    "                          eval_metric='mlogloss', random_state=42)\n",
    "    xgb_H.fit(X_train_aug_norm, y_train_aug)\n",
    "\n",
    "    with open(os.path.join(model_dir, \"xgb_nopca_aug_norm.pkl\"), \"wb\") as f:\n",
    "        pickle.dump(xgb_H, f)\n",
    "\n",
    "# ==========================================================\n",
    "# FONCTION D'ÉVALUATION COMMUNE\n",
    "# ==========================================================\n",
    "def evaluate_model(model, X_test, y_test, description):\n",
    "    preds = model.predict(X_test)\n",
    "    classes = np.unique(y_test)\n",
    "\n",
    "    precision = precision_score(y_test, preds, average=None, labels=classes)\n",
    "    recall    = recall_score(y_test,    preds, average=None, labels=classes)\n",
    "    cm        = confusion_matrix(y_test, preds, labels=classes)\n",
    "\n",
    "    test_acc_per_class = [cm[i, i] / cm[i, :].sum() for i in range(len(classes))]\n",
    "    cv_acc = cross_val_score(model, X_test, y_test, cv=5, scoring='accuracy').mean()\n",
    "\n",
    "    print(f\"\\n=== {description} ===\")\n",
    "    print(f\"Overall Test Accuracy: {np.mean(preds == y_test):.4f}\")\n",
    "    print(f\"Mean CV Accuracy  (5‑fold): {cv_acc:.4f}\")\n",
    "    print(\"Per‑class metrics:\")\n",
    "    for i, cls in enumerate(classes):\n",
    "        print(f\"  Class {cls}: Precision={precision[i]:.4f}, Recall={recall[i]:.4f}, Accuracy={test_acc_per_class[i]:.4f}\")\n",
    "\n",
    "# ==========================================================\n",
    "# ÉVALUATION DE TOUTES LES VARIANTES\n",
    "# ==========================================================\n",
    "if TEST_SET:\n",
    "    if A: evaluate_model(xgb_A, X_test_A, y_test, \"Scenario A : PCA NOAUG NONORM\")\n",
    "    if B: evaluate_model(xgb_B, X_test,    y_test, \"Scenario B : NOPCA NOAUG NONORM\")\n",
    "    if C: evaluate_model(xgb_C, X_test_C,  y_test_aug, \"Scenario C : PCA AUG NONORM\")\n",
    "    if D: evaluate_model(xgb_D, X_test_aug, y_test_aug, \"Scenario D : NOPCA AUG NONORM\")\n",
    "    if E: evaluate_model(xgb_E, X_test_norm,    y_test, \"Scenario E : NOPCA NOAUG NORM\")\n",
    "    if F: evaluate_model(xgb_F, X_test_F,  y_test, \"Scenario F : PCA NOAUG NORM\")\n",
    "    if G: evaluate_model(xgb_G, X_test_G,  y_test_aug, \"Scenario G : PCA AUG NORM\")\n",
    "    if H: evaluate_model(xgb_H, X_test_aug_norm, y_test_aug, \"Scenario H : NOPCA AUG NORM\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HYPERPARAMETER TUNING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Bayesian Optimization...\n",
      "|   iter    |  target   | colsam... | learni... | max_depth | n_esti... | subsample |\n",
      "-------------------------------------------------------------------------------------\n",
      "| \u001b[39m1        \u001b[39m | \u001b[39m0.7766   \u001b[39m | \u001b[39m0.6873   \u001b[39m | \u001b[39m0.2857   \u001b[39m | \u001b[39m11.52    \u001b[39m | \u001b[39m259.5    \u001b[39m | \u001b[39m0.578    \u001b[39m |\n",
      "| \u001b[35m2        \u001b[39m | \u001b[35m0.7803   \u001b[39m | \u001b[35m0.578    \u001b[39m | \u001b[35m0.02684  \u001b[39m | \u001b[35m13.26    \u001b[39m | \u001b[35m260.4    \u001b[39m | \u001b[35m0.854    \u001b[39m |\n",
      "| \u001b[35m3        \u001b[39m | \u001b[35m0.7858   \u001b[39m | \u001b[35m0.5103   \u001b[39m | \u001b[35m0.2913   \u001b[39m | \u001b[35m12.82    \u001b[39m | \u001b[35m124.3    \u001b[39m | \u001b[35m0.5909   \u001b[39m |\n",
      "| \u001b[35m4        \u001b[39m | \u001b[35m0.7931   \u001b[39m | \u001b[35m0.921    \u001b[39m | \u001b[35m0.1091   \u001b[39m | \u001b[35m14.45    \u001b[39m | \u001b[35m123.3    \u001b[39m | \u001b[35m0.5191   \u001b[39m |\n",
      "| \u001b[39m5        \u001b[39m | \u001b[39m0.7803   \u001b[39m | \u001b[39m0.9183   \u001b[39m | \u001b[39m0.09327  \u001b[39m | \u001b[39m14.38    \u001b[39m | \u001b[39m121.1    \u001b[39m | \u001b[39m0.6043   \u001b[39m |\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import cross_val_score, train_test_split\n",
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score\n",
    "from bayes_opt import BayesianOptimization\n",
    "\n",
    "# Your custom accuracy function\n",
    "from classification.utils.utils import accuracy\n",
    "\n",
    "# --- CONFIG FLAGS ---\n",
    "NORMALIZATION = True   # utilise X_train_aug_norm\n",
    "TRANSFORMATION = True  # utilise jeu de données augmenté\n",
    "\n",
    "# === Chargement des données ===\n",
    "if TRANSFORMATION:\n",
    "    X = np.load(os.path.join(fm_dir, \"X_train_aug_norm.npy\" if NORMALIZATION else \"X_train_aug.npy\"))\n",
    "    y = np.load(os.path.join(fm_dir, \"y_train_aug.npy\"), allow_pickle=True)\n",
    "else:\n",
    "    X = np.load(os.path.join(fm_dir, \"X_train_norm.npy\" if NORMALIZATION else \"X_train.npy\"))\n",
    "    y = np.load(os.path.join(fm_dir, \"y_train.npy\"), allow_pickle=True)\n",
    "\n",
    "# --- STEP 2: Define the Objective Function for Bayesian Optimization ---\n",
    "def xgb_cv(n_estimators, max_depth, learning_rate, subsample, colsample_bytree):\n",
    "    n_estimators = int(n_estimators)\n",
    "    max_depth = int(max_depth)\n",
    "\n",
    "    model = XGBClassifier(\n",
    "        n_estimators=n_estimators,\n",
    "        max_depth=max_depth,\n",
    "        learning_rate=learning_rate,\n",
    "        subsample=subsample,\n",
    "        colsample_bytree=colsample_bytree,\n",
    "        eval_metric='mlogloss',\n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "    cv_scores = cross_val_score(model, X, y, cv=5, scoring='accuracy')\n",
    "    return cv_scores.mean()\n",
    "\n",
    "# --- STEP 3: Define hyperparameter search space ---\n",
    "pbounds = {\n",
    "    'n_estimators': (50, 400),\n",
    "    'max_depth': (2, 15),\n",
    "    'learning_rate': (0.01, 0.3),\n",
    "    'subsample': (0.5, 1),\n",
    "    'colsample_bytree': (0.5, 1)\n",
    "}\n",
    "\n",
    "optimizer = BayesianOptimization(\n",
    "    f=xgb_cv,\n",
    "    pbounds=pbounds,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# --- STEP 4: Run Bayesian Optimization ---\n",
    "init_points = 3\n",
    "n_iter = 20\n",
    "print(\"Starting Bayesian Optimization...\")\n",
    "\n",
    "best_score_so_far = -1.0\n",
    "early_stop_threshold = 0.90\n",
    "\n",
    "optimizer.maximize(init_points=init_points, n_iter=n_iter)\n",
    "\n",
    "for i, res in enumerate(optimizer.res):\n",
    "    score = res['target']\n",
    "    print(f\"Iteration {i+1}, CV Accuracy: {score:.4f}, Parameters: {res['params']}\")\n",
    "    if score > best_score_so_far:\n",
    "        best_score_so_far = score\n",
    "    if best_score_so_far > early_stop_threshold:\n",
    "        print(f\"\\nEarly stopping: Found cross-validation accuracy above {early_stop_threshold}\\n\")\n",
    "        break\n",
    "\n",
    "# --- STEP 5: Retrieve best hyperparameters ---\n",
    "best_params = optimizer.max['params']\n",
    "best_n_estimators = int(best_params['n_estimators'])\n",
    "best_max_depth = int(best_params['max_depth'])\n",
    "best_learning_rate = best_params['learning_rate']\n",
    "best_subsample = best_params['subsample']\n",
    "best_colsample_bytree = best_params['colsample_bytree']\n",
    "\n",
    "print(\"\\n=== BEST HYPERPARAMETERS FOUND ===\")\n",
    "print(f\"n_estimators = {best_n_estimators}\")\n",
    "print(f\"max_depth = {best_max_depth}\")\n",
    "print(f\"learning_rate = {best_learning_rate:.4f}\")\n",
    "print(f\"subsample = {best_subsample:.4f}\")\n",
    "print(f\"colsample_bytree = {best_colsample_bytree:.4f}\")\n",
    "print(f\"CV Accuracy = {optimizer.max['target']:.4f}\")\n",
    "\n",
    "# --- STEP 6: Final evaluation on hold-out test set ---\n",
    "X_test = np.load(os.path.join(fm_dir, \"X_test_aug_norm.npy\" if (TRANSFORMATION and NORMALIZATION) else \n",
    "                              \"X_test_aug.npy\" if TRANSFORMATION else\n",
    "                              \"X_test_norm.npy\" if NORMALIZATION else\n",
    "                              \"X_test.npy\"))\n",
    "y_test = np.load(os.path.join(fm_dir, \"y_test_aug.npy\" if TRANSFORMATION else \"y_test.npy\"), allow_pickle=True)\n",
    "\n",
    "final_model = XGBClassifier(\n",
    "    n_estimators=best_n_estimators,\n",
    "    max_depth=best_max_depth,\n",
    "    learning_rate=best_learning_rate,\n",
    "    subsample=best_subsample,\n",
    "    colsample_bytree=best_colsample_bytree,\n",
    "    eval_metric='mlogloss',\n",
    "    random_state=999\n",
    ")\n",
    "\n",
    "final_model.fit(X, y)\n",
    "y_pred = final_model.predict(X_test)\n",
    "test_acc = accuracy(y_pred, y_test)\n",
    "\n",
    "print(\"\\n=== FINAL EVALUATION ON HOLDOUT TEST SET ===\")\n",
    "print(f\"Test Accuracy: {test_acc:.4f}\")\n",
    "\n",
    "# --- Save best model and optionally normalization stats ---\n",
    "os.makedirs(\"models\", exist_ok=True)\n",
    "with open(\"models/xgb_bayesopt_best_model.pkl\", \"wb\") as f:\n",
    "    pickle.dump(final_model, f)\n",
    "\n",
    "if NORMALIZATION:\n",
    "    with open(\"models/zscore_normalization_stats.pickle\", \"wb\") as f:\n",
    "        pickle.dump((np.mean(X, axis=0), np.std(X, axis=0)), f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
