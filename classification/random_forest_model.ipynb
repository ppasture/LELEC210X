{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "\"Machine learning tools\"\n",
    "import pickle\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "from classification.datasets import Dataset\n",
    "from classification.utils.audio_student import AudioUtil, Feature_vector_DS\n",
    "\n",
    "from classification.utils.plots import (\n",
    "    plot_decision_boundaries,\n",
    "    plot_specgram,\n",
    "    show_confusion_matrix,\n",
    ")\n",
    "from classification.utils.utils import accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chainsaw\n",
      "fire\n",
      "fireworks\n",
      "gunshot\n"
     ]
    }
   ],
   "source": [
    "### TO RUN\n",
    "dataset = Dataset()\n",
    "classnames = dataset.list_classes()\n",
    "\n",
    "print(\"\\n\".join(classnames))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TO RUN\n",
    "fm_dir = \"data/feature_matrices/\"  # where to save the features matrices\n",
    "new_dataset_dir = \"src/classification/datasets/new_dataset/melvecs/\"\n",
    "model_dir = \"data/models/random_forest\"  # where to save the models\n",
    "os.makedirs(fm_dir, exist_ok=True)\n",
    "os.makedirs(model_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TO RUN\n",
    "\n",
    "\"Creation of the dataset\"\n",
    "myds = Feature_vector_DS(dataset, Nft=512, nmel=20, duration=950, shift_pct=0.0)\n",
    "\n",
    "\"Some attributes...\"\n",
    "myds.nmel\n",
    "myds.duration\n",
    "myds.shift_pct\n",
    "myds.sr\n",
    "myds.data_aug\n",
    "myds.ncol\n",
    "\n",
    "\n",
    "idx = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRANSFORMATION ON FEATURE VECTOR\n",
    "\n",
    "def add_noise(feature_vector, snr_db=20):\n",
    "    \"\"\"Adds white noise to a feature vector based on the given SNR (Signal-to-Noise Ratio).\"\"\"\n",
    "    power_signal = np.mean(feature_vector ** 2)\n",
    "    power_noise = power_signal / (10 ** (snr_db / 10))\n",
    "    noise = np.random.normal(0, np.sqrt(power_noise), feature_vector.shape)\n",
    "    return feature_vector + noise\n",
    "\n",
    "def shifting(feature_vector, shift_max=20):\n",
    "    \"\"\"Shifts mel spectrogram feature vectors along the time axis by a random shift between 0 and shift_max.\"\"\"\n",
    "    shift = np.random.randint(0, shift_max)\n",
    "    return np.roll(feature_vector, shift, axis=0)  # Rolling along the first axis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the basic feature matrix : (268, 400)\n",
      "Number of labels : (268,)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "\n",
    "train_pct = 0.7\n",
    "data_aug_factor = 1\n",
    "featveclen = len(myds[\"fire\", 0, \"\", \"\"])  # Same for all classes\n",
    "classnames = [\"chainsaw\", \"fire\", \"fireworks\", \"gunshot\"]  # Or wherever you store class names\n",
    "nclass = len(classnames)\n",
    "\n",
    "# Determine number of samples per class\n",
    "naudio_per_class = {\"chainsaw\" : 76, \"fire\" : 76, \"fireworks\" : 76, \"gunshot\" : 40}\n",
    "\n",
    "\n",
    "# Allocate feature matrix\n",
    "total_samples_basic = sum(naudio_per_class[c] for c in classnames)\n",
    "X_basic = np.zeros((total_samples_basic, featveclen))\n",
    "y_basic = np.zeros((total_samples_basic), dtype=object)\n",
    "total_samples_basic\n",
    "# Fill feature matrix\n",
    "idx = 0\n",
    "for class_idx, classname in enumerate(classnames):\n",
    "    for i in range(naudio_per_class[classname]):\n",
    "        featvec = myds[classname, i, \"\", \"\"]\n",
    "        X_basic[idx, :] = featvec\n",
    "        y_basic[idx] = classname\n",
    "        idx += 1\n",
    "\n",
    "# Save features and labels\n",
    "np.save(fm_dir + \"X_basic.npy\", X_basic)\n",
    "np.save(fm_dir + \"y_basic.npy\", y_basic)\n",
    "\n",
    "print(f\"Shape of the basic feature matrix : {X_basic.shape}\")\n",
    "print(f\"Number of labels : {y_basic.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now create a new augmented dataset and observe if the classification results improve. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of transformations :  3\n",
      "Shape of the feature matrix : (804, 400)\n",
      "Number of labels : (804,)\n",
      "------------------------------------------------------------\n",
      "Transformations: ['original', 'noise', 'shifting']. Labels aligned dynamically with class sizes.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "### AUGMENTED DATASET\n",
    "list_augmentation = [\"original\", \"noise\", \"shifting\"]\n",
    "myds.mod_data_aug(list_augmentation)\n",
    "print(\"Number of transformations : \", myds.data_aug_factor)\n",
    "\n",
    "\n",
    "# Calcul total des Ã©chantillons\n",
    "total_aug_samples = sum(naudio_per_class[c] for c in classnames) * len(list_augmentation)\n",
    "X_basic_aug = np.zeros((total_aug_samples, featveclen))\n",
    "y_basic_aug = np.zeros((total_aug_samples), dtype=object)\n",
    "\n",
    "# Remplissage des features\n",
    "idx = 0\n",
    "for aug in list_augmentation:\n",
    "    for classname in classnames:\n",
    "        for i in range(naudio_per_class[classname]):\n",
    "            featvec = myds[classname, i, aug, \"\"]\n",
    "            X_basic_aug[idx, :] = featvec\n",
    "            y_basic_aug[idx] = classname\n",
    "            idx += 1\n",
    "\n",
    "# Sauvegarde\n",
    "np.save(fm_dir + \"X_basic_aug.npy\", X_basic_aug)\n",
    "np.save(fm_dir + \"y_basic_aug.npy\", y_basic_aug)\n",
    "\n",
    "print(f\"Shape of the feature matrix : {X_basic_aug.shape}\")\n",
    "print(f\"Number of labels : {y_basic_aug.shape}\")\n",
    "print(f\"------------------------------------------------------------\")\n",
    "print(f\"Transformations: {list_augmentation}. Labels aligned dynamically with class sizes.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FINAL MODEL SAVE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Object arrays cannot be loaded when allow_pickle=False",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 29\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# =========================\u001b[39;00m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# 2) LOAD DATA\u001b[39;00m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m# =========================\u001b[39;00m\n\u001b[1;32m     28\u001b[0m X_basic_aug \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mload(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(fm_dir, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX_basic_aug.npy\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m---> 29\u001b[0m y_basic_aug \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfm_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43my_basic_aug.npy\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m X_basic \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mload(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(fm_dir, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX_basic.npy\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m     32\u001b[0m y_basic \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mload(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(fm_dir, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my_basic.npy\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n",
      "File \u001b[0;32m~/master_projet_elec/LELEC210X/.venv/lib/python3.9/site-packages/numpy/lib/_npyio_impl.py:484\u001b[0m, in \u001b[0;36mload\u001b[0;34m(file, mmap_mode, allow_pickle, fix_imports, encoding, max_header_size)\u001b[0m\n\u001b[1;32m    481\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mformat\u001b[39m\u001b[38;5;241m.\u001b[39mopen_memmap(file, mode\u001b[38;5;241m=\u001b[39mmmap_mode,\n\u001b[1;32m    482\u001b[0m                                   max_header_size\u001b[38;5;241m=\u001b[39mmax_header_size)\n\u001b[1;32m    483\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 484\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mformat\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallow_pickle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallow_pickle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    485\u001b[0m \u001b[43m                                 \u001b[49m\u001b[43mpickle_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpickle_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    486\u001b[0m \u001b[43m                                 \u001b[49m\u001b[43mmax_header_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_header_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    487\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    488\u001b[0m     \u001b[38;5;66;03m# Try a pickle\u001b[39;00m\n\u001b[1;32m    489\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m allow_pickle:\n",
      "File \u001b[0;32m~/master_projet_elec/LELEC210X/.venv/lib/python3.9/site-packages/numpy/lib/format.py:822\u001b[0m, in \u001b[0;36mread_array\u001b[0;34m(fp, allow_pickle, pickle_kwargs, max_header_size)\u001b[0m\n\u001b[1;32m    819\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dtype\u001b[38;5;241m.\u001b[39mhasobject:\n\u001b[1;32m    820\u001b[0m     \u001b[38;5;66;03m# The array contained Python objects. We need to unpickle the data.\u001b[39;00m\n\u001b[1;32m    821\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m allow_pickle:\n\u001b[0;32m--> 822\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mObject arrays cannot be loaded when \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    823\u001b[0m                          \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_pickle=False\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    824\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m pickle_kwargs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    825\u001b[0m         pickle_kwargs \u001b[38;5;241m=\u001b[39m {}\n",
      "\u001b[0;31mValueError\u001b[0m: Object arrays cannot be loaded when allow_pickle=False"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score\n",
    "\n",
    "# =========================\n",
    "# 1) HYPERPARAMS & PATHS\n",
    "# =========================\n",
    "# Define or ensure these variables exist:\n",
    "# fm_dir = \"/path/to/features/\"\n",
    "# model_dir = \"/path/to/save/models/\"\n",
    "\n",
    "n_estimators = 400\n",
    "max_depth = 20\n",
    "min_samples_split = 5\n",
    "min_samples_leaf = 2\n",
    "random_state = 42\n",
    "\n",
    "# =========================\n",
    "# 2) LOAD DATA\n",
    "# =========================\n",
    "X_basic_aug = np.load(os.path.join(fm_dir, \"X_basic_aug.npy\"))\n",
    "y_basic_aug = np.load(os.path.join(fm_dir, \"y_basic_aug.npy\"), allow_pickle=True)\n",
    "\n",
    "X_basic = np.load(os.path.join(fm_dir, \"X_basic.npy\"))\n",
    "y_basic = np.load(os.path.join(fm_dir, \"y_basic.npy\"), allow_pickle=True)\n",
    "\n",
    "# =========================\n",
    "# 3) SPLIT DATASETS\n",
    "# =========================\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_basic, y_basic, \n",
    "    test_size=0.3, \n",
    "    stratify=y_basic, \n",
    "    random_state=random_state\n",
    ")\n",
    "\n",
    "X_train_aug, X_test_aug, y_train_aug, y_test_aug = train_test_split(\n",
    "    X_basic_aug, y_basic_aug, \n",
    "    test_size=0.3, \n",
    "    stratify=y_basic_aug, \n",
    "    random_state=random_state\n",
    ")\n",
    "\n",
    "# ==============================================\n",
    "# SCENARIO A: PCA + NO AUG + NO NORMALIZATION\n",
    "# ==============================================\n",
    "pca = PCA(n_components=0.99)\n",
    "X_train_pca = pca.fit_transform(X_train)\n",
    "X_test_pca = pca.transform(X_test)\n",
    "\n",
    "pca_path = os.path.join(model_dir, \"pca_noaug_nonorm.pickle\")\n",
    "with open(pca_path, \"wb\") as f:\n",
    "    pickle.dump(pca, f)\n",
    "\n",
    "rf_pca_noaug_nonorm = RandomForestClassifier(\n",
    "    n_estimators=n_estimators,\n",
    "    max_depth=max_depth,\n",
    "    min_samples_split=min_samples_split,\n",
    "    min_samples_leaf=min_samples_leaf,\n",
    "    random_state=random_state\n",
    ")\n",
    "rf_pca_noaug_nonorm.fit(X_train_pca, y_train)\n",
    "\n",
    "model_path = os.path.join(model_dir, \"rf_pca_noaug_nonorm.pickle\")\n",
    "with open(model_path, \"wb\") as f:\n",
    "    pickle.dump(rf_pca_noaug_nonorm, f)\n",
    "\n",
    "# ===========================================\n",
    "# SCENARIO B: NO PCA + NO AUG + NO NORMALIZATION\n",
    "# ===========================================\n",
    "rf_nopca_noaug_nonorm = RandomForestClassifier(\n",
    "    n_estimators=n_estimators,\n",
    "    max_depth=max_depth,\n",
    "    min_samples_split=min_samples_split,\n",
    "    min_samples_leaf=min_samples_leaf,\n",
    "    random_state=random_state\n",
    ")\n",
    "rf_nopca_noaug_nonorm.fit(X_train, y_train)\n",
    "\n",
    "model_path = os.path.join(model_dir, \"rf_nopca_noaug_nonorm.pickle\")\n",
    "with open(model_path, \"wb\") as f:\n",
    "    pickle.dump(rf_nopca_noaug_nonorm, f)\n",
    "\n",
    "# ===========================================\n",
    "# SCENARIO C: PCA + AUG + NO NORMALIZATION\n",
    "# ===========================================\n",
    "pca_aug = PCA(n_components=0.99)\n",
    "X_train_aug_pca = pca_aug.fit_transform(X_train_aug)\n",
    "X_test_aug_pca = pca_aug.transform(X_test_aug)\n",
    "\n",
    "pca_path = os.path.join(model_dir, \"pca_aug_nonorm.pickle\")\n",
    "with open(pca_path, \"wb\") as f:\n",
    "    pickle.dump(pca_aug, f)\n",
    "\n",
    "rf_pca_aug_nonorm = RandomForestClassifier(\n",
    "    n_estimators=n_estimators,\n",
    "    max_depth=max_depth,\n",
    "    min_samples_split=min_samples_split,\n",
    "    min_samples_leaf=min_samples_leaf,\n",
    "    random_state=random_state\n",
    ")\n",
    "rf_pca_aug_nonorm.fit(X_train_aug_pca, y_train_aug)\n",
    "\n",
    "model_path = os.path.join(model_dir, \"rf_pca_aug_nonorm.pickle\")\n",
    "with open(model_path, \"wb\") as f:\n",
    "    pickle.dump(rf_pca_aug_nonorm, f)\n",
    "\n",
    "# ===========================================\n",
    "# SCENARIO D: NO PCA + AUG + NO NORMALIZATION\n",
    "# ===========================================\n",
    "rf_nopca_aug_nonorm = RandomForestClassifier(\n",
    "    n_estimators=n_estimators,\n",
    "    max_depth=max_depth,\n",
    "    min_samples_split=min_samples_split,\n",
    "    min_samples_leaf=min_samples_leaf,\n",
    "    random_state=random_state\n",
    ")\n",
    "rf_nopca_aug_nonorm.fit(X_train_aug, y_train_aug)\n",
    "\n",
    "model_path = os.path.join(model_dir, \"rf_nopca_aug_nonorm.pickle\")\n",
    "with open(model_path, \"wb\") as f:\n",
    "    pickle.dump(rf_nopca_aug_nonorm, f)\n",
    "\n",
    "# ===========================================\n",
    "# SCENARIO E: NO DATA TRANSFORMATION (NO AUG) \n",
    "#             - effectively the same as B\n",
    "# ===========================================\n",
    "# If you want a separate reference model for \"no data transformation,\" \n",
    "# we simply reuse X_train, y_train:\n",
    "rf_no_transform = RandomForestClassifier(\n",
    "    n_estimators=n_estimators,\n",
    "    max_depth=max_depth,\n",
    "    min_samples_split=min_samples_split,\n",
    "    min_samples_leaf=min_samples_leaf,\n",
    "    random_state=random_state\n",
    ")\n",
    "rf_no_transform.fit(X_train, y_train)\n",
    "\n",
    "model_path = os.path.join(model_dir, \"rf_nopca_noaug_nonorm.pickle\")\n",
    "with open(model_path, \"wb\") as f:\n",
    "    pickle.dump(rf_no_transform, f)\n",
    "\n",
    "# ===========================================\n",
    "# SCENARIO F: NORMALIZATION + PCA (NO AUG)\n",
    "# ===========================================\n",
    "X_train_norm = np.array([\n",
    "    x / np.linalg.norm(x) if np.linalg.norm(x) != 0 else x\n",
    "    for x in X_train\n",
    "])\n",
    "X_test_norm = np.array([\n",
    "    x / np.linalg.norm(x) if np.linalg.norm(x) != 0 else x\n",
    "    for x in X_test\n",
    "])\n",
    "\n",
    "pca_norm = PCA(n_components=0.99)\n",
    "X_train_norm_pca = pca_norm.fit_transform(X_train_norm)\n",
    "X_test_norm_pca = pca_norm.transform(X_test_norm)\n",
    "\n",
    "pca_path = os.path.join(model_dir, \"pca_noaug_norm.pickle\")\n",
    "with open(pca_path, \"wb\") as f:\n",
    "    pickle.dump(pca_norm, f)\n",
    "\n",
    "rf_pca_noaug_norm = RandomForestClassifier(\n",
    "    n_estimators=n_estimators,\n",
    "    max_depth=max_depth,\n",
    "    min_samples_split=min_samples_split,\n",
    "    min_samples_leaf=min_samples_leaf,\n",
    "    random_state=random_state\n",
    ")\n",
    "rf_pca_noaug_norm.fit(X_train_norm_pca, y_train)\n",
    "\n",
    "model_path = os.path.join(model_dir, \"rf_pca_noaug_norm.pickle\")\n",
    "with open(model_path, \"wb\") as f:\n",
    "    pickle.dump(rf_pca_noaug_norm, f)\n",
    "\n",
    "# ===========================================\n",
    "# SCENARIO G: NORMALIZATION + PCA (AUG)\n",
    "# ===========================================\n",
    "X_train_aug_norm = np.array([\n",
    "    x / np.linalg.norm(x) if np.linalg.norm(x) != 0 else x\n",
    "    for x in X_train_aug\n",
    "])\n",
    "X_test_aug_norm = np.array([\n",
    "    x / np.linalg.norm(x) if np.linalg.norm(x) != 0 else x\n",
    "    for x in X_test_aug\n",
    "])\n",
    "\n",
    "pca_aug_norm = PCA(n_components=0.99)\n",
    "X_train_aug_norm_pca = pca_aug_norm.fit_transform(X_train_aug_norm)\n",
    "X_test_aug_norm_pca = pca_aug_norm.transform(X_test_aug_norm)\n",
    "\n",
    "pca_path = os.path.join(model_dir, \"pca_aug_norm.pickle\")\n",
    "with open(pca_path, \"wb\") as f:\n",
    "    pickle.dump(pca_aug_norm, f)\n",
    "\n",
    "rf_pca_aug_norm = RandomForestClassifier(\n",
    "    n_estimators=n_estimators,\n",
    "    max_depth=max_depth,\n",
    "    min_samples_split=min_samples_split,\n",
    "    min_samples_leaf=min_samples_leaf,\n",
    "    random_state=random_state\n",
    ")\n",
    "rf_pca_aug_norm.fit(X_train_aug_norm_pca, y_train_aug)\n",
    "\n",
    "model_path = os.path.join(model_dir, \"rf_pca_aug_norm.pickle\")\n",
    "with open(model_path, \"wb\") as f:\n",
    "    pickle.dump(rf_pca_aug_norm, f)\n",
    "\n",
    "# ===========================================\n",
    "# EVALUATION FUNCTION\n",
    "# ===========================================\n",
    "def evaluate_model(model, X_test, y_test, description):\n",
    "    predictions = model.predict(X_test)\n",
    "    classes = np.unique(y_test)\n",
    "\n",
    "    # Overall accuracy\n",
    "    overall_accuracy = np.mean(predictions == y_test)\n",
    "\n",
    "    # Per-class precision & recall\n",
    "    precision_per_class = precision_score(y_test, predictions, average=None, labels=classes)\n",
    "    recall_per_class = recall_score(y_test, predictions, average=None, labels=classes)\n",
    "\n",
    "    # Confusion Matrix -> per-class accuracy\n",
    "    conf_matrix = confusion_matrix(y_test, predictions, labels=classes)\n",
    "    accuracy_per_class = []\n",
    "    for i, cls in enumerate(classes):\n",
    "        acc_cls = conf_matrix[i, i] / conf_matrix[i, :].sum()\n",
    "        accuracy_per_class.append(acc_cls)\n",
    "\n",
    "    # Cross-validation (on the test set, typically not done, but for demonstration):\n",
    "    cv_scores = cross_val_score(model, X_test, y_test, cv=5, scoring='accuracy')\n",
    "    mean_cv_accuracy = np.mean(cv_scores)\n",
    "\n",
    "    # Print results\n",
    "    print(f\"\\n=== {description} ===\")\n",
    "    print(f\"Overall Test Accuracy: {overall_accuracy:.4f}\")\n",
    "    print(f\"Mean CV Accuracy (5-fold on test): {mean_cv_accuracy:.4f}\")\n",
    "    print(\"Per-Class Metrics:\")\n",
    "    for i, cls in enumerate(classes):\n",
    "        print(f\"  Class {cls}: Precision={precision_per_class[i]:.4f}, \"\n",
    "              f\"Recall={recall_per_class[i]:.4f}, Accuracy={accuracy_per_class[i]:.4f}\")\n",
    "\n",
    "# ===========================================\n",
    "#  EVALUATE ALL MODELS\n",
    "# ===========================================\n",
    "evaluate_model(rf_pca_noaug_nonorm,  X_test_pca,      y_test,     \"Scenario A: PCA NOAUG NONORM\")\n",
    "evaluate_model(rf_nopca_noaug_nonorm, X_test,         y_test,     \"Scenario B: NOPCA NOAUG NONORM\")\n",
    "evaluate_model(rf_pca_aug_nonorm,     X_test_aug_pca, y_test_aug, \"Scenario C: PCA AUG NONORM\")\n",
    "evaluate_model(rf_nopca_aug_nonorm,   X_test_aug,     y_test_aug, \"Scenario D: NOPCA AUG NONORM\")\n",
    "evaluate_model(rf_no_transform,       X_test,         y_test,     \"Scenario E: NOPCA NOAUG NONORM\")\n",
    "evaluate_model(rf_pca_noaug_norm,     X_test_norm_pca, y_test,    \"Scenario F: PCA NOAUG NORM\")\n",
    "evaluate_model(rf_pca_aug_norm,       X_test_aug_norm_pca, y_test_aug, \"Scenario G: PCA AUG NORM\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MEAN ACCURACY ON 20 ITERATIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nimport os\\nimport numpy as np\\nimport pickle\\nimport matplotlib.pyplot as plt\\nfrom sklearn.ensemble import RandomForestClassifier\\nfrom sklearn.model_selection import train_test_split, cross_val_score\\nfrom sklearn.metrics import confusion_matrix, precision_score, recall_score\\nfrom classification.utils.utils import accuracy\\n\\nNORMALIZATION = True\\nTRANSFORMATION = True\\n\\n# Ensure dataset (X_aug, y_aug) exists\\n\\nif TRANSFORMATION:\\n    try:\\n        X = X_basic_aug\\n        y = y_basic_aug\\n    except NameError:\\n        raise ValueError(\"X_aug and y_aug must be defined before running this script.\")\\nelse:\\n    try:\\n        X = X_basic\\n        y = y_basic\\n    except NameError:\\n        raise ValueError(\"X and y must be defined before running this script.\")\\nif NORMALIZATION:\\n    X = np.array([x/np.linalg.norm(x) if np.linalg.norm(x) != 0 else x for x in X])\\n    \\n\\n\\n# Number of iterations\\nnum_iterations = 20\\n\\n# Lists to store scores\\naccuracy_scores = []\\ncv_accuracy_scores = []\\n\\nfor i in range(num_iterations):\\n    print(f\"\\nIteration {i + 1}/{num_iterations}\")\\n\\n    # Split the dataset into training and testing subsets\\n    X_train, X_test, y_train, y_test = train_test_split(\\n        X, y, test_size=0.3, stratify=y, random_state=i  # Different splits per iteration\\n    )\\n\\n    # Train the Random Forest model\\n    model = RandomForestClassifier(\\n        n_estimators=400,\\n        max_depth=20,\\n        min_samples_split=5,\\n        min_samples_leaf=2,\\n        random_state=i  # Different initialization per iteration\\n    )\\n    model.fit(X_train, y_train)\\n\\n    # Make predictions\\n    y_pred = model.predict(X_test)\\n\\n    # Compute overall accuracy\\n    test_accuracy = accuracy(y_pred, y_test)\\n    accuracy_scores.append(test_accuracy)\\n\\n    # Perform cross-validation on the training set\\n    cv_scores = cross_val_score(model, X_train, y_train, cv=10, scoring=\\'accuracy\\')\\n    mean_cv_accuracy = np.mean(cv_scores)\\n    cv_accuracy_scores.append(mean_cv_accuracy)\\n\\n    print(f\"Test Accuracy: {test_accuracy:.4f} | Mean CV Accuracy: {mean_cv_accuracy:.4f}\")\\n\\n# Compute overall statistics\\nmean_test_accuracy = np.mean(accuracy_scores)\\nstd_test_accuracy = np.std(accuracy_scores)\\n\\nmean_cv_accuracy = np.mean(cv_accuracy_scores)\\nstd_cv_accuracy = np.std(cv_accuracy_scores)\\n\\n# Print final results\\nprint(\"\\n=== FINAL RESULTS AFTER 20 ITERATIONS ===\")\\nprint(f\"Mean Test Accuracy: {mean_test_accuracy:.4f} Â± {std_test_accuracy:.4f}\")\\nprint(f\"Mean Cross-Validation Accuracy: {mean_cv_accuracy:.4f} Â± {std_cv_accuracy:.4f}\")\\n\\n'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "import os\n",
    "import numpy as np\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score\n",
    "from classification.utils.utils import accuracy\n",
    "\n",
    "NORMALIZATION = True\n",
    "TRANSFORMATION = True\n",
    "\n",
    "# Ensure dataset (X_aug, y_aug) exists\n",
    "\n",
    "if TRANSFORMATION:\n",
    "    try:\n",
    "        X = X_basic_aug\n",
    "        y = y_basic_aug\n",
    "    except NameError:\n",
    "        raise ValueError(\"X_aug and y_aug must be defined before running this script.\")\n",
    "else:\n",
    "    try:\n",
    "        X = X_basic\n",
    "        y = y_basic\n",
    "    except NameError:\n",
    "        raise ValueError(\"X and y must be defined before running this script.\")\n",
    "if NORMALIZATION:\n",
    "    X = np.array([x/np.linalg.norm(x) if np.linalg.norm(x) != 0 else x for x in X])\n",
    "    \n",
    "\n",
    "\n",
    "# Number of iterations\n",
    "num_iterations = 20\n",
    "\n",
    "# Lists to store scores\n",
    "accuracy_scores = []\n",
    "cv_accuracy_scores = []\n",
    "\n",
    "for i in range(num_iterations):\n",
    "    print(f\"\\nIteration {i + 1}/{num_iterations}\")\n",
    "\n",
    "    # Split the dataset into training and testing subsets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.3, stratify=y, random_state=i  # Different splits per iteration\n",
    "    )\n",
    "\n",
    "    # Train the Random Forest model\n",
    "    model = RandomForestClassifier(\n",
    "        n_estimators=400,\n",
    "        max_depth=20,\n",
    "        min_samples_split=5,\n",
    "        min_samples_leaf=2,\n",
    "        random_state=i  # Different initialization per iteration\n",
    "    )\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # Make predictions\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    # Compute overall accuracy\n",
    "    test_accuracy = accuracy(y_pred, y_test)\n",
    "    accuracy_scores.append(test_accuracy)\n",
    "\n",
    "    # Perform cross-validation on the training set\n",
    "    cv_scores = cross_val_score(model, X_train, y_train, cv=10, scoring='accuracy')\n",
    "    mean_cv_accuracy = np.mean(cv_scores)\n",
    "    cv_accuracy_scores.append(mean_cv_accuracy)\n",
    "\n",
    "    print(f\"Test Accuracy: {test_accuracy:.4f} | Mean CV Accuracy: {mean_cv_accuracy:.4f}\")\n",
    "\n",
    "# Compute overall statistics\n",
    "mean_test_accuracy = np.mean(accuracy_scores)\n",
    "std_test_accuracy = np.std(accuracy_scores)\n",
    "\n",
    "mean_cv_accuracy = np.mean(cv_accuracy_scores)\n",
    "std_cv_accuracy = np.std(cv_accuracy_scores)\n",
    "\n",
    "# Print final results\n",
    "print(\"\\n=== FINAL RESULTS AFTER 20 ITERATIONS ===\")\n",
    "print(f\"Mean Test Accuracy: {mean_test_accuracy:.4f} Â± {std_test_accuracy:.4f}\")\n",
    "print(f\"Mean Cross-Validation Accuracy: {mean_cv_accuracy:.4f} Â± {std_cv_accuracy:.4f}\")\n",
    "\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
