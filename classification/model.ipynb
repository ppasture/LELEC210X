{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'classification.datasets'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 14\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_selection\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m StratifiedKFold, train_test_split\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mneighbors\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m KNeighborsClassifier\n\u001b[1;32m---> 14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mclassification\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdatasets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Dataset\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mclassification\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01maudio_student\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AudioUtil, Feature_vector_DS\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mclassification\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mplots\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     18\u001b[0m     plot_decision_boundaries,\n\u001b[0;32m     19\u001b[0m     plot_specgram,\n\u001b[0;32m     20\u001b[0m     show_confusion_matrix,\n\u001b[0;32m     21\u001b[0m )\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'classification.datasets'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "\"Machine learning tools\"\n",
    "import pickle\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "from classification.datasets import Dataset\n",
    "from classification.utils.audio_student import AudioUtil, Feature_vector_DS\n",
    "\n",
    "from classification.utils.plots import (\n",
    "    plot_decision_boundaries,\n",
    "    plot_specgram,\n",
    "    show_confusion_matrix,\n",
    ")\n",
    "from classification.utils.utils import accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TO RUN\n",
    "dataset = Dataset()\n",
    "classnames = dataset.list_classes()\n",
    "\n",
    "print(\"\\n\".join(classnames))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TO RUN\n",
    "fm_dir = \"data/feature_matrices/\"  # where to save the features matrices\n",
    "new_dataset_dir = \"src/classification/datasets/new_dataset/melvecs/\"\n",
    "model_dir = \"data/models/\"  # where to save the models\n",
    "os.makedirs(fm_dir, exist_ok=True)\n",
    "os.makedirs(model_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TO RUN\n",
    "\n",
    "\"Creation of the dataset\"\n",
    "myds = Feature_vector_DS(dataset, Nft=512, nmel=20, duration=950, shift_pct=0.0)\n",
    "\n",
    "\"Some attributes...\"\n",
    "myds.nmel\n",
    "myds.duration\n",
    "myds.shift_pct\n",
    "myds.sr\n",
    "myds.data_aug\n",
    "myds.ncol\n",
    "\n",
    "\n",
    "idx = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRANSFORMATION ON FEATURE VECTOR\n",
    "\n",
    "def add_noise(feature_vector, snr_db=20):\n",
    "    \"\"\"Adds white noise to a feature vector based on the given SNR (Signal-to-Noise Ratio).\"\"\"\n",
    "    power_signal = np.mean(feature_vector ** 2)\n",
    "    power_noise = power_signal / (10 ** (snr_db / 10))\n",
    "    noise = np.random.normal(0, np.sqrt(power_noise), feature_vector.shape)\n",
    "    return feature_vector + noise\n",
    "\n",
    "def shifting(feature_vector, shift_max=20):\n",
    "    \"\"\"Shifts mel spectrogram feature vectors along the time axis by a random shift between 0 and shift_max.\"\"\"\n",
    "    shift = np.random.randint(0, shift_max)\n",
    "    return np.roll(feature_vector, shift, axis=0)  # Rolling along the first axis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### FEATURE EXTRACTION FROM SOUND\n",
    "\"Random split of 70:30 between training and validation\"\n",
    "train_pct = 0.7\n",
    "\n",
    "featveclen = len(myds[\"fire\", 0, \"\"])  # number of items in a feature vector\n",
    "nitems = len(myds)  # number of sounds in the dataset\n",
    "naudio = dataset.naudio  # number of audio files in each class\n",
    "nclass = dataset.nclass  # number of classes\n",
    "nlearn = round(naudio * train_pct)  # number of sounds among naudio for training\n",
    "\n",
    "data_aug_factor = 1\n",
    "class_ids_aug = np.repeat(classnames, naudio * data_aug_factor)\n",
    "\n",
    "X = np.zeros((data_aug_factor * nclass * naudio, featveclen))\n",
    "for s in range(data_aug_factor):\n",
    "    for class_idx, classname in enumerate(classnames):\n",
    "        for idx in range(naudio):\n",
    "            featvec = myds[classname, idx, \"\"]\n",
    "            X[s * nclass * naudio + class_idx * naudio + idx, :] = featvec\n",
    "np.save(fm_dir + \"X_basic.npy\", X)\n",
    "y = class_ids_aug.copy()\n",
    "np.save(fm_dir + \"y_basic.npy\", y)\n",
    "\n",
    "print(f\"Shape of the basic feature matrix : {X.shape}\")\n",
    "print(f\"Number of labels : {len(y)}\")\n",
    "\n",
    "\n",
    "# FEATURE EXTRACTION FROM FILE\n",
    "\n",
    "naudio_new = 40\n",
    "\n",
    "X_new_dataset = []\n",
    "y_new_dataset = []\n",
    "X_new_dataset_aug = []\n",
    "y_new_dataset_aug = []\n",
    "\n",
    "for classname in classnames:\n",
    "    for idx in range(1, naudio_new + 1):  # Assuming files are named from 1 to 40\n",
    "        file_path = os.path.join(new_dataset_dir, f\"{classname}_{idx:02d}.npy\")\n",
    "        if os.path.exists(file_path):\n",
    "            featvec = np.load(file_path)\n",
    "            featvec_shifted = shifting(featvec)  # Apply shifting transformation\n",
    "            featvec_noisy = add_noise(featvec)  # Apply noise transformation\n",
    "            X_new_dataset.append(featvec.flatten())\n",
    "            y_new_dataset.append(classname)\n",
    "            \n",
    "            X_new_dataset_aug.append(featvec.flatten())\n",
    "            X_new_dataset_aug.append(featvec_shifted.flatten())\n",
    "            X_new_dataset_aug.append(featvec_noisy.flatten())\n",
    "            y_new_dataset_aug.append(classname)\n",
    "            y_new_dataset_aug.append(classname)\n",
    "            y_new_dataset_aug.append(classname)\n",
    "        else:\n",
    "            print(f\"Warning: File {file_path} not found.\")\n",
    "\n",
    "X_new_dataset = np.array(X_new_dataset)\n",
    "y_new_dataset = np.array(y_new_dataset)\n",
    "X_new_dataset_aug = np.array(X_new_dataset_aug)\n",
    "y_new_dataset_aug = np.array(y_new_dataset_aug)\n",
    "\n",
    "np.save(fm_dir + \"X_new_dataset.npy\", X_new_dataset)\n",
    "np.save(fm_dir + \"y_new_dataset.npy\", y_new_dataset)\n",
    "np.save(fm_dir + \"X_new_dataset_aug.npy\", X_new_dataset_aug)\n",
    "np.save(fm_dir + \"y_new_dataset_aug.npy\", y_new_dataset_aug)\n",
    "\n",
    "print(f\"Shape of the new feature matrix: {X_new_dataset.shape}\")\n",
    "print(f\"Number of labels in new dataset: {len(y_new_dataset)}\")\n",
    "\n",
    "print(f\"Shape of the new feature matrix with data augmentation: {X_new_dataset_aug.shape}\")\n",
    "print(f\"Number of labels in new dataset with data augmentation: {len(y_new_dataset_aug)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now create a new augmented dataset and observe if the classification results improve. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### AUGMENTED DATASET\n",
    "list_augmentation = [\"original\", \"noise\", \"echo\", \"shifting\"]\n",
    "myds.mod_data_aug(list_augmentation)\n",
    "print(\"Number of transformations : \", myds.data_aug_factor)\n",
    "y_basic_aug = np.repeat(classnames, dataset.naudio * myds.data_aug_factor)\n",
    "X_basic_aug = np.zeros((myds.data_aug_factor * nclass * naudio, featveclen))\n",
    "\n",
    "for s in range(len(list_augmentation)):\n",
    "    aug = list_augmentation[s]\n",
    "    for idx in range(dataset.naudio):\n",
    "        for class_idx, classname in enumerate(classnames):\n",
    "            featvec = myds[classname, idx, aug]\n",
    "            X_basic_aug[s * nclass * naudio + class_idx * naudio + idx, :] = featvec\n",
    "            y_basic_aug[s * nclass * naudio + class_idx * naudio + idx] = classname\n",
    "\n",
    "np.save(fm_dir + \"X_basic_aug.npy\", X_new_dataset)\n",
    "np.save(fm_dir + \"y_basic_aug.npy\", y_new_dataset)\n",
    "\n",
    "print(f\"Shape of the feature matrix : {X_basic_aug.shape}\")\n",
    "print(f\"------------------------------------------------------------\")\n",
    "print(f\"200 of each transformation. Order : chainsaw1, fire1, fireworks1, gun1, chainsaw2, fire2, ...\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FINAL MODEL SAVE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score\n",
    "\n",
    "# Directories\n",
    "fm_dir = \"data/feature_matrices/\"\n",
    "model_dir = \"data/models/\"\n",
    "os.makedirs(fm_dir, exist_ok=True)\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Load datasets\n",
    "\n",
    "X_new_dataset = np.load(fm_dir + \"X_new_dataset.npy\")  # Original dataset\n",
    "y_new_dataset = np.load(fm_dir + \"y_new_dataset.npy\")\n",
    "\n",
    "X_new_dataset_aug = np.load(fm_dir + \"X_new_dataset_aug.npy\")  # Original dataset\n",
    "y_new_dataset_aug = np.load(fm_dir + \"y_new_dataset_aug.npy\")\n",
    "\n",
    "\n",
    "# Split dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_new_dataset, y_new_dataset, test_size=0.3, stratify=y_new_dataset)\n",
    "X_train_aug, X_test_aug, y_train_aug, y_test_aug = train_test_split(\n",
    "    X_new_dataset_aug, y_new_dataset_aug, test_size=0.3, stratify=y_new_dataset_aug\n",
    ")\n",
    "\n",
    "# ===== TRAIN MODEL WITH PCA =====\n",
    "pca = PCA(n_components=0.98)  # Keep 98% variance\n",
    "X_train_aug_pca = pca.fit_transform(X_train_aug)\n",
    "X_test_aug_pca = pca.transform(X_test_aug)  # Apply the same transformation to test data\n",
    "print(f\"Number of principal features kept: {pca.n_components_}\")\n",
    "\n",
    "# Save the PCA model separately\n",
    "pca_filename = os.path.join(model_dir, \"pca.pickle\")\n",
    "with open(pca_filename, \"wb\") as pca_file:\n",
    "    pickle.dump(pca, pca_file)\n",
    "print(f\"\\nPCA model saved as {pca_filename}\")\n",
    "\n",
    "# Train the Random Forest model on PCA-reduced features\n",
    "rf_model_pca = RandomForestClassifier(\n",
    "    n_estimators=400,\n",
    "    max_depth=20,\n",
    "    min_samples_split=5,\n",
    "    min_samples_leaf=2,\n",
    "    random_state=42\n",
    ")\n",
    "rf_model_pca.fit(X_train_aug_pca, y_train_aug)\n",
    "\n",
    "# Save the model trained **with PCA**\n",
    "rf_filename_pca = os.path.join(model_dir, \"model_pca.pickle\")\n",
    "with open(rf_filename_pca, \"wb\") as rf_file:\n",
    "    pickle.dump(rf_model_pca, rf_file)\n",
    "print(f\"\\nRandom Forest model WITH PCA saved as {rf_filename_pca}\")\n",
    "\n",
    "# ===== TRAIN MODEL WITHOUT PCA =====\n",
    "rf_model_no_pca = RandomForestClassifier(\n",
    "    n_estimators=400,\n",
    "    max_depth=20,\n",
    "    min_samples_split=5,\n",
    "    min_samples_leaf=2,\n",
    "    random_state=42\n",
    ")\n",
    "rf_model_no_pca.fit(X_train_aug, y_train_aug)  # Train directly on raw `X_aug` features\n",
    "\n",
    "# Save the model trained **without PCA**\n",
    "rf_filename_no_pca = os.path.join(model_dir, \"model_without_pca.pickle\")\n",
    "with open(rf_filename_no_pca, \"wb\") as rf_file:\n",
    "    pickle.dump(rf_model_no_pca, rf_file)\n",
    "print(f\"\\nRandom Forest model WITHOUT PCA saved as {rf_filename_no_pca}\")\n",
    "\n",
    "# ===== TRAIN MODEL WITHOUT DATA TRANSFORMATION (Original Data `X_basic`) =====\n",
    "rf_model_no_transform = RandomForestClassifier(\n",
    "    n_estimators=400,\n",
    "    max_depth=20,\n",
    "    min_samples_split=5,\n",
    "    min_samples_leaf=2,\n",
    "    random_state=42\n",
    ")\n",
    "rf_model_no_transform.fit(X_train, y_train)  # Train on untransformed data\n",
    "\n",
    "# Save the model trained **on X_basic (original features)**\n",
    "rf_filename_no_transform = os.path.join(model_dir, \"model_without_data_transformation.pickle\")\n",
    "with open(rf_filename_no_transform, \"wb\") as rf_file:\n",
    "    pickle.dump(rf_model_no_transform, rf_file)\n",
    "print(f\"\\nRandom Forest model WITHOUT DATA TRANSFORMATION saved as {rf_filename_no_transform}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ===== TRAIN MODEL WITH NORMALIZATION =====\n",
    "X_train_normalized = []\n",
    "X_test_normalized = []\n",
    "for i in range(len(X_train)):\n",
    "    X_train_normalized.append(X_train[i]/np.linalg.norm(X_train[i]))\n",
    "\n",
    "for i in range(len(X_test)):\n",
    "    X_test_normalized.append(X_test[i]/np.linalg.norm(X_test[i]))\n",
    "    \n",
    "\n",
    "# Train the model on normalized features\n",
    "rf_model_normalized = RandomForestClassifier(\n",
    "    n_estimators=400,\n",
    "    max_depth=20,\n",
    "    min_samples_split=5,\n",
    "    min_samples_leaf=2,\n",
    "    random_state=42\n",
    ")\n",
    "rf_model_normalized.fit(X_train_normalized, y_train)\n",
    "\n",
    "# Save the model trained **with normalization**\n",
    "rf_filename_normalized = os.path.join(model_dir, \"model_normalized_no_transformation.pickle\")\n",
    "with open(rf_filename_normalized, \"wb\") as rf_file:\n",
    "    pickle.dump(rf_model_normalized, rf_file)\n",
    "print(f\"\\nRandom Forest model WITH NORMALIZATION saved as {rf_filename_normalized}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ===== EVALUATION FUNCTION (optional but useful) =====\n",
    "def evaluate_model(model, X_test, y_test, description):\n",
    "    predict = model.predict(X_test)\n",
    "\n",
    "    # Compute metrics for each class\n",
    "    classes = np.unique(y_test)\n",
    "    precision_per_class = precision_score(y_test, predict, average=None, labels=classes)\n",
    "    recall_per_class = recall_score(y_test, predict, average=None, labels=classes)\n",
    "\n",
    "    test_accuracy_per_class = []\n",
    "    conf_matrix = confusion_matrix(y_test, predict, labels=classes)\n",
    "\n",
    "    for i, cls in enumerate(classes):\n",
    "        class_accuracy = conf_matrix[i, i] / conf_matrix[i, :].sum()\n",
    "        test_accuracy_per_class.append(class_accuracy)\n",
    "\n",
    "    # Cross-validation accuracy\n",
    "    cv_scores = cross_val_score(model, X_test, y_test, cv=10, scoring='accuracy')\n",
    "    mean_cv_accuracy = np.mean(cv_scores)\n",
    "\n",
    "    # Print results\n",
    "    print(f\"\\n=== {description} ===\")\n",
    "    print(f\"Test Accuracy (Overall): {np.mean(predict == y_test):.4f}\")\n",
    "    print(f\"Mean CV Accuracy: {mean_cv_accuracy:.4f}\")\n",
    "\n",
    "    print(\"\\nPer-Class Metrics:\")\n",
    "    for i, cls in enumerate(classes):\n",
    "        print(f\"Class {cls}:\")\n",
    "        print(f\"  Precision: {precision_per_class[i]:.4f}\")\n",
    "        print(f\"  Recall: {recall_per_class[i]:.4f}\")\n",
    "        print(f\"  Accuracy: {test_accuracy_per_class[i]:.4f}\")\n",
    "\n",
    "# Evaluate all models\n",
    "evaluate_model(rf_model_pca, X_test_aug_pca, y_test_aug, \"Model WITH PCA\")\n",
    "evaluate_model(rf_model_no_pca, X_test, y_test, \"Model WITHOUT PCA\")\n",
    "evaluate_model(rf_model_no_transform, X_test, y_test, \"Model WITHOUT DATA TRANSFORMATION\")\n",
    "evaluate_model(rf_model_normalized, X_test_normalized, y_test, \"Model WITH NORMALIZATION\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MEAN ACCURACY ON 100 ITERATIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score\n",
    "from classification.utils.utils import accuracy\n",
    "\n",
    "# Directories for saving models\n",
    "model_dir = \"data/models/\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Ensure dataset (X_aug, y_aug) exists\n",
    "try:\n",
    "    X = X_aug\n",
    "    y = y_aug\n",
    "except NameError:\n",
    "    raise ValueError(\"X_aug and y_aug must be defined before running this script.\")\n",
    "\n",
    "# Number of iterations\n",
    "num_iterations = 20\n",
    "\n",
    "# Lists to store scores\n",
    "accuracy_scores = []\n",
    "cv_accuracy_scores = []\n",
    "\n",
    "for i in range(num_iterations):\n",
    "    print(f\"\\nIteration {i + 1}/{num_iterations}\")\n",
    "\n",
    "    # Split the dataset into training and testing subsets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.3, stratify=y, random_state=i  # Different splits per iteration\n",
    "    )\n",
    "\n",
    "    # Train the Random Forest model\n",
    "    model = RandomForestClassifier(\n",
    "        n_estimators=400,\n",
    "        max_depth=20,\n",
    "        min_samples_split=5,\n",
    "        min_samples_leaf=2,\n",
    "        random_state=i  # Different initialization per iteration\n",
    "    )\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # Make predictions\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    # Compute overall accuracy\n",
    "    test_accuracy = accuracy(y_pred, y_test)\n",
    "    accuracy_scores.append(test_accuracy)\n",
    "\n",
    "    # Perform cross-validation on the training set\n",
    "    cv_scores = cross_val_score(model, X_train, y_train, cv=10, scoring='accuracy')\n",
    "    mean_cv_accuracy = np.mean(cv_scores)\n",
    "    cv_accuracy_scores.append(mean_cv_accuracy)\n",
    "\n",
    "    print(f\"Test Accuracy: {test_accuracy:.4f} | Mean CV Accuracy: {mean_cv_accuracy:.4f}\")\n",
    "\n",
    "# Compute overall statistics\n",
    "mean_test_accuracy = np.mean(accuracy_scores)\n",
    "std_test_accuracy = np.std(accuracy_scores)\n",
    "\n",
    "mean_cv_accuracy = np.mean(cv_accuracy_scores)\n",
    "std_cv_accuracy = np.std(cv_accuracy_scores)\n",
    "\n",
    "# Print final results\n",
    "print(\"\\n=== FINAL RESULTS AFTER 100 ITERATIONS ===\")\n",
    "print(f\"Mean Test Accuracy: {mean_test_accuracy:.4f} ± {std_test_accuracy:.4f}\")\n",
    "print(f\"Mean Cross-Validation Accuracy: {mean_cv_accuracy:.4f} ± {std_cv_accuracy:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
