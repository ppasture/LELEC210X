{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "\"Machine learning tools\"\n",
    "import pickle\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "\n",
    "\n",
    "from classification.datasets import Dataset\n",
    "from classification.utils.audio_student import AudioUtil, Feature_vector_DS\n",
    "\n",
    "from classification.utils.plots import (\n",
    "    plot_decision_boundaries,\n",
    "    plot_specgram,\n",
    "    show_confusion_matrix,\n",
    ")\n",
    "from classification.utils.utils import accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chainsaw\n",
      "fire\n",
      "fireworks\n",
      "gunshot\n"
     ]
    }
   ],
   "source": [
    "### TO RUN\n",
    "dataset = Dataset()\n",
    "classnames = dataset.list_classes()\n",
    "\n",
    "print(\"\\n\".join(classnames))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TO RUN\n",
    "fm_dir = \"data/feature_matrices/\"  # where to save the features matrices\n",
    "new_dataset_dir = \"src/classification/datasets/new_dataset/melvecs/\"\n",
    "model_dir = \"data/models/xgboost\"  # where to save the models\n",
    "os.makedirs(fm_dir, exist_ok=True)\n",
    "os.makedirs(model_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TO RUN\n",
    "\n",
    "\"Creation of the dataset\"\n",
    "myds = Feature_vector_DS(dataset, Nft=512, nmel=20, duration=950, shift_pct=0.0)\n",
    "\n",
    "\"Some attributes...\"\n",
    "myds.nmel\n",
    "myds.duration\n",
    "myds.shift_pct\n",
    "myds.sr\n",
    "myds.data_aug\n",
    "myds.ncol\n",
    "\n",
    "idx = 0\n",
    "\n",
    "# XGBOOST PARAMETERS\n",
    "n_estimators = 231\n",
    "max_depth = 7\n",
    "learning_rate = 0.0538\n",
    "subsample = 0.5679\n",
    "colsample_bytree = 0.7183"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRANSFORMATION ON FEATURE VECTOR\n",
    "\n",
    "def add_noise(feature_vector, snr_db=20):\n",
    "    \"\"\"Adds white noise to a feature vector based on the given SNR (Signal-to-Noise Ratio).\"\"\"\n",
    "    power_signal = np.mean(feature_vector ** 2)\n",
    "    power_noise = power_signal / (10 ** (snr_db / 10))\n",
    "    noise = np.random.normal(0, np.sqrt(power_noise), feature_vector.shape)\n",
    "    return feature_vector + noise\n",
    "\n",
    "def shifting(feature_vector, shift_max=20):\n",
    "    \"\"\"Shifts mel spectrogram feature vectors along the time axis by a random shift between 0 and shift_max.\"\"\"\n",
    "    shift = np.random.randint(0, shift_max)\n",
    "    return np.roll(feature_vector, shift, axis=0)  # Rolling along the first axis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the basic feature matrix : (304, 400)\n",
      "Number of labels : 304\n"
     ]
    }
   ],
   "source": [
    "### FEATURE EXTRACTION FROM SOUND\n",
    "\"Random split of 70:30 between training and validation\"\n",
    "train_pct = 0.7\n",
    "\n",
    "featveclen = len(myds[\"fire\", 0, \"\", \"\"])  # number of items in a feature vector\n",
    "nitems = len(myds)  # number of sounds in the dataset\n",
    "naudio = dataset.naudio  # number of audio files in each class\n",
    "nclass = dataset.nclass  # number of classes\n",
    "nlearn = round(naudio * train_pct)  # number of sounds among naudio for training\n",
    "\n",
    "data_aug_factor = 1\n",
    "class_ids_aug = np.repeat(classnames, naudio * data_aug_factor)\n",
    "\n",
    "X = np.zeros((data_aug_factor * nclass * naudio, featveclen))\n",
    "for s in range(data_aug_factor):\n",
    "    for class_idx, classname in enumerate(classnames):\n",
    "        for idx in range(naudio):\n",
    "            featvec = myds[classname, idx, \"\", \"lowpass\"]\n",
    "            X[s * nclass * naudio + class_idx * naudio + idx, :] = featvec\n",
    "np.save(fm_dir + \"X_basic.npy\", X)\n",
    "y = class_ids_aug.copy()\n",
    "np.save(fm_dir + \"y_basic.npy\", y)\n",
    "\n",
    "print(f\"Shape of the basic feature matrix : {X.shape}\")\n",
    "print(f\"Number of labels : {len(y)}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now create a new augmented dataset and observe if the classification results improve. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of transformations :  3\n",
      "Shape of the feature matrix : (912, 400)\n",
      "------------------------------------------------------------\n",
      "200 of each transformation. Order : chainsaw1, fire1, fireworks1, gun1, chainsaw2, fire2, ...\n"
     ]
    }
   ],
   "source": [
    "### AUGMENTED DATASET\n",
    "list_augmentation = [\"original\", \"noise\", \"shifting\"]\n",
    "myds.mod_data_aug(list_augmentation)\n",
    "print(\"Number of transformations : \", myds.data_aug_factor)\n",
    "y_basic_aug = np.repeat(classnames, dataset.naudio * myds.data_aug_factor)\n",
    "X_basic_aug = np.zeros((myds.data_aug_factor * nclass * naudio, featveclen))\n",
    "\n",
    "for s in range(len(list_augmentation)):\n",
    "    aug = list_augmentation[s]\n",
    "    for idx in range(dataset.naudio):\n",
    "        for class_idx, classname in enumerate(classnames):\n",
    "            featvec = myds[classname, idx, aug, \"lowpass\"]\n",
    "            X_basic_aug[s * nclass * naudio + class_idx * naudio + idx, :] = featvec\n",
    "            y_basic_aug[s * nclass * naudio + class_idx * naudio + idx] = classname\n",
    "\n",
    "np.save(fm_dir + \"X_basic_aug.npy\", X_basic_aug)\n",
    "np.save(fm_dir + \"y_basic_aug.npy\", y_basic_aug)\n",
    "\n",
    "print(f\"Shape of the feature matrix : {X_basic_aug.shape}\")\n",
    "print(f\"------------------------------------------------------------\")\n",
    "print(f\"200 of each transformation. Order : chainsaw1, fire1, fireworks1, gun1, chainsaw2, fire2, ...\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FINAL MODEL SAVE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Scenario A: WITH PCA (NO AUG) ===\n",
      "Test Accuracy (Overall): 0.7065\n",
      "Mean CV Accuracy: 0.5433\n",
      "\n",
      "Per-Class Metrics:\n",
      "Class 0: Precision=0.6538, Recall=0.7391, Accuracy=0.7391\n",
      "Class 1: Precision=0.7500, Recall=0.7826, Accuracy=0.7826\n",
      "Class 2: Precision=0.7500, Recall=0.3913, Accuracy=0.3913\n",
      "Class 3: Precision=0.7000, Recall=0.9130, Accuracy=0.9130\n",
      "\n",
      "=== Scenario B: WITHOUT PCA (NO AUG) ===\n",
      "Test Accuracy (Overall): 0.8696\n",
      "Mean CV Accuracy: 0.7468\n",
      "\n",
      "Per-Class Metrics:\n",
      "Class 0: Precision=0.8000, Recall=0.6957, Accuracy=0.6957\n",
      "Class 1: Precision=0.8750, Recall=0.9130, Accuracy=0.9130\n",
      "Class 2: Precision=0.8696, Recall=0.8696, Accuracy=0.8696\n",
      "Class 3: Precision=0.9200, Recall=1.0000, Accuracy=1.0000\n",
      "\n",
      "=== Scenario C: WITH PCA (AUG) ===\n",
      "Test Accuracy (Overall): 0.6977\n",
      "Mean CV Accuracy: 0.5928\n",
      "\n",
      "Per-Class Metrics:\n",
      "Class 0: Precision=0.6364, Recall=0.9545, Accuracy=0.9545\n",
      "Class 1: Precision=0.6538, Recall=0.8095, Accuracy=0.8095\n",
      "Class 2: Precision=0.8125, Recall=0.5909, Accuracy=0.5909\n",
      "Class 3: Precision=0.8182, Recall=0.4286, Accuracy=0.4286\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ppasture/master_projet_elec/LELEC210X/.venv/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Scenario D: WITHOUT PCA (AUG) ===\n",
      "Test Accuracy (Overall): 0.2500\n",
      "Mean CV Accuracy: 0.7251\n",
      "\n",
      "Per-Class Metrics:\n",
      "Class 0: Precision=0.0000, Recall=0.0000, Accuracy=0.0000\n",
      "Class 1: Precision=0.0000, Recall=0.0000, Accuracy=0.0000\n",
      "Class 2: Precision=0.0000, Recall=0.0000, Accuracy=0.0000\n",
      "Class 3: Precision=0.2500, Recall=1.0000, Accuracy=1.0000\n",
      "\n",
      "=== Scenario E: NO DATA TRANSFORMATION ===\n",
      "Test Accuracy (Overall): 0.8370\n",
      "Mean CV Accuracy: 0.7251\n",
      "\n",
      "Per-Class Metrics:\n",
      "Class 0: Precision=0.7826, Recall=0.7826, Accuracy=0.7826\n",
      "Class 1: Precision=0.9048, Recall=0.8261, Accuracy=0.8261\n",
      "Class 2: Precision=0.7826, Recall=0.7826, Accuracy=0.7826\n",
      "Class 3: Precision=0.8800, Recall=0.9565, Accuracy=0.9565\n",
      "\n",
      "=== Scenario F: NORMALIZATION + PCA (NO AUG) ===\n",
      "Test Accuracy (Overall): 0.7283\n",
      "Mean CV Accuracy: 0.6836\n",
      "\n",
      "Per-Class Metrics:\n",
      "Class 0: Precision=0.6296, Recall=0.7391, Accuracy=0.7391\n",
      "Class 1: Precision=0.8947, Recall=0.7391, Accuracy=0.7391\n",
      "Class 2: Precision=0.6250, Recall=0.6522, Accuracy=0.6522\n",
      "Class 3: Precision=0.8182, Recall=0.7826, Accuracy=0.7826\n",
      "\n",
      "=== Scenario G: NORMALIZATION + AUG + PCA ===\n",
      "Test Accuracy (Overall): 0.7326\n",
      "Mean CV Accuracy: 0.4523\n",
      "\n",
      "Per-Class Metrics:\n",
      "Class 0: Precision=0.6800, Recall=0.7727, Accuracy=0.7727\n",
      "Class 1: Precision=0.6786, Recall=0.9048, Accuracy=0.9048\n",
      "Class 2: Precision=0.7500, Recall=0.6818, Accuracy=0.6818\n",
      "Class 3: Precision=0.9231, Recall=0.5714, Accuracy=0.5714\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Load datasets\n",
    "X_basic_aug = np.load(os.path.join(fm_dir, \"X_basic_aug.npy\"))\n",
    "y_basic_aug = np.load(os.path.join(fm_dir, \"y_basic_aug.npy\"))\n",
    "\n",
    "X_basic = np.load(os.path.join(fm_dir, \"X_basic.npy\"))\n",
    "y_basic = np.load(os.path.join(fm_dir, \"y_basic.npy\"))\n",
    "\n",
    "# Encode labels\n",
    "label_encoder = LabelEncoder()\n",
    "y_basic = label_encoder.fit_transform(y_basic)\n",
    "y_basic_aug = label_encoder.transform(y_basic_aug)\n",
    "\n",
    "# Split datasets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_basic, y_basic, test_size=0.3, stratify=y_basic)\n",
    "X_train_aug, X_test_aug, y_train_aug, y_test_aug = train_test_split(X_basic_aug, y_basic_aug, test_size=0.3, stratify=y_basic_aug)\n",
    "\n",
    "# =========================\n",
    "# SCENARIO A: WITH PCA (no aug)\n",
    "# =========================\n",
    "pca = PCA(n_components=0.99)\n",
    "X_train_pca = pca.fit_transform(X_train)\n",
    "X_test_pca = pca.transform(X_test)\n",
    "\n",
    "pca_filename = os.path.join(model_dir, \"pca_noaug_nonorm.pickle\")\n",
    "with open(pca_filename, \"wb\") as f:\n",
    "    pickle.dump(pca, f)\n",
    "\n",
    "xgb_pca = XGBClassifier(n_estimators=100, max_depth=5, learning_rate=0.1,\n",
    "                        subsample=0.8, colsample_bytree=0.8, eval_metric='mlogloss', random_state=42)\n",
    "xgb_pca.fit(X_train_pca, y_train)\n",
    "\n",
    "model_filename = os.path.join(model_dir, \"xgb_pca_noaug_nonorm.pickle\")\n",
    "with open(model_filename, \"wb\") as f:\n",
    "    pickle.dump(xgb_pca, f)\n",
    "\n",
    "# =========================\n",
    "# SCENARIO B: WITHOUT PCA (no aug)\n",
    "# =========================\n",
    "xgb_no_pca = XGBClassifier(n_estimators=100, max_depth=5, learning_rate=0.1,\n",
    "                           subsample=0.8, colsample_bytree=0.8, eval_metric='mlogloss', random_state=42)\n",
    "xgb_no_pca.fit(X_train, y_train)\n",
    "\n",
    "model_filename = os.path.join(model_dir, \"xgb_nopca_noaug_nonorm.pickle\")\n",
    "with open(model_filename, \"wb\") as f:\n",
    "    pickle.dump(xgb_no_pca, f)\n",
    "\n",
    "# =========================\n",
    "# SCENARIO C: WITH PCA (aug)\n",
    "# =========================\n",
    "pca = PCA(n_components=0.99)\n",
    "X_train_aug_pca = pca.fit_transform(X_train_aug)\n",
    "X_test_aug_pca = pca.transform(X_test_aug)\n",
    "\n",
    "pca_filename = os.path.join(model_dir, \"pca_aug_nonorm.pickle\")\n",
    "with open(pca_filename, \"wb\") as f:\n",
    "    pickle.dump(pca, f)\n",
    "\n",
    "xgb_model_pca = XGBClassifier(n_estimators=n_estimators, max_depth=max_depth, learning_rate=learning_rate,\n",
    "                              subsample=subsample, colsample_bytree=colsample_bytree,\n",
    "                              eval_metric='mlogloss', random_state=42)\n",
    "xgb_model_pca.fit(X_train_aug_pca, y_train_aug)\n",
    "\n",
    "model_filename = os.path.join(model_dir, \"xgb_pca_aug_nonorm.pickle\")\n",
    "with open(model_filename, \"wb\") as f:\n",
    "    pickle.dump(xgb_model_pca, f)\n",
    "\n",
    "# =========================\n",
    "# SCENARIO D: WITHOUT PCA (aug)\n",
    "# =========================\n",
    "xgb_model_no_pca = XGBClassifier(n_estimators=n_estimators, max_depth=max_depth, learning_rate=learning_rate,\n",
    "                                 subsample=subsample, colsample_bytree=colsample_bytree,\n",
    "                                 eval_metric='mlogloss', random_state=42)\n",
    "xgb_model_no_pca.fit(X_train_aug, y_train_aug)\n",
    "\n",
    "model_filename = os.path.join(model_dir, \"xgb_nopca_aug_nonorm.pickle\")\n",
    "with open(model_filename, \"wb\") as f:\n",
    "    pickle.dump(xgb_model_no_pca, f)\n",
    "\n",
    "# =========================\n",
    "# SCENARIO E: NO DATA TRANSFORMATION (no aug)\n",
    "# =========================\n",
    "xgb_model_no_transform = XGBClassifier(n_estimators=n_estimators, max_depth=max_depth, learning_rate=learning_rate,\n",
    "                                       subsample=subsample, colsample_bytree=colsample_bytree,\n",
    "                                       eval_metric='mlogloss', random_state=42)\n",
    "xgb_model_no_transform.fit(X_train, y_train)\n",
    "\n",
    "model_filename = os.path.join(model_dir, \"xgb_nopca_noaug_nonorm.pickle\")\n",
    "with open(model_filename, \"wb\") as f:\n",
    "    pickle.dump(xgb_model_no_transform, f)\n",
    "\n",
    "# =========================\n",
    "# SCENARIO F: NORMALIZATION + PCA (no aug)\n",
    "# =========================\n",
    "X_train_norm = np.array([x/np.linalg.norm(x) if np.linalg.norm(x) != 0 else x for x in X_train])\n",
    "X_test_norm = np.array([x/np.linalg.norm(x) if np.linalg.norm(x) != 0 else x for x in X_test])\n",
    "\n",
    "pca = PCA(n_components=0.99)\n",
    "X_train_norm_pca = pca.fit_transform(X_train_norm)\n",
    "X_test_norm_pca = pca.transform(X_test_norm)\n",
    "\n",
    "pca_filename = os.path.join(model_dir, \"pca_noaug_norm.pickle\")\n",
    "with open(pca_filename, \"wb\") as f:\n",
    "    pickle.dump(pca, f)\n",
    "\n",
    "xgb_model_norm_pca = XGBClassifier(n_estimators=n_estimators, max_depth=max_depth, learning_rate=learning_rate,\n",
    "                                   subsample=subsample, colsample_bytree=colsample_bytree,\n",
    "                                   eval_metric='mlogloss', random_state=42)\n",
    "xgb_model_norm_pca.fit(X_train_norm_pca, y_train)\n",
    "\n",
    "model_filename = os.path.join(model_dir, \"xgb_pca_noaug_norm.pickle\")\n",
    "with open(model_filename, \"wb\") as f:\n",
    "    pickle.dump(xgb_model_norm_pca, f)\n",
    "\n",
    "# =========================\n",
    "# SCENARIO G: NORMALIZATION + AUG + PCA\n",
    "# =========================\n",
    "X_train_aug_norm = np.array([x/np.linalg.norm(x) if np.linalg.norm(x) != 0 else x for x in X_train_aug])\n",
    "X_test_aug_norm = np.array([x/np.linalg.norm(x) if np.linalg.norm(x) != 0 else x for x in X_test_aug])\n",
    "\n",
    "pca = PCA(n_components=0.99)\n",
    "X_train_aug_norm_pca = pca.fit_transform(X_train_aug_norm)\n",
    "X_test_aug_norm_pca = pca.transform(X_test_aug_norm)\n",
    "\n",
    "pca_filename = os.path.join(model_dir, \"pca_aug_norm.pickle\")\n",
    "with open(pca_filename, \"wb\") as f:\n",
    "    pickle.dump(pca, f)\n",
    "\n",
    "xgb_model_norm_aug_pca = XGBClassifier(n_estimators=n_estimators, max_depth=max_depth, learning_rate=learning_rate,\n",
    "                                       subsample=subsample, colsample_bytree=colsample_bytree,\n",
    "                                       eval_metric='mlogloss', random_state=42)\n",
    "xgb_model_norm_aug_pca.fit(X_train_aug_norm_pca, y_train_aug)\n",
    "\n",
    "model_filename = os.path.join(model_dir, \"xgb_pca_aug_norm.pickle\")\n",
    "with open(model_filename, \"wb\") as f:\n",
    "    pickle.dump(xgb_model_norm_aug_pca, f)\n",
    "\n",
    "# =========================\n",
    "# EVALUATION FUNCTION\n",
    "# =========================\n",
    "def evaluate_model(model, X_test, y_test, description):\n",
    "    predict = model.predict(X_test)\n",
    "\n",
    "    classes = np.unique(y_test)\n",
    "    precision_per_class = precision_score(y_test, predict, average=None, labels=classes)\n",
    "    recall_per_class = recall_score(y_test, predict, average=None, labels=classes)\n",
    "    test_accuracy_per_class = []\n",
    "    conf_matrix = confusion_matrix(y_test, predict, labels=classes)\n",
    "\n",
    "    for i, cls in enumerate(classes):\n",
    "        acc = conf_matrix[i, i] / conf_matrix[i, :].sum()\n",
    "        test_accuracy_per_class.append(acc)\n",
    "\n",
    "    cv_scores = cross_val_score(model, X_test, y_test, cv=5, scoring='accuracy')\n",
    "    mean_cv_accuracy = np.mean(cv_scores)\n",
    "\n",
    "    print(f\"\\n=== {description} ===\")\n",
    "    print(f\"Test Accuracy (Overall): {np.mean(predict == y_test):.4f}\")\n",
    "    print(f\"Mean CV Accuracy: {mean_cv_accuracy:.4f}\")\n",
    "\n",
    "    print(\"\\nPer-Class Metrics:\")\n",
    "    for i, cls in enumerate(classes):\n",
    "        print(f\"Class {cls}: Precision={precision_per_class[i]:.4f}, Recall={recall_per_class[i]:.4f}, Accuracy={test_accuracy_per_class[i]:.4f}\")\n",
    "\n",
    "# =========================\n",
    "# EVALUATE ALL MODELS\n",
    "# =========================\n",
    "evaluate_model(xgb_pca, X_test_pca, y_test, \"Scenario A: PCA NOAUG NONORM\")\n",
    "evaluate_model(xgb_no_pca, X_test, y_test, \"Scenario B: NOPCA NOAUG NONORM\")\n",
    "evaluate_model(xgb_model_pca, X_test_aug_pca, y_test_aug, \"Scenario C: PCA AUG NONORM\")\n",
    "evaluate_model(xgb_model_no_pca, X_test, y_test, \"Scenario D: NOPCA AUG NONORM\")\n",
    "evaluate_model(xgb_model_no_transform, X_test, y_test, \"Scenario E: NOPCA NOAUG NONORM\")\n",
    "evaluate_model(xgb_model_norm_pca, X_test_norm_pca, y_test, \"Scenario F: PCA NOAUG NORM\")\n",
    "evaluate_model(xgb_model_norm_aug_pca, X_test_aug_norm_pca, y_test_aug, \"Scenario G: PCA AUG NORM\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HYPERPARAMETER TUNING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Bayesian Optimization...\n",
      "|   iter    |  target   | colsam... | learni... | max_depth | n_esti... | subsample |\n",
      "-------------------------------------------------------------------------------------\n",
      "| \u001b[39m1        \u001b[39m | \u001b[39m0.6035   \u001b[39m | \u001b[39m0.6873   \u001b[39m | \u001b[39m0.2857   \u001b[39m | \u001b[39m11.52    \u001b[39m | \u001b[39m259.5    \u001b[39m | \u001b[39m0.578    \u001b[39m |\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 93\u001b[0m\n\u001b[1;32m     90\u001b[0m best_score_so_far \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1.0\u001b[39m\n\u001b[1;32m     91\u001b[0m early_stop_threshold \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.90\u001b[39m  \u001b[38;5;66;03m# Stop if we exceed 90% cross-val accuracy\u001b[39;00m\n\u001b[0;32m---> 93\u001b[0m \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmaximize\u001b[49m\u001b[43m(\u001b[49m\u001b[43minit_points\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minit_points\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_iter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_iter\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, res \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(optimizer\u001b[38;5;241m.\u001b[39mres):\n\u001b[1;32m     96\u001b[0m     score \u001b[38;5;241m=\u001b[39m res[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtarget\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[0;32m~/master_projet_elec/LELEC210X/.venv/lib/python3.9/site-packages/bayes_opt/bayesian_optimization.py:338\u001b[0m, in \u001b[0;36mBayesianOptimization.maximize\u001b[0;34m(self, init_points, n_iter)\u001b[0m\n\u001b[1;32m    336\u001b[0m     x_probe \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msuggest()\n\u001b[1;32m    337\u001b[0m     iteration \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m--> 338\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprobe\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_probe\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlazy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    340\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bounds_transformer \u001b[38;5;129;01mand\u001b[39;00m iteration \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    341\u001b[0m     \u001b[38;5;66;03m# The bounds transformer should only modify the bounds after\u001b[39;00m\n\u001b[1;32m    342\u001b[0m     \u001b[38;5;66;03m# the init_points points (only for the true iterations)\u001b[39;00m\n\u001b[1;32m    343\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mset_bounds(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bounds_transformer\u001b[38;5;241m.\u001b[39mtransform(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_space))\n",
      "File \u001b[0;32m~/master_projet_elec/LELEC210X/.venv/lib/python3.9/site-packages/bayes_opt/bayesian_optimization.py:270\u001b[0m, in \u001b[0;36mBayesianOptimization.probe\u001b[0;34m(self, params, lazy)\u001b[0m\n\u001b[1;32m    268\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_queue\u001b[38;5;241m.\u001b[39mappend(params)\n\u001b[1;32m    269\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 270\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_space\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprobe\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    271\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdispatch(Events\u001b[38;5;241m.\u001b[39mOPTIMIZATION_STEP)\n",
      "File \u001b[0;32m~/master_projet_elec/LELEC210X/.venv/lib/python3.9/site-packages/bayes_opt/target_space.py:418\u001b[0m, in \u001b[0;36mTargetSpace.probe\u001b[0;34m(self, params)\u001b[0m\n\u001b[1;32m    416\u001b[0m     error_msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo target function has been provided.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    417\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(error_msg)\n\u001b[0;32m--> 418\u001b[0m target \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mdict_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    420\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_constraint \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    421\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mregister(x, target)\n",
      "Cell \u001b[0;32mIn[10], line 62\u001b[0m, in \u001b[0;36mxgb_cv\u001b[0;34m(n_estimators, max_depth, learning_rate, subsample, colsample_bytree)\u001b[0m\n\u001b[1;32m     55\u001b[0m model \u001b[38;5;241m=\u001b[39m XGBClassifier(n_estimators\u001b[38;5;241m=\u001b[39mn_estimators, max_depth\u001b[38;5;241m=\u001b[39mmax_depth, learning_rate\u001b[38;5;241m=\u001b[39mlearning_rate, subsample\u001b[38;5;241m=\u001b[39msubsample, colsample_bytree\u001b[38;5;241m=\u001b[39mcolsample_bytree,\n\u001b[1;32m     56\u001b[0m     eval_metric\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmlogloss\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     57\u001b[0m     \u001b[38;5;66;03m# remove use_label_encoder (deprecated)\u001b[39;00m\n\u001b[1;32m     58\u001b[0m     random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m\n\u001b[1;32m     59\u001b[0m )\n\u001b[1;32m     61\u001b[0m \u001b[38;5;66;03m# 5-fold cross-validation on the *entire dataset* X, y\u001b[39;00m\n\u001b[0;32m---> 62\u001b[0m cv_scores \u001b[38;5;241m=\u001b[39m \u001b[43mcross_val_score\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcv\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscoring\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43maccuracy\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;66;03m# Return the mean of cross-validation accuracy\u001b[39;00m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m cv_scores\u001b[38;5;241m.\u001b[39mmean()\n",
      "File \u001b[0;32m~/master_projet_elec/LELEC210X/.venv/lib/python3.9/site-packages/sklearn/utils/_param_validation.py:216\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    210\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    211\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m    212\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m    213\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m    214\u001b[0m         )\n\u001b[1;32m    215\u001b[0m     ):\n\u001b[0;32m--> 216\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    217\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[1;32m    219\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[1;32m    220\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[1;32m    221\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[1;32m    222\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[1;32m    223\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    224\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    225\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[1;32m    226\u001b[0m     )\n",
      "File \u001b[0;32m~/master_projet_elec/LELEC210X/.venv/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:684\u001b[0m, in \u001b[0;36mcross_val_score\u001b[0;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, params, pre_dispatch, error_score)\u001b[0m\n\u001b[1;32m    681\u001b[0m \u001b[38;5;66;03m# To ensure multimetric format is not supported\u001b[39;00m\n\u001b[1;32m    682\u001b[0m scorer \u001b[38;5;241m=\u001b[39m check_scoring(estimator, scoring\u001b[38;5;241m=\u001b[39mscoring)\n\u001b[0;32m--> 684\u001b[0m cv_results \u001b[38;5;241m=\u001b[39m \u001b[43mcross_validate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    685\u001b[0m \u001b[43m    \u001b[49m\u001b[43mestimator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    686\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    687\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    688\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgroups\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    689\u001b[0m \u001b[43m    \u001b[49m\u001b[43mscoring\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mscore\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mscorer\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    690\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcv\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    691\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    692\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    693\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    694\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpre_dispatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpre_dispatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    695\u001b[0m \u001b[43m    \u001b[49m\u001b[43merror_score\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merror_score\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    696\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    697\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m cv_results[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_score\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m~/master_projet_elec/LELEC210X/.venv/lib/python3.9/site-packages/sklearn/utils/_param_validation.py:216\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    210\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    211\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m    212\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m    213\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m    214\u001b[0m         )\n\u001b[1;32m    215\u001b[0m     ):\n\u001b[0;32m--> 216\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    217\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[1;32m    219\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[1;32m    220\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[1;32m    221\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[1;32m    222\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[1;32m    223\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    224\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    225\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[1;32m    226\u001b[0m     )\n",
      "File \u001b[0;32m~/master_projet_elec/LELEC210X/.venv/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:411\u001b[0m, in \u001b[0;36mcross_validate\u001b[0;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, params, pre_dispatch, return_train_score, return_estimator, return_indices, error_score)\u001b[0m\n\u001b[1;32m    408\u001b[0m \u001b[38;5;66;03m# We clone the estimator to make sure that all the folds are\u001b[39;00m\n\u001b[1;32m    409\u001b[0m \u001b[38;5;66;03m# independent, and that it is pickle-able.\u001b[39;00m\n\u001b[1;32m    410\u001b[0m parallel \u001b[38;5;241m=\u001b[39m Parallel(n_jobs\u001b[38;5;241m=\u001b[39mn_jobs, verbose\u001b[38;5;241m=\u001b[39mverbose, pre_dispatch\u001b[38;5;241m=\u001b[39mpre_dispatch)\n\u001b[0;32m--> 411\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mparallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    412\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_fit_and_score\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    413\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    414\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    415\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    416\u001b[0m \u001b[43m        \u001b[49m\u001b[43mscorer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscorers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    417\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    418\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtest\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    419\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    420\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparameters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    421\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfit_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrouted_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mestimator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    422\u001b[0m \u001b[43m        \u001b[49m\u001b[43mscore_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrouted_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscorer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscore\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    423\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_train_score\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_train_score\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    424\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_times\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    425\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_estimator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_estimator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    426\u001b[0m \u001b[43m        \u001b[49m\u001b[43merror_score\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merror_score\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    427\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    428\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mindices\u001b[49m\n\u001b[1;32m    429\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    431\u001b[0m _warn_or_raise_about_fit_failures(results, error_score)\n\u001b[1;32m    433\u001b[0m \u001b[38;5;66;03m# For callable scoring, the return type is only know after calling. If the\u001b[39;00m\n\u001b[1;32m    434\u001b[0m \u001b[38;5;66;03m# return type is a dictionary, the error scores can now be inserted with\u001b[39;00m\n\u001b[1;32m    435\u001b[0m \u001b[38;5;66;03m# the correct key.\u001b[39;00m\n",
      "File \u001b[0;32m~/master_projet_elec/LELEC210X/.venv/lib/python3.9/site-packages/sklearn/utils/parallel.py:77\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     72\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[1;32m     73\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     74\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[1;32m     75\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[1;32m     76\u001b[0m )\n\u001b[0;32m---> 77\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/master_projet_elec/LELEC210X/.venv/lib/python3.9/site-packages/joblib/parallel.py:1918\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1916\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_sequential_output(iterable)\n\u001b[1;32m   1917\u001b[0m     \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[0;32m-> 1918\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1920\u001b[0m \u001b[38;5;66;03m# Let's create an ID that uniquely identifies the current call. If the\u001b[39;00m\n\u001b[1;32m   1921\u001b[0m \u001b[38;5;66;03m# call is interrupted early and that the same instance is immediately\u001b[39;00m\n\u001b[1;32m   1922\u001b[0m \u001b[38;5;66;03m# re-used, this id will be used to prevent workers that were\u001b[39;00m\n\u001b[1;32m   1923\u001b[0m \u001b[38;5;66;03m# concurrently finalizing a task from the previous call to run the\u001b[39;00m\n\u001b[1;32m   1924\u001b[0m \u001b[38;5;66;03m# callback.\u001b[39;00m\n\u001b[1;32m   1925\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n",
      "File \u001b[0;32m~/master_projet_elec/LELEC210X/.venv/lib/python3.9/site-packages/joblib/parallel.py:1847\u001b[0m, in \u001b[0;36mParallel._get_sequential_output\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1845\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_batches \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1846\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m-> 1847\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1848\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_completed_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1849\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprint_progress()\n",
      "File \u001b[0;32m~/master_projet_elec/LELEC210X/.venv/lib/python3.9/site-packages/sklearn/utils/parallel.py:139\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    137\u001b[0m     config \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m    138\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig):\n\u001b[0;32m--> 139\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/master_projet_elec/LELEC210X/.venv/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:866\u001b[0m, in \u001b[0;36m_fit_and_score\u001b[0;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, score_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, split_progress, candidate_progress, error_score)\u001b[0m\n\u001b[1;32m    864\u001b[0m         estimator\u001b[38;5;241m.\u001b[39mfit(X_train, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\n\u001b[1;32m    865\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 866\u001b[0m         \u001b[43mestimator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    868\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m    869\u001b[0m     \u001b[38;5;66;03m# Note fit time as time until error\u001b[39;00m\n\u001b[1;32m    870\u001b[0m     fit_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start_time\n",
      "File \u001b[0;32m~/master_projet_elec/LELEC210X/.venv/lib/python3.9/site-packages/xgboost/core.py:726\u001b[0m, in \u001b[0;36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    724\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sig\u001b[38;5;241m.\u001b[39mparameters, args):\n\u001b[1;32m    725\u001b[0m     kwargs[k] \u001b[38;5;241m=\u001b[39m arg\n\u001b[0;32m--> 726\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/master_projet_elec/LELEC210X/.venv/lib/python3.9/site-packages/xgboost/sklearn.py:1599\u001b[0m, in \u001b[0;36mXGBClassifier.fit\u001b[0;34m(self, X, y, sample_weight, base_margin, eval_set, verbose, xgb_model, sample_weight_eval_set, base_margin_eval_set, feature_weights)\u001b[0m\n\u001b[1;32m   1579\u001b[0m model, metric, params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_configure_fit(xgb_model, params)\n\u001b[1;32m   1580\u001b[0m train_dmatrix, evals \u001b[38;5;241m=\u001b[39m _wrap_evaluation_matrices(\n\u001b[1;32m   1581\u001b[0m     missing\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmissing,\n\u001b[1;32m   1582\u001b[0m     X\u001b[38;5;241m=\u001b[39mX,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1596\u001b[0m     feature_types\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeature_types,\n\u001b[1;32m   1597\u001b[0m )\n\u001b[0;32m-> 1599\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_Booster \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1600\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1601\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_dmatrix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1602\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_num_boosting_rounds\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1603\u001b[0m \u001b[43m    \u001b[49m\u001b[43mevals\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mevals\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1604\u001b[0m \u001b[43m    \u001b[49m\u001b[43mearly_stopping_rounds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mearly_stopping_rounds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1605\u001b[0m \u001b[43m    \u001b[49m\u001b[43mevals_result\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mevals_result\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1606\u001b[0m \u001b[43m    \u001b[49m\u001b[43mobj\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1607\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcustom_metric\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetric\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1608\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1609\u001b[0m \u001b[43m    \u001b[49m\u001b[43mxgb_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1610\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1611\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1613\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobjective):\n\u001b[1;32m   1614\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobjective \u001b[38;5;241m=\u001b[39m params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mobjective\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m~/master_projet_elec/LELEC210X/.venv/lib/python3.9/site-packages/xgboost/core.py:726\u001b[0m, in \u001b[0;36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    724\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sig\u001b[38;5;241m.\u001b[39mparameters, args):\n\u001b[1;32m    725\u001b[0m     kwargs[k] \u001b[38;5;241m=\u001b[39m arg\n\u001b[0;32m--> 726\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/master_projet_elec/LELEC210X/.venv/lib/python3.9/site-packages/xgboost/training.py:181\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(params, dtrain, num_boost_round, evals, obj, feval, maximize, early_stopping_rounds, evals_result, verbose_eval, xgb_model, callbacks, custom_metric)\u001b[0m\n\u001b[1;32m    179\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cb_container\u001b[38;5;241m.\u001b[39mbefore_iteration(bst, i, dtrain, evals):\n\u001b[1;32m    180\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m--> 181\u001b[0m \u001b[43mbst\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43miteration\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfobj\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cb_container\u001b[38;5;241m.\u001b[39mafter_iteration(bst, i, dtrain, evals):\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/master_projet_elec/LELEC210X/.venv/lib/python3.9/site-packages/xgboost/core.py:2101\u001b[0m, in \u001b[0;36mBooster.update\u001b[0;34m(self, dtrain, iteration, fobj)\u001b[0m\n\u001b[1;32m   2097\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_assign_dmatrix_features(dtrain)\n\u001b[1;32m   2099\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m fobj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   2100\u001b[0m     _check_call(\n\u001b[0;32m-> 2101\u001b[0m         \u001b[43m_LIB\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mXGBoosterUpdateOneIter\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2102\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mc_int\u001b[49m\u001b[43m(\u001b[49m\u001b[43miteration\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtrain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle\u001b[49m\n\u001b[1;32m   2103\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2104\u001b[0m     )\n\u001b[1;32m   2105\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2106\u001b[0m     pred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredict(dtrain, output_margin\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import cross_val_score, train_test_split\n",
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score\n",
    "from bayes_opt import BayesianOptimization\n",
    "\n",
    "# Your custom accuracy function\n",
    "from classification.utils.utils import accuracy\n",
    "\n",
    "# --- CONFIG FLAGS ---\n",
    "NORMALIZATION = True\n",
    "TRANSFORMATION = True\n",
    "\n",
    "# --- STEP 1: Load/Select Data ---\n",
    "if TRANSFORMATION:\n",
    "    try:\n",
    "        X = X_basic_aug   # Make sure X_basic_aug is defined in your environment\n",
    "        y = y_basic_aug   # Make sure y_basic_aug is defined in your environment\n",
    "    except NameError:\n",
    "        raise ValueError(\"X_aug and y_aug must be defined before running this script.\")\n",
    "else:\n",
    "    try:\n",
    "        X = X_basic       # Make sure X_basic is defined in your environment\n",
    "        y = y_basic       # Make sure y_basic is defined in your environment\n",
    "    except NameError:\n",
    "        raise ValueError(\"X and y must be defined before running this script.\")\n",
    "\n",
    "# Optional normalization\n",
    "if NORMALIZATION:\n",
    "    X = np.array([\n",
    "        x / np.linalg.norm(x) if np.linalg.norm(x) != 0 else x\n",
    "        for x in X\n",
    "    ])\n",
    "\n",
    "# --- STEP 2: Define the Objective Function for Bayesian Optimization ---\n",
    "def xgb_cv(\n",
    "    n_estimators,\n",
    "    max_depth,\n",
    "    learning_rate,\n",
    "    subsample,\n",
    "    colsample_bytree\n",
    "):\n",
    "    \"\"\"\n",
    "    This function trains an XGBClassifier with given hyperparameters\n",
    "    and returns the mean CV accuracy as the objective to maximize.\n",
    "    \"\"\"\n",
    "    # Convert some parameters to int, as required by XGBoost\n",
    "    n_estimators = int(n_estimators)\n",
    "    max_depth = int(max_depth)\n",
    "\n",
    "    model = XGBClassifier(n_estimators=n_estimators, max_depth=max_depth, learning_rate=learning_rate, subsample=subsample, colsample_bytree=colsample_bytree,\n",
    "        eval_metric='mlogloss',\n",
    "        # remove use_label_encoder (deprecated)\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    # 5-fold cross-validation on the *entire dataset* X, y\n",
    "    cv_scores = cross_val_score(model, X, y, cv=5, scoring='accuracy')\n",
    "    \n",
    "    # Return the mean of cross-validation accuracy\n",
    "    return cv_scores.mean()\n",
    "\n",
    "# --- STEP 3: Set Up the Bayesian Optimizer ---\n",
    "# Hyperparameter search space\n",
    "pbounds = {\n",
    "    'n_estimators': (50, 400),      # e.g. from 50 to 300\n",
    "    'max_depth': (2, 15),           # integer between 2 and 12\n",
    "    'learning_rate': (0.01, 0.3),   # from 0.01 to 0.3\n",
    "    'subsample': (0.5, 1),        # from 0.5 to 1.0\n",
    "    'colsample_bytree': (0.5, 1)  # from 0.5 to 1.0\n",
    "}\n",
    "\n",
    "optimizer = BayesianOptimization(\n",
    "    f=xgb_cv,        # The function we want to maximize\n",
    "    pbounds=pbounds, # The search space\n",
    "    random_state=42  # Ensures reproducibility\n",
    ")\n",
    "\n",
    "# --- STEP 4: Run the Bayesian Optimization Loop ---\n",
    "# We'll do a few initial random explorations (init_points) \n",
    "# and then a certain number of optimization steps (n_iter).\n",
    "init_points = 3\n",
    "n_iter = 20\n",
    "\n",
    "print(\"Starting Bayesian Optimization...\")\n",
    "best_score_so_far = -1.0\n",
    "early_stop_threshold = 0.90  # Stop if we exceed 90% cross-val accuracy\n",
    "\n",
    "optimizer.maximize(init_points=init_points, n_iter=n_iter)\n",
    "\n",
    "for i, res in enumerate(optimizer.res):\n",
    "    score = res['target']\n",
    "    print(f\"Iteration {i+1}, CV Accuracy: {score:.4f}, Parameters: {res['params']}\")\n",
    "    \n",
    "    if score > best_score_so_far:\n",
    "        best_score_so_far = score\n",
    "    \n",
    "    # Early stopping if we found a \"good\" configuration\n",
    "    if best_score_so_far > early_stop_threshold:\n",
    "        print(f\"\\nEarly stopping: Found cross-validation accuracy above {early_stop_threshold}\\n\")\n",
    "        break\n",
    "\n",
    "# --- STEP 5: Get the Best Found Hyperparameters ---\n",
    "best_params = optimizer.max['params']\n",
    "best_n_estimators = int(best_params['n_estimators'])\n",
    "best_max_depth = int(best_params['max_depth'])\n",
    "best_learning_rate = best_params['learning_rate']\n",
    "best_subsample = best_params['subsample']\n",
    "best_colsample_bytree = best_params['colsample_bytree']\n",
    "\n",
    "print(\"\\n=== BEST HYPERPARAMETERS FOUND ===\")\n",
    "print(f\"n_estimators = {best_n_estimators}\")\n",
    "print(f\"max_depth = {best_max_depth}\")\n",
    "print(f\"learning_rate = {best_learning_rate:.4f}\")\n",
    "print(f\"subsample = {best_subsample:.4f}\")\n",
    "print(f\"colsample_bytree = {best_colsample_bytree:.4f}\")\n",
    "print(f\"CV Accuracy = {optimizer.max['target']:.4f}\")\n",
    "\n",
    "# --- STEP 6: Train/Validate Model Once More on a Train/Test Split ---\n",
    "# Final check on a separate holdout set\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, stratify=y, random_state=999\n",
    ")\n",
    "\n",
    "final_model = XGBClassifier(\n",
    "    n_estimators=best_n_estimators,\n",
    "    max_depth=best_max_depth,\n",
    "    learning_rate=best_learning_rate,\n",
    "    subsample=best_subsample,\n",
    "    colsample_bytree=best_colsample_bytree,\n",
    "    eval_metric='mlogloss',\n",
    "    random_state=999\n",
    ")\n",
    "\n",
    "final_model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = final_model.predict(X_test)\n",
    "test_acc = accuracy(y_pred, y_test)\n",
    "\n",
    "print(\"\\n=== FINAL EVALUATION ON HOLDOUT TEST SET ===\")\n",
    "print(f\"Test Accuracy: {test_acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MEAN ACCURACY ON 100 ITERATIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Iteration 1/3\n",
      "Test Accuracy: 0.7674 | Mean CV Accuracy: 0.6834\n",
      "\n",
      "Iteration 2/3\n",
      "Test Accuracy: 0.7326 | Mean CV Accuracy: 0.7942\n",
      "\n",
      "Iteration 3/3\n",
      "Test Accuracy: 0.6279 | Mean CV Accuracy: 0.7337\n",
      "\n",
      "=== FINAL RESULTS AFTER 20 ITERATIONS ===\n",
      "Mean Test Accuracy: 0.7093  0.0593\n",
      "Mean Cross-Validation Accuracy: 0.7371  0.0453\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score\n",
    "from classification.utils.utils import accuracy\n",
    "\n",
    "\n",
    "NORMALIZATION = True\n",
    "TRANSFORMATION = True\n",
    "\n",
    "# Ensure dataset (X_aug, y_aug) exists\n",
    "if TRANSFORMATION:\n",
    "    try:\n",
    "        X = X_basic_aug\n",
    "        y = y_basic_aug\n",
    "    except NameError:\n",
    "        raise ValueError(\"X_aug and y_aug must be defined before running this script.\")\n",
    "else:\n",
    "    try:\n",
    "        X = X_basic\n",
    "        y = y_basic\n",
    "    except NameError:\n",
    "        raise ValueError(\"X and y must be defined before running this script.\")\n",
    "\n",
    "# Normalize if needed\n",
    "if NORMALIZATION:\n",
    "    X = np.array([x / np.linalg.norm(x) if np.linalg.norm(x) != 0 else x for x in X])\n",
    "\n",
    "# Number of iterations\n",
    "num_iterations = 3\n",
    "\n",
    "# Lists to store scores\n",
    "accuracy_scores = []\n",
    "cv_accuracy_scores = []\n",
    "\n",
    "for i in range(num_iterations):\n",
    "    print(f\"\\nIteration {i + 1}/{num_iterations}\")\n",
    "    \n",
    "    # Split the dataset into training and testing subsets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.3, stratify=y, random_state=i\n",
    "    )\n",
    "\n",
    "    # Train the XGBoost model\n",
    "    model = XGBClassifier(\n",
    "        n_estimators=200,\n",
    "        max_depth=5,\n",
    "        learning_rate=0.1,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        eval_metric='mlogloss',\n",
    "        random_state=i\n",
    "    )\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # Make predictions\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    # Compute overall accuracy\n",
    "    test_accuracy = accuracy(y_pred, y_test)\n",
    "    accuracy_scores.append(test_accuracy)\n",
    "\n",
    "    # Perform cross-validation on the training set\n",
    "    cv_scores = cross_val_score(model, X_train, y_train, cv=10, scoring='accuracy')\n",
    "    mean_cv_accuracy = np.mean(cv_scores)\n",
    "    cv_accuracy_scores.append(mean_cv_accuracy)\n",
    "\n",
    "    print(f\"Test Accuracy: {test_accuracy:.4f} | Mean CV Accuracy: {mean_cv_accuracy:.4f}\")\n",
    "\n",
    "# Compute overall statistics\n",
    "mean_test_accuracy = np.mean(accuracy_scores)\n",
    "std_test_accuracy = np.std(accuracy_scores)\n",
    "\n",
    "mean_cv_accuracy = np.mean(cv_accuracy_scores)\n",
    "std_cv_accuracy = np.std(cv_accuracy_scores)\n",
    "\n",
    "# Print final results\n",
    "print(\"\\n=== FINAL RESULTS AFTER 20 ITERATIONS ===\")\n",
    "print(f\"Mean Test Accuracy: {mean_test_accuracy:.4f}  {std_test_accuracy:.4f}\")\n",
    "print(f\"Mean Cross-Validation Accuracy: {mean_cv_accuracy:.4f}  {std_cv_accuracy:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
