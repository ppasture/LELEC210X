{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "\"Machine learning tools\"\n",
    "import pickle\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "\n",
    "\n",
    "from classification.datasets import Dataset\n",
    "from classification.utils.audio_student import AudioUtil, Feature_vector_DS\n",
    "\n",
    "from classification.utils.plots import (\n",
    "    plot_decision_boundaries,\n",
    "    plot_specgram,\n",
    "    show_confusion_matrix,\n",
    ")\n",
    "from classification.utils.utils import accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chainsaw\n",
      "fire\n",
      "fireworks\n",
      "gunshot\n"
     ]
    }
   ],
   "source": [
    "### TO RUN\n",
    "dataset = Dataset()\n",
    "classnames = dataset.list_classes()\n",
    "\n",
    "print(\"\\n\".join(classnames))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TO RUN\n",
    "fm_dir = \"data/feature_matrices/\"  # where to save the features matrices\n",
    "new_dataset_dir = \"src/classification/datasets/new_dataset/melvecs/\"\n",
    "model_dir = \"data/models/xgboost\"  # where to save the models\n",
    "os.makedirs(fm_dir, exist_ok=True)\n",
    "os.makedirs(model_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TO RUN\n",
    "\n",
    "\"Creation of the dataset\"\n",
    "myds = Feature_vector_DS(dataset, Nft=512, nmel=20, duration=950, shift_pct=0.0)\n",
    "\n",
    "\"Some attributes...\"\n",
    "myds.nmel\n",
    "myds.duration\n",
    "myds.shift_pct\n",
    "myds.sr\n",
    "myds.data_aug\n",
    "myds.ncol\n",
    "\n",
    "idx = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRANSFORMATION ON FEATURE VECTOR\n",
    "\n",
    "def add_noise(feature_vector, snr_db=15):\n",
    "    \"\"\"Adds white noise to a feature vector based on the given SNR (Signal-to-Noise Ratio).\"\"\"\n",
    "    power_signal = np.mean(feature_vector ** 2)\n",
    "    power_noise = power_signal / (10 ** (snr_db / 10))\n",
    "    noise = np.random.normal(0, np.sqrt(power_noise), feature_vector.shape)\n",
    "    return feature_vector + noise\n",
    "\n",
    "def shifting(feature_vector, shift_max=20):\n",
    "    \"\"\"Shifts mel spectrogram feature vectors along the time axis by a random shift between 0 and shift_max.\"\"\"\n",
    "    shift = np.random.randint(0, shift_max)\n",
    "    return np.roll(feature_vector, shift, axis=0)  # Rolling along the first axis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the basic feature matrix : (268, 400)\n",
      "Number of labels : (268,)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "\n",
    "train_pct = 0.7\n",
    "data_aug_factor = 1\n",
    "featveclen = len(myds[\"fire\", 0, \"\", \"\"])  # Same for all classes\n",
    "classnames = [\"chainsaw\", \"fire\", \"fireworks\", \"gunshot\"]  # Or wherever you store class names\n",
    "nclass = len(classnames)\n",
    "\n",
    "# Determine number of samples per class\n",
    "naudio_per_class = {\"chainsaw\" : 76, \"fire\" : 76, \"fireworks\" : 76, \"gunshot\" : 40}\n",
    "\n",
    "\n",
    "# Allocate feature matrix\n",
    "total_samples_basic = sum(naudio_per_class[c] for c in classnames)\n",
    "X_basic = np.zeros((total_samples_basic, featveclen))\n",
    "y_basic = np.zeros((total_samples_basic), dtype=object)\n",
    "total_samples_basic\n",
    "# Fill feature matrix\n",
    "idx = 0\n",
    "for class_idx, classname in enumerate(classnames):\n",
    "    for i in range(naudio_per_class[classname]):\n",
    "        featvec = myds[classname, i, \"\", \"\"]\n",
    "        X_basic[idx, :] = featvec\n",
    "        y_basic[idx] = classname\n",
    "        idx += 1\n",
    "\n",
    "# Save features and labels\n",
    "np.save(fm_dir + \"X_basic.npy\", X_basic)\n",
    "np.save(fm_dir + \"y_basic.npy\", y_basic)\n",
    "\n",
    "print(f\"Shape of the basic feature matrix : {X_basic.shape}\")\n",
    "print(f\"Number of labels : {y_basic.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now create a new augmented dataset and observe if the classification results improve. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of transformations :  3\n",
      "Shape of the feature matrix : (804, 400)\n",
      "Number of labels : (804,)\n",
      "------------------------------------------------------------\n",
      "Transformations: ['original', 'noise', 'shifting']. Labels aligned dynamically with class sizes.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "### AUGMENTED DATASET\n",
    "list_augmentation = [\"original\", \"noise\", \"shifting\"]\n",
    "myds.mod_data_aug(list_augmentation)\n",
    "print(\"Number of transformations : \", myds.data_aug_factor)\n",
    "\n",
    "\n",
    "# Calcul total des échantillons\n",
    "total_aug_samples = sum(naudio_per_class[c] for c in classnames) * len(list_augmentation)\n",
    "X_basic_aug = np.zeros((total_aug_samples, featveclen))\n",
    "y_basic_aug = np.zeros((total_aug_samples), dtype=object)\n",
    "\n",
    "# Remplissage des features\n",
    "idx = 0\n",
    "for aug in list_augmentation:\n",
    "    for classname in classnames:\n",
    "        for i in range(naudio_per_class[classname]):\n",
    "            featvec = myds[classname, i, aug, \"\"]\n",
    "            X_basic_aug[idx, :] = featvec\n",
    "            y_basic_aug[idx] = classname\n",
    "            idx += 1\n",
    "\n",
    "# Sauvegarde\n",
    "np.save(fm_dir + \"X_basic_aug.npy\", X_basic_aug)\n",
    "np.save(fm_dir + \"y_basic_aug.npy\", y_basic_aug)\n",
    "\n",
    "print(f\"Shape of the feature matrix : {X_basic_aug.shape}\")\n",
    "print(f\"Number of labels : {y_basic_aug.shape}\")\n",
    "print(f\"------------------------------------------------------------\")\n",
    "print(f\"Transformations: {list_augmentation}. Labels aligned dynamically with class sizes.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "RUN = False\n",
    "\n",
    "if RUN:\n",
    "    import os\n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "    from classification.utils.plots import plot_specgram_textlabel\n",
    "\n",
    "    # Charger les données\n",
    "    X = np.load(os.path.join(fm_dir, \"X_basic_aug.npy\"), allow_pickle=True)\n",
    "    y = np.load(os.path.join(fm_dir, \"y_basic_aug.npy\"), allow_pickle=True)\n",
    "\n",
    "    # Dossier où sauvegarder les images\n",
    "    save_dir = os.path.join(\"src/classification/soundfiles_melspec_augmentation\")\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "    # Nombre d'exemples de base (avant augmentation)\n",
    "    length_X_basic = int(len(X) / len(list_augmentation))\n",
    "\n",
    "    # Boucle de sauvegarde\n",
    "    for i in range(length_X_basic):\n",
    "        for j, aug_name in enumerate(list_augmentation):\n",
    "            idx = i + j * length_X_basic\n",
    "            melspec = X[idx]\n",
    "            class_of_spec = y[idx]\n",
    "\n",
    "            fig, ax = plt.subplots()\n",
    "            plot_specgram_textlabel(\n",
    "                melspec.reshape((20, 20)),\n",
    "                ax=ax,\n",
    "                is_mel=True,\n",
    "                title=f\"MEL Spectrogram #{i} - {aug_name}\",\n",
    "                xlabel=\"Mel vector\",\n",
    "                textlabel=f\"{class_of_spec} (aug: {aug_name})\",\n",
    "            )\n",
    "            plt.tight_layout()\n",
    "            save_path = os.path.join(save_dir, f\"melspec_{i}_{aug_name}.png\")\n",
    "            fig.savefig(save_path)\n",
    "            plt.close(fig)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FINAL MODEL SAVE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-14 14:09:44.993239: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-04-14 14:09:44.995186: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-04-14 14:09:44.998363: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-04-14 14:09:45.004973: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1744632585.019196     852 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1744632585.022674     852 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1744632585.035663     852 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1744632585.035689     852 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1744632585.035691     852 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1744632585.035692     852 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-04-14 14:09:45.043162: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "/home/ppasture/master_projet_elec/LELEC210X/.venv/lib/python3.9/site-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
      "2025-04-14 14:09:50.209695: E external/local_xla/xla/stream_executor/cuda/cuda_platform.cc:51] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: UNKNOWN ERROR (303)\n",
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n",
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n",
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n",
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "\n",
      "=== Scenario B: CNN NOPCA NOAUG NONORM ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.58      0.83      0.68        23\n",
      "           1       0.80      0.74      0.77        27\n",
      "           2       0.79      0.61      0.69        18\n",
      "           3       0.78      0.54      0.64        13\n",
      "\n",
      "    accuracy                           0.70        81\n",
      "   macro avg       0.73      0.68      0.69        81\n",
      "weighted avg       0.73      0.70      0.70        81\n",
      "\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "=== Scenario D: CNN NOPCA AUG NONORM ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.93      0.94        73\n",
      "           1       0.94      0.92      0.93        65\n",
      "           2       0.86      0.91      0.89        69\n",
      "           3       0.91      0.89      0.90        35\n",
      "\n",
      "    accuracy                           0.92       242\n",
      "   macro avg       0.92      0.91      0.92       242\n",
      "weighted avg       0.92      0.92      0.92       242\n",
      "\n",
      "WARNING:tensorflow:5 out of the last 12 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x7f7871ef0040> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 12 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x7f7871ef0040> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "\n",
      "=== Scenario E: CNN NOPCA NOAUG NONORM ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.78      0.82        23\n",
      "           1       0.88      0.85      0.87        27\n",
      "           2       0.75      0.83      0.79        18\n",
      "           3       0.79      0.85      0.81        13\n",
      "\n",
      "    accuracy                           0.83        81\n",
      "   macro avg       0.82      0.83      0.82        81\n",
      "weighted avg       0.83      0.83      0.83        81\n",
      "\n",
      "WARNING:tensorflow:5 out of the last 12 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x7f7871ef0790> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 12 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x7f7871ef0790> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "=== Scenario H: CNN NOPCA AUG NORM ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.30      1.00      0.46        73\n",
      "           1       0.00      0.00      0.00        65\n",
      "           2       0.00      0.00      0.00        69\n",
      "           3       0.00      0.00      0.00        35\n",
      "\n",
      "    accuracy                           0.30       242\n",
      "   macro avg       0.08      0.25      0.12       242\n",
      "weighted avg       0.09      0.30      0.14       242\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ppasture/master_projet_elec/LELEC210X/.venv/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/ppasture/master_projet_elec/LELEC210X/.venv/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/ppasture/master_projet_elec/LELEC210X/.venv/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "TEST_SET = True\n",
    "\n",
    "A = True # PCA NOAUG NONORM\n",
    "B = True # NOPCA NOAUG NONORM\n",
    "C = True # PCA AUG NONORM\n",
    "D = True # NOPCA AUG NONORM\n",
    "E = True # NOPCA NOAUG NONORM\n",
    "F = True # PCA NOAUG NORM\n",
    "G = True # PCA AUG NORM\n",
    "H = True # NOPCA AUG NORM\n",
    "\n",
    "# Load datasets\n",
    "X_basic_aug = np.load(os.path.join(fm_dir, \"X_basic_aug.npy\"))\n",
    "y_basic_aug = np.load(os.path.join(fm_dir, \"y_basic_aug.npy\"), allow_pickle=True)\n",
    "\n",
    "X_basic = np.load(os.path.join(fm_dir, \"X_basic.npy\"))\n",
    "y_basic = np.load(os.path.join(fm_dir, \"y_basic.npy\"), allow_pickle=True)\n",
    "\n",
    "# Encode labels\n",
    "label_encoder = LabelEncoder()\n",
    "y_basic = label_encoder.fit_transform(y_basic)\n",
    "y_basic_aug = label_encoder.transform(y_basic_aug)\n",
    "\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "if TEST_SET:\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_basic, y_basic, test_size=0.3, random_state=42)\n",
    "    X_train_aug, X_test_aug, y_train_aug, y_test_aug = train_test_split(X_basic_aug, y_basic_aug, test_size=0.3, random_state=42)\n",
    "else:\n",
    "    X_train = X_basic\n",
    "    y_train = y_basic\n",
    "    X_train_aug = X_basic_aug\n",
    "    y_train_aug = y_basic_aug\n",
    "\n",
    "# =========================\n",
    "# SCENARIO A: WITH PCA (no aug)\n",
    "# =========================\n",
    "if A:\n",
    "    pca = PCA(n_components=0.98)\n",
    "    X_train_pca = pca.fit_transform(X_train)\n",
    "    if TEST_SET:\n",
    "        X_test_pca = pca.transform(X_test)\n",
    "\n",
    "    pca_filename = os.path.join(model_dir, \"pca_noaug_nonorm.pickle\")\n",
    "    with open(pca_filename, \"wb\") as f:\n",
    "        pickle.dump(pca, f)\n",
    "\n",
    "    xgb_pca = XGBClassifier(n_estimators=100, max_depth=5, learning_rate=0.1,\n",
    "                            subsample=0.8, colsample_bytree=0.8, eval_metric='mlogloss', random_state=42)\n",
    "    xgb_pca.fit(X_train_pca, y_train)\n",
    "\n",
    "    model_filename = os.path.join(model_dir, \"xgb_pca_noaug_nonorm.pickle\")\n",
    "    with open(model_filename, \"wb\") as f:\n",
    "        pickle.dump(xgb_pca, f)\n",
    "\n",
    "# =========================\n",
    "# SCENARIO B: WITHOUT PCA (no aug)\n",
    "# =========================\n",
    "if B:\n",
    "    xgb_no_pca = XGBClassifier(n_estimators=100, max_depth=5, learning_rate=0.1,\n",
    "                            subsample=0.8, colsample_bytree=0.8, eval_metric='mlogloss', random_state=42)\n",
    "    xgb_no_pca.fit(X_train, y_train)\n",
    "\n",
    "    model_filename = os.path.join(model_dir, \"xgb_nopca_noaug_nonorm.pickle\")\n",
    "    with open(model_filename, \"wb\") as f:\n",
    "        pickle.dump(xgb_no_pca, f)\n",
    "\n",
    "# =========================\n",
    "# SCENARIO C: WITH PCA (aug)\n",
    "# =========================\n",
    "if C :\n",
    "    pca = PCA(n_components=0.98)\n",
    "    X_train_aug_pca = pca.fit_transform(X_train_aug)\n",
    "    if TEST_SET:\n",
    "        X_test_aug_pca = pca.transform(X_test_aug)\n",
    "\n",
    "    pca_filename = os.path.join(model_dir, \"pca_aug_nonorm.pickle\")\n",
    "    with open(pca_filename, \"wb\") as f:\n",
    "        pickle.dump(pca, f)\n",
    "\n",
    "    xgb_model_pca = XGBClassifier(n_estimators=n_estimators, max_depth=max_depth, learning_rate=learning_rate,\n",
    "                                subsample=subsample, colsample_bytree=colsample_bytree,\n",
    "                                eval_metric='mlogloss', random_state=42)\n",
    "    xgb_model_pca.fit(X_train_aug_pca, y_train_aug)\n",
    "\n",
    "    model_filename = os.path.join(model_dir, \"xgb_pca_aug_nonorm.pickle\")\n",
    "    with open(model_filename, \"wb\") as f:\n",
    "        pickle.dump(xgb_model_pca, f)\n",
    "\n",
    "# =========================\n",
    "# SCENARIO D: WITHOUT PCA (aug)\n",
    "# =========================\n",
    "if D :\n",
    "    xgb_model_no_pca = XGBClassifier(n_estimators=n_estimators, max_depth=max_depth, learning_rate=learning_rate,\n",
    "                                    subsample=subsample, colsample_bytree=colsample_bytree,\n",
    "                                    eval_metric='mlogloss', random_state=42)\n",
    "    xgb_model_no_pca.fit(X_train_aug, y_train_aug)\n",
    "\n",
    "    model_filename = os.path.join(model_dir, \"xgb_nopca_aug_nonorm.pickle\")\n",
    "    with open(model_filename, \"wb\") as f:\n",
    "        pickle.dump(xgb_model_no_pca, f)\n",
    "\n",
    "# =========================\n",
    "# SCENARIO E: NO DATA TRANSFORMATION (no aug)\n",
    "# =========================\n",
    "if E :\n",
    "    xgb_model_no_transform = XGBClassifier(n_estimators=n_estimators, max_depth=max_depth, learning_rate=learning_rate,\n",
    "                                        subsample=subsample, colsample_bytree=colsample_bytree,\n",
    "                                        eval_metric='mlogloss', random_state=42)\n",
    "    xgb_model_no_transform.fit(X_train, y_train)\n",
    "\n",
    "    model_filename = os.path.join(model_dir, \"xgb_nopca_noaug_nonorm.pickle\")\n",
    "    with open(model_filename, \"wb\") as f:\n",
    "        pickle.dump(xgb_model_no_transform, f)\n",
    "\n",
    "# =========================\n",
    "# SCENARIO F: NORMALIZATION + PCA (no aug)\n",
    "# =========================\n",
    "if F :\n",
    "    X_train_norm = np.array([x/np.linalg.norm(x) if np.linalg.norm(x) != 0 else x for x in X_train])\n",
    "    if TEST_SET:\n",
    "        X_test_norm = np.array([x/np.linalg.norm(x) if np.linalg.norm(x) != 0 else x for x in X_test])\n",
    "\n",
    "    pca = PCA(n_components=0.98)\n",
    "    X_train_norm_pca = pca.fit_transform(X_train_norm)\n",
    "    if TEST_SET:\n",
    "        X_test_norm_pca = pca.transform(X_test_norm)\n",
    "\n",
    "    pca_filename = os.path.join(model_dir, \"pca_noaug_norm.pickle\")\n",
    "    with open(pca_filename, \"wb\") as f:\n",
    "        pickle.dump(pca, f)\n",
    "\n",
    "    xgb_model_norm_pca = XGBClassifier(n_estimators=n_estimators, max_depth=max_depth, learning_rate=learning_rate,\n",
    "                                    subsample=subsample, colsample_bytree=colsample_bytree,\n",
    "                                    eval_metric='mlogloss', random_state=42)\n",
    "    xgb_model_norm_pca.fit(X_train_norm_pca, y_train)\n",
    "\n",
    "    model_filename = os.path.join(model_dir, \"xgb_pca_noaug_norm.pickle\")\n",
    "    with open(model_filename, \"wb\") as f:\n",
    "        pickle.dump(xgb_model_norm_pca, f)\n",
    "\n",
    "# =========================\n",
    "# SCENARIO G: NORMALIZATION + AUG + PCA\n",
    "# =========================\n",
    "if G : \n",
    "    X_train_aug_norm = np.array([x/np.linalg.norm(x) if np.linalg.norm(x) != 0 else x for x in X_train_aug])\n",
    "    if TEST_SET:\n",
    "        X_test_aug_norm = np.array([x/np.linalg.norm(x) if np.linalg.norm(x) != 0 else x for x in X_test_aug])\n",
    "\n",
    "    pca = PCA(n_components=0.98)\n",
    "    X_train_aug_norm_pca = pca.fit_transform(X_train_aug_norm)\n",
    "    if TEST_SET:\n",
    "        X_test_aug_norm_pca = pca.transform(X_test_aug_norm)\n",
    "\n",
    "    pca_filename = os.path.join(model_dir, \"pca_aug_norm.pickle\")\n",
    "    with open(pca_filename, \"wb\") as f:\n",
    "        pickle.dump(pca, f)\n",
    "\n",
    "    xgb_model_norm_aug_pca = XGBClassifier(n_estimators=n_estimators, max_depth=max_depth, learning_rate=learning_rate,\n",
    "                                        subsample=subsample, colsample_bytree=colsample_bytree,\n",
    "                                        eval_metric='mlogloss', random_state=42)\n",
    "    xgb_model_norm_aug_pca.fit(X_train_aug_norm_pca, y_train_aug)\n",
    "\n",
    "    model_filename = os.path.join(model_dir, \"xgb_pca_aug_norm.pickle\")\n",
    "    with open(model_filename, \"wb\") as f:\n",
    "        pickle.dump(xgb_model_norm_aug_pca, f)\n",
    "\n",
    "\n",
    "# =========================\n",
    "# SCENARIO H: NORMALIZATION + AUG (no PCA)\n",
    "# =========================\n",
    "if H:\n",
    "    X_train_aug_norm = np.array([x / np.linalg.norm(x) if np.linalg.norm(x) != 0 else x for x in X_train_aug])\n",
    "    if TEST_SET:\n",
    "        X_test_aug_norm = np.array([x / np.linalg.norm(x) if np.linalg.norm(x) != 0 else x for x in X_test_aug])\n",
    "\n",
    "    xgb_model_aug_norm = XGBClassifier(n_estimators=100, max_depth=5, learning_rate=0.1,\n",
    "                                       subsample=0.8, colsample_bytree=0.8,\n",
    "                                       eval_metric='mlogloss', random_state=42)\n",
    "    xgb_model_aug_norm.fit(X_train_aug_norm, y_train_aug)\n",
    "\n",
    "    model_filename = os.path.join(model_dir, \"xgb_nopca_aug_norm.pickle\")\n",
    "    with open(model_filename, \"wb\") as f:\n",
    "        pickle.dump(xgb_model_aug_norm, f)\n",
    "\n",
    "\n",
    "# =========================\n",
    "# EVALUATION FUNCTION\n",
    "# =========================\n",
    "def evaluate_model(model, X_test, y_test, description):\n",
    "    predict = model.predict(X_test)\n",
    "\n",
    "    classes = np.unique(y_test)\n",
    "    precision_per_class = precision_score(y_test, predict, average=None, labels=classes)\n",
    "    recall_per_class = recall_score(y_test, predict, average=None, labels=classes)\n",
    "    test_accuracy_per_class = []\n",
    "    conf_matrix = confusion_matrix(y_test, predict, labels=classes)\n",
    "\n",
    "    for i, cls in enumerate(classes):\n",
    "        acc = conf_matrix[i, i] / conf_matrix[i, :].sum()\n",
    "        test_accuracy_per_class.append(acc)\n",
    "\n",
    "    cv_scores = cross_val_score(model, X_test, y_test, cv=5, scoring='accuracy')\n",
    "    mean_cv_accuracy = np.mean(cv_scores)\n",
    "\n",
    "    print(f\"\\n=== {description} ===\")\n",
    "    print(f\"Test Accuracy (Overall): {np.mean(predict == y_test):.4f}\")\n",
    "    print(f\"Mean CV Accuracy: {mean_cv_accuracy:.4f}\")\n",
    "\n",
    "    print(\"\\nPer-Class Metrics:\")\n",
    "    for i, cls in enumerate(classes):\n",
    "        print(f\"Class {cls}: Precision={precision_per_class[i]:.4f}, Recall={recall_per_class[i]:.4f}, Accuracy={test_accuracy_per_class[i]:.4f}\")\n",
    "\n",
    "# =========================\n",
    "# EVALUATE ALL MODELS\n",
    "# =========================\n",
    "if TEST_SET:\n",
    "    if A :\n",
    "        evaluate_model(xgb_pca, X_test_pca, y_test, \"Scenario A: PCA NOAUG NONORM\")\n",
    "    if B :\n",
    "        evaluate_model(xgb_no_pca, X_test, y_test, \"Scenario B: NOPCA NOAUG NONORM\")\n",
    "    if C:\n",
    "        evaluate_model(xgb_model_pca, X_test_aug_pca, y_test_aug, \"Scenario C: PCA AUG NONORM\")\n",
    "    if D:\n",
    "        evaluate_model(xgb_model_no_pca, X_test, y_test, \"Scenario D: NOPCA AUG NONORM\")\n",
    "    if E:\n",
    "        evaluate_model(xgb_model_no_transform, X_test, y_test, \"Scenario E: NOPCA NOAUG NONORM\")\n",
    "    if F:\n",
    "        evaluate_model(xgb_model_norm_pca, X_test_norm_pca, y_test, \"Scenario F: PCA NOAUG NORM\")\n",
    "    if G:\n",
    "        evaluate_model(xgb_model_norm_aug_pca, X_test_aug_norm_pca, y_test_aug, \"Scenario G: PCA AUG NORM\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HYPERPARAMETER TUNING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow.keras.wrappers.scikit_learn'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 13\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlayers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m to_categorical\n\u001b[0;32m---> 13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mwrappers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mscikit_learn\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m KerasClassifier\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcallbacks\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m EarlyStopping\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# --- CONFIG FLAGS ---\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow.keras.wrappers.scikit_learn'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import cross_val_score, train_test_split\n",
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score\n",
    "from bayes_opt import BayesianOptimization\n",
    "\n",
    "# Your custom accuracy function\n",
    "from classification.utils.utils import accuracy\n",
    "\n",
    "# --- CONFIG FLAGS ---\n",
    "NORMALIZATION = False\n",
    "TRANSFORMATION = True\n",
    "\n",
    "# --- STEP 1: Load/Select Data ---\n",
    "if TRANSFORMATION:\n",
    "    try:\n",
    "        X = X_basic_aug   # Make sure X_basic_aug is defined in your environment\n",
    "        y = y_basic_aug   # Make sure y_basic_aug is defined in your environment\n",
    "    except NameError:\n",
    "        raise ValueError(\"X_aug and y_aug must be defined before running this script.\")\n",
    "else:\n",
    "    try:\n",
    "        X = X_basic       # Make sure X_basic is defined in your environment\n",
    "        y = y_basic       # Make sure y_basic is defined in your environment\n",
    "    except NameError:\n",
    "        raise ValueError(\"X and y must be defined before running this script.\")\n",
    "\n",
    "# Optional normalization\n",
    "if NORMALIZATION:\n",
    "    X = np.array([\n",
    "        x / np.linalg.norm(x) if np.linalg.norm(x) != 0 else x\n",
    "        for x in X\n",
    "    ])\n",
    "\n",
    "# --- STEP 2: Define the Objective Function for Bayesian Optimization ---\n",
    "def xgb_cv(\n",
    "    n_estimators,\n",
    "    max_depth,\n",
    "    learning_rate,\n",
    "    subsample,\n",
    "    colsample_bytree\n",
    "):\n",
    "    \"\"\"\n",
    "    This function trains an XGBClassifier with given hyperparameters\n",
    "    and returns the mean CV accuracy as the objective to maximize.\n",
    "    \"\"\"\n",
    "    # Convert some parameters to int, as required by XGBoost\n",
    "    n_estimators = int(n_estimators)\n",
    "    max_depth = int(max_depth)\n",
    "\n",
    "    model = XGBClassifier(n_estimators=n_estimators, max_depth=max_depth, learning_rate=learning_rate, subsample=subsample, colsample_bytree=colsample_bytree,\n",
    "        eval_metric='mlogloss',\n",
    "        # remove use_label_encoder (deprecated)\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    # 5-fold cross-validation on the *entire dataset* X, y\n",
    "    cv_scores = cross_val_score(model, X, y, cv=5, scoring='accuracy')\n",
    "    \n",
    "    # Return the mean of cross-validation accuracy\n",
    "    return cv_scores.mean()\n",
    "\n",
    "# --- STEP 3: Set Up the Bayesian Optimizer ---\n",
    "# Hyperparameter search space\n",
    "pbounds = {\n",
    "    'n_estimators': (50, 400),      # e.g. from 50 to 300\n",
    "    'max_depth': (2, 15),           # integer between 2 and 12\n",
    "    'learning_rate': (0.01, 0.3),   # from 0.01 to 0.3\n",
    "    'subsample': (0.5, 1),        # from 0.5 to 1.0\n",
    "    'colsample_bytree': (0.5, 1)  # from 0.5 to 1.0\n",
    "}\n",
    "\n",
    "optimizer = BayesianOptimization(\n",
    "    f=xgb_cv,        # The function we want to maximize\n",
    "    pbounds=pbounds, # The search space\n",
    "    random_state=42  # Ensures reproducibility\n",
    ")\n",
    "\n",
    "# --- STEP 4: Run the Bayesian Optimization Loop ---\n",
    "# We'll do a few initial random explorations (init_points) \n",
    "# and then a certain number of optimization steps (n_iter).\n",
    "init_points = 3\n",
    "n_iter = 20\n",
    "\n",
    "print(\"Starting Bayesian Optimization...\")\n",
    "best_score_so_far = -1.0\n",
    "early_stop_threshold = 0.90  # Stop if we exceed 90% cross-val accuracy\n",
    "\n",
    "optimizer.maximize(init_points=init_points, n_iter=n_iter)\n",
    "\n",
    "for i, res in enumerate(optimizer.res):\n",
    "    score = res['target']\n",
    "    print(f\"Iteration {i+1}, CV Accuracy: {score:.4f}, Parameters: {res['params']}\")\n",
    "    \n",
    "    if score > best_score_so_far:\n",
    "        best_score_so_far = score\n",
    "    \n",
    "    # Early stopping if we found a \"good\" configuration\n",
    "    if best_score_so_far > early_stop_threshold:\n",
    "        print(f\"\\nEarly stopping: Found cross-validation accuracy above {early_stop_threshold}\\n\")\n",
    "        break\n",
    "\n",
    "# --- STEP 5: Get the Best Found Hyperparameters ---\n",
    "best_params = optimizer.max['params']\n",
    "best_n_estimators = int(best_params['n_estimators'])\n",
    "best_max_depth = int(best_params['max_depth'])\n",
    "best_learning_rate = best_params['learning_rate']\n",
    "best_subsample = best_params['subsample']\n",
    "best_colsample_bytree = best_params['colsample_bytree']\n",
    "\n",
    "print(\"\\n=== BEST HYPERPARAMETERS FOUND ===\")\n",
    "print(f\"n_estimators = {best_n_estimators}\")\n",
    "print(f\"max_depth = {best_max_depth}\")\n",
    "print(f\"learning_rate = {best_learning_rate:.4f}\")\n",
    "print(f\"subsample = {best_subsample:.4f}\")\n",
    "print(f\"colsample_bytree = {best_colsample_bytree:.4f}\")\n",
    "print(f\"CV Accuracy = {optimizer.max['target']:.4f}\")\n",
    "\n",
    "# --- STEP 6: Train/Validate Model Once More on a Train/Test Split ---\n",
    "# Final check on a separate holdout set\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, stratify=y, random_state=999\n",
    ")\n",
    "\n",
    "final_model = XGBClassifier(\n",
    "    n_estimators=best_n_estimators,\n",
    "    max_depth=best_max_depth,\n",
    "    learning_rate=best_learning_rate,\n",
    "    subsample=best_subsample,\n",
    "    colsample_bytree=best_colsample_bytree,\n",
    "    eval_metric='mlogloss',\n",
    "    random_state=999\n",
    ")\n",
    "\n",
    "final_model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = final_model.predict(X_test)\n",
    "test_acc = accuracy(y_pred, y_test)\n",
    "\n",
    "print(\"\\n=== FINAL EVALUATION ON HOLDOUT TEST SET ===\")\n",
    "print(f\"Test Accuracy: {test_acc:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
